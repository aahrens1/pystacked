-------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_130.txt
  log type:  text
 opened on:  24 Oct 2025, 11:19:35

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.8e
*! last edited: 23oct2025
*! authors: aa/ms
*! pystacked wrapper - calls pystacked1 (default) or pystacked2 (altpython option)

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.8e
*! last edited: 23oct2025
*! authors: aa/ms
*! pystacked wrapper - calls pystacked1 (default) or pystacked2 (altpython option)

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/41/ld3ks1hx60513_z47fz5n33m0000gq/T//S_68861.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309122
  lassocv        |      0.0000000
  gradboost      |      0.8690878

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739518
  lassocv        |      0.3260482

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179569
  v2              |     -0.1900100
  v3              |      0.0581990
  v4              |      5.3697248
  v5              |      0.3795651
  v6              |      0.2380470
  v7              |      0.8980884
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830520
  v11             |     -0.0505583
  v12             |     -0.1205222
  v13             |     -0.0258984
  v14             |      0.0469273
  v15             |      0.3218610
  v16             |      0.8646395
  v17             |      0.4195419
  v18             |      0.0639255
  v19             |      0.1425000
  v20             |      0.5225859
  v21             |      0.2921303
  v22             |      0.2075705
  v23             |      0.7907717
  v24             |      0.1876249
  v25             |     -3.1780905
  v26             |     -0.9711114
  v27             |    -35.5968972
  v28             |      0.2426810
  v29             |     -1.2685533
  v30             |     -0.1515458
  v31             |     -0.0739819
  v32             |      0.8836881
  v33             |     -0.4070677
  v34             |      0.1678270
  v35             |     -1.1232154
  v36             |      0.3752010
  v37             |      0.0178832
  v38             |     -0.1347784
  v39             |     -0.3787394
  v40             |     -0.1097709
  v41             |    -14.2160216
  v42             |     -2.0756730
  v43             |     -0.2731493
  v44             |     -0.9848931
  v45             |     -0.7940632
  v46             |     -1.2974393
  v47             |     -0.1757831
  v48             |     -1.1157710
  v49             |     -0.3168069
  v50             |     -0.0501774
  v51             |     -0.0725830
  v52             |      0.2863579
  v53             |      1.3153398
  v54             |      1.0699341
  v55             |      0.2032155
  v56             |      1.7614043
  v57             |      0.5055226
  _cons           |    -10.9866960
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936633
  elasticcv      |      0.2063367

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179569
  v2              |     -0.1900100
  v3              |      0.0581990
  v4              |      5.3697248
  v5              |      0.3795651
  v6              |      0.2380470
  v7              |      0.8980884
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830520
  v11             |     -0.0505583
  v12             |     -0.1205222
  v13             |     -0.0258984
  v14             |      0.0469273
  v15             |      0.3218610
  v16             |      0.8646395
  v17             |      0.4195419
  v18             |      0.0639255
  v19             |      0.1425000
  v20             |      0.5225859
  v21             |      0.2921303
  v22             |      0.2075705
  v23             |      0.7907717
  v24             |      0.1876249
  v25             |     -3.1780905
  v26             |     -0.9711114
  v27             |    -35.5968972
  v28             |      0.2426810
  v29             |     -1.2685533
  v30             |     -0.1515458
  v31             |     -0.0739819
  v32             |      0.8836881
  v33             |     -0.4070677
  v34             |      0.1678270
  v35             |     -1.1232154
  v36             |      0.3752010
  v37             |      0.0178832
  v38             |     -0.1347784
  v39             |     -0.3787394
  v40             |     -0.1097709
  v41             |    -14.2160216
  v42             |     -2.0756730
  v43             |     -0.2731493
  v44             |     -0.9848931
  v45             |     -0.7940632
  v46             |     -1.2974393
  v47             |     -0.1757831
  v48             |     -1.1157710
  v49             |     -0.3168069
  v50             |     -0.0501774
  v51             |     -0.0725830
  v52             |      0.2863579
  v53             |      1.3153398
  v54             |      1.0699341
  v55             |      0.2032155
  v56             |      1.7614043
  v57             |      0.5055226
  _cons           |    -10.9866960
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2571388
  ridgecv        |      0.7428612

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179569
  v2              |     -0.1900100
  v3              |      0.0581990
  v4              |      5.3697248
  v5              |      0.3795651
  v6              |      0.2380470
  v7              |      0.8980884
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830520
  v11             |     -0.0505583
  v12             |     -0.1205222
  v13             |     -0.0258984
  v14             |      0.0469273
  v15             |      0.3218610
  v16             |      0.8646395
  v17             |      0.4195419
  v18             |      0.0639255
  v19             |      0.1425000
  v20             |      0.5225859
  v21             |      0.2921303
  v22             |      0.2075705
  v23             |      0.7907717
  v24             |      0.1876249
  v25             |     -3.1780905
  v26             |     -0.9711114
  v27             |    -35.5968972
  v28             |      0.2426810
  v29             |     -1.2685533
  v30             |     -0.1515458
  v31             |     -0.0739819
  v32             |      0.8836881
  v33             |     -0.4070677
  v34             |      0.1678270
  v35             |     -1.1232154
  v36             |      0.3752010
  v37             |      0.0178832
  v38             |     -0.1347784
  v39             |     -0.3787394
  v40             |     -0.1097709
  v41             |    -14.2160216
  v42             |     -2.0756730
  v43             |     -0.2731493
  v44             |     -0.9848931
  v45             |     -0.7940632
  v46             |     -1.2974393
  v47             |     -0.1757831
  v48             |     -1.1157710
  v49             |     -0.3168069
  v50             |     -0.0501774
  v51             |     -0.0725830
  v52             |      0.2863579
  v53             |      1.3153398
  v54             |      1.0699341
  v55             |      0.2032155
  v56             |      1.7614043
  v57             |      0.5055226
  _cons           |    -10.9866960
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145563
  v2              |     -0.1924029
  v3              |      0.0588298
  v4              |      3.0809974
  v5              |      0.3789062
  v6              |      0.2334226
  v7              |      0.8969648
  v8              |      0.2276950
  v9              |      0.1892616
  v10             |      0.0820557
  v11             |     -0.0512706
  v12             |     -0.1220139
  v13             |     -0.0239620
  v14             |      0.0460281
  v15             |      0.3179727
  v16             |      0.8626064
  v17             |      0.4202598
  v18             |      0.0638220
  v19             |      0.1429570
  v20             |      0.5323170
  v21             |      0.2923603
  v22             |      0.2163213
  v23             |      0.7860337
  v24             |      0.1926800
  v25             |     -3.2122350
  v26             |     -0.9382823
  v27             |    -31.3480732
  v28             |      0.2407732
  v29             |     -1.4883714
  v30             |     -0.1492362
  v31             |     -0.0772496
  v32             |      0.7407724
  v33             |     -0.4217351
  v34             |      0.1948139
  v35             |     -1.0935657
  v36             |      0.3757144
  v37             |      0.0171432
  v38             |     -0.1315939
  v39             |     -0.3739390
  v40             |     -0.1067312
  v41             |    -10.9919975
  v42             |     -2.0270442
  v43             |     -0.2715964
  v44             |     -0.9808851
  v45             |     -0.7978560
  v46             |     -1.3281756
  v47             |     -0.1770925
  v48             |     -1.1143785
  v49             |     -0.3171295
  v50             |     -0.0490968
  v51             |     -0.0723734
  v52             |      0.2871558
  v53             |      1.3100070
  v54             |      1.0505197
  v55             |      0.0393954
  v56             |      1.7200847
  v57             |      0.5017272
  _cons           |     -9.8062126
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2949737
  rf             |      0.7050263

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179569
  v2              |     -0.1900100
  v3              |      0.0581990
  v4              |      5.3697248
  v5              |      0.3795651
  v6              |      0.2380470
  v7              |      0.8980884
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830520
  v11             |     -0.0505583
  v12             |     -0.1205222
  v13             |     -0.0258984
  v14             |      0.0469273
  v15             |      0.3218610
  v16             |      0.8646395
  v17             |      0.4195419
  v18             |      0.0639255
  v19             |      0.1425000
  v20             |      0.5225859
  v21             |      0.2921303
  v22             |      0.2075705
  v23             |      0.7907717
  v24             |      0.1876249
  v25             |     -3.1780905
  v26             |     -0.9711114
  v27             |    -35.5968972
  v28             |      0.2426810
  v29             |     -1.2685533
  v30             |     -0.1515458
  v31             |     -0.0739819
  v32             |      0.8836881
  v33             |     -0.4070677
  v34             |      0.1678270
  v35             |     -1.1232154
  v36             |      0.3752010
  v37             |      0.0178832
  v38             |     -0.1347784
  v39             |     -0.3787394
  v40             |     -0.1097709
  v41             |    -14.2160216
  v42             |     -2.0756730
  v43             |     -0.2731493
  v44             |     -0.9848931
  v45             |     -0.7940632
  v46             |     -1.2974393
  v47             |     -0.1757831
  v48             |     -1.1157710
  v49             |     -0.3168069
  v50             |     -0.0501774
  v51             |     -0.0725830
  v52             |      0.2863579
  v53             |      1.3153398
  v54             |      1.0699341
  v55             |      0.2032155
  v56             |      1.7614043
  v57             |      0.5055226
  _cons           |    -10.9866960
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0025273
  v2              |      0.0029971
  v3              |      0.0030034
  v4              |      0.0016560
  v5              |      0.0174204
  v6              |      0.0045900
  v7              |      0.1318335
  v8              |      0.0087491
  v9              |      0.0027065
  v10             |      0.0062498
  v11             |      0.0051083
  v12             |      0.0080391
  v13             |      0.0027215
  v14             |      0.0022103
  v15             |      0.0006526
  v16             |      0.0402159
  v17             |      0.0094713
  v18             |      0.0063349
  v19             |      0.0171523
  v20             |      0.0020533
  v21             |      0.0139302
  v22             |      0.0028815
  v23             |      0.0050454
  v24             |      0.0175657
  v25             |      0.0484425
  v26             |      0.0045138
  v27             |      0.0164349
  v28             |      0.0055786
  v29             |      0.0011975
  v30             |      0.0022078
  v31             |      0.0005795
  v32             |      0.0003580
  v33             |      0.0022416
  v34             |      0.0001047
  v35             |      0.0010074
  v36             |      0.0031663
  v37             |      0.0049129
  v38             |      0.0001894
  v39             |      0.0023176
  v40             |      0.0005439
  v41             |      0.0003161
  v42             |      0.0072555
  v43             |      0.0015977
  v44             |      0.0016438
  v45             |      0.0080291
  v46             |      0.0202367
  v47             |      0.0000885
  v48             |      0.0019986
  v49             |      0.0048426
  v50             |      0.0099387
  v51             |      0.0011018
  v52             |      0.2130498
  v53             |      0.2053771
  v54             |      0.0015430
  v55             |      0.0515305
  v56             |      0.0339504
  v57             |      0.0285884

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342842
  gradboost      |      0.8657158

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179569
  v2              |     -0.1900100
  v3              |      0.0581990
  v4              |      5.3697248
  v5              |      0.3795651
  v6              |      0.2380470
  v7              |      0.8980884
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830520
  v11             |     -0.0505583
  v12             |     -0.1205222
  v13             |     -0.0258984
  v14             |      0.0469273
  v15             |      0.3218610
  v16             |      0.8646395
  v17             |      0.4195419
  v18             |      0.0639255
  v19             |      0.1425000
  v20             |      0.5225859
  v21             |      0.2921303
  v22             |      0.2075705
  v23             |      0.7907717
  v24             |      0.1876249
  v25             |     -3.1780905
  v26             |     -0.9711114
  v27             |    -35.5968972
  v28             |      0.2426810
  v29             |     -1.2685533
  v30             |     -0.1515458
  v31             |     -0.0739819
  v32             |      0.8836881
  v33             |     -0.4070677
  v34             |      0.1678270
  v35             |     -1.1232154
  v36             |      0.3752010
  v37             |      0.0178832
  v38             |     -0.1347784
  v39             |     -0.3787394
  v40             |     -0.1097709
  v41             |    -14.2160216
  v42             |     -2.0756730
  v43             |     -0.2731493
  v44             |     -0.9848931
  v45             |     -0.7940632
  v46             |     -1.2974393
  v47             |     -0.1757831
  v48             |     -1.1157710
  v49             |     -0.3168069
  v50             |     -0.0501774
  v51             |     -0.0725830
  v52             |      0.2863579
  v53             |      1.3153398
  v54             |      1.0699341
  v55             |      0.2032155
  v56             |      1.7614043
  v57             |      0.5055226
  _cons           |    -10.9866960
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033138
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0344599
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0190199
  v25             |      0.0847560
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0323373
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(log_loss)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4877239
  gradboost      |      0.3376509
  nnet           |      0.1174397
  nnet           |      0.0571855

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3591152
  rf             |      0.6408848

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1

Iteration 0:  Log likelihood = -230.91815  
Iteration 1:  Log likelihood = -221.30531  
Iteration 2:  Log likelihood = -221.28762  
Iteration 3:  Log likelihood = -221.28762  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:  Log likelihood = -230.47022  
Iteration 1:  Log likelihood = -217.54677  
Iteration 2:  Log likelihood = -217.49653  
Iteration 3:  Log likelihood = -217.49645  
Iteration 4:  Log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1537294
  lassocv        |      0.0000000
  gradboost      |      0.8462706

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. |  .0216046 |
  2. | .01309151 |
  3. | .00747316 |
  4. | .96340126 |
  5. | .98903596 |
     |-----------|
  6. | .01358385 |
  7. | .10368481 |
  8. | .98449093 |
  9. | .07362045 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:  Log likelihood = -461.39142  
Iteration 1:  Log likelihood = -438.05678  
Iteration 2:  Log likelihood = -437.95577  
Iteration 3:  Log likelihood = -437.95561  
Iteration 4:  Log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0809648
  lassocv        |      0.0000000
  gradboost      |      0.6113212
  nnet           |      0.3077140
logit lassocv rf nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0602419
  lassocv        |      0.0000000
  rf             |      0.4216817
  nnet           |      0.5180764
logit ridgecv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0341032
  gradboost      |      0.4721050
  nnet           |      0.4937918
logit elasticcv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803134
  nnet           |      0.4196866
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1575093
  elasticcv      |      0.0000000
  gradboost      |      0.4809305
  svm            |      0.3615602


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2039791
  lassocv        |      0.0000000
  gradboost      |      0.7960209

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1419118
  rf             |      0.3417257
  gradboost      |      0.5163625

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417      13         .       .         .       .
  STACKING     1 |    .         4     256         .       .         .       .
  logit        0 | 0.142      399      39       389      49         .       .
  logit        1 | 0.142       22     230        32     220         .       .
  rf           0 | 0.342      421       1       393      36         .       .
  rf           1 | 0.342        0     268        28     233         .       .
  gradboost    0 | 0.516      416      13       398      34         .       .
  gradboost    1 | 0.516        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4221173
  rf             |      0.1808551
  gradboost      |      0.3970276

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       204      19
  STACKING     1 |    .         2     118         .       .        14     128
  logit        0 | 0.422      198       8       183      16       200      20
  logit        1 | 0.422        5     114        20     106        18     127
  rf           0 | 0.181      203       1       190      21       206      22
  rf           1 | 0.181        0     121        13     101        12     125
  gradboost    0 | 0.397      203       4       196      24       203      24
  gradboost    1 | 0.397        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .        96      11
  STACKING     1 |    .         2     118         .       .         5      63
  logit        0 | 0.422      198       8       183      16        93      11
  logit        1 | 0.422        5     114        20     106         8      63
  rf           0 | 0.181      203       1       190      21        96      13
  rf           1 | 0.181        0     121        13     101         5      61
  gradboost    0 | 0.397      203       4       196      24        96      13
  gradboost    1 | 0.397        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       6         .       .       200      17
  STACKING     1 |    .         2     116         .       .        18     130
  logit        0 | 0.555      198       8       184      15       200      20
  logit        1 | 0.555        5     114        19     107        18     127
  rf           0 | 0.000      203       1       189      23       206      22
  rf           1 | 0.000        0     121        14      99        12     125
  gradboost    0 | 0.445      203       4       192      24       203      24
  gradboost    1 | 0.445        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.5392442
  rf             |      0.1527844
  gradboost      |      0.3079714

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       5         .       .       200      18
  STACKING     1 |    .         2     117         .       .        18     129
  logit        0 | 0.539      196       7       188      22       200      21
  logit        1 | 0.539        7     115        15     100        18     126
  rf           0 | 0.153      203       1       187      21       206      22
  rf           1 | 0.153        0     121        16     101        12     125
  gradboost    0 | 0.308      203       4       193      24       203      24
  gradboost    1 | 0.308        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2224654
  rf             |      0.1009032
  gradboost      |      0.6766315

. pystacked, graph

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4649089
  rf             |      0.3468763
  gradboost      |      0.1882148

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout
Number of holdout observations:  365

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4262303
  rf             |      0.0978901
  gradboost      |      0.4758795

. pystacked, graph holdout
Number of holdout observations:  365

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.8e
*! last edited: 23oct2025
*! authors: aa/ms
*! pystacked wrapper - calls pystacked1 (default) or pystacked2 (altpython option)

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = s
> qrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm_s
> tart = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1; 
> min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; ma
> x_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; rand
> om_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_state
>  = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_
> state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0.
> 01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentum 
> = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_chang
> e = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; dec
> ision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state = 
> RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/kahrens/MyProjects/pystacked/pystacked.py", line 637, in run_stacked
    Tx = np.max(np.sqrt(nobs) * zeta_m / zeta_sd)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/lib/python3.12/site-packages/numpy/core/fromnumeric.py", line 2810, in max
    return _wrapreduction(a, np.maximum, 'max', axis, None, out,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/lib/python3.12/site-packages/numpy/core/fromnumeric.py", line 88, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: zero-size array to reduction operation maximum which has no identity
r(7102);

end of do-file
r(7102);

end of do-file
r(7102);
