
  ___  ____  ____  ____  ____ ®
 /__    /   ____/   /   ____/      17.0
___/   /   /___/   /   /___/       SE—Standard Edition

 Statistics and Data Science       Copyright 1985-2021 StataCorp LLC
                                   StataCorp
                                   4905 Lakeway Drive
                                   College Station, Texas 77845 USA
                                   800-STATA-PC        https://www.stata.com
                                   979-696-4600        stata@stata.com

Stata license: 100-user network perpetual
Serial number: 401706318278
  Licensed to: Staff
               ETH Zurich

Notes:
      1. Stata is running in batch mode.
      2. Unicode is supported; see help unicode_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

. do cs_pystacked_runall.do 024 


Notes:
      1. Stata is running in batch mode.
      2. Unicode is supported; see help unicode_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

. do cs_pystacked_runall.do 130 


Notes:
      1. Stata is running in batch mode.
      2. Unicode is supported; see help unicode_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

. do cs_pystacked_runall.do 113 

. clear all
. clear all
. clear all

Notes:
      1. Stata is running in batch mode.
      2. Unicode is supported; see help unicode_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

. do cs_pystacked_runall.do 102 


Notes:
      1. Stata is running in batch mode.
      2. Unicode is supported; see help unicode_advice.
      3. Maximum number of variables is set to 5,000; see help set_maxvar.

. do cs_pystacked_runall.do 121 

. clear all
. clear all

. 

. 

. 
. cap cd "/Users/kahrens/MyProjects/pystacked/cert"

. cap cd "/Users/kahrens/MyProjects/pystacked/cert"

. cap cd "/Users/kahrens/MyProjects/pystacked/cert"

. cap cd "/Users/kahrens/MyProjects/pystacked/cert"

. cap cd "/Users/kahrens/MyProjects/pystacked/cert"

. cap cd "/Users/ecomes/Documents/GitHub/pystacked/cert"

. cap cd "/Users/ecomes/Documents/GitHub/pystacked/cert"

. cap cd "/Users/ecomes/Documents/GitHub/pystacked/cert"

. cap cd "/Users/ecomes/Documents/GitHub/pystacked/cert"

. cap cd "/Users/ecomes/Documents/GitHub/pystacked/cert"

. 
. 
. 
. 
. 
. if "`c(username)'"=="kahrens" {
. if "`c(username)'"=="kahrens" {
. if "`c(username)'"=="kahrens" {
. if "`c(username)'"=="kahrens" {
. if "`c(username)'"=="kahrens" {
.         adopath + "/Users/kahrens/MyProjects/pystacked"
.         adopath + "/Users/kahrens/MyProjects/pystacked"
.         adopath + "/Users/kahrens/MyProjects/pystacked"
.         adopath + "/Users/kahrens/MyProjects/pystacked"
  [1]  (BASE)      "/Applications/Stata 17/ado/base/"
  [1]  (BASE)      "/Applications/Stata 17/ado/base/"
  [2]  (SITE)      "/Applications/Stata 17/ado/site/"
  [3]              "."
plications/Stata 17/ado/site/"
  [4]  (PERSONAL)  "/Users/kahrens/Documents/Stata/ado/personal/"
  [5]  (PLUS)      "/Users/kahrens/Library/Application Support/Stata/ado/plus/"
  [6]  (OLDPLACE)  "~/ado/"
ahrens/Library/Application Support/Stata/ado/plus/"
  [1]  (BASE)      "/Applications/Stata 17/ado/base/"
d"
  [6]  (OLDPLACE)  "~/ado/"
. }

  [7]              "/Users/kahrens/MyProjects/pystacked"

  [2]  (SITE)      "/Applications/Stata 17/ado/site/"
. }

 [3]              "."
  [4]  (PERSONAL)  "/Users/kahrens/Documents/Stata/ado/personal/"
  [5]  (PLUS)      "/Users/kahrens/Library/Application Support/Stata/ado/plus/"
  [6]  (OLDPLACE)  "~/ado/"
  [7]              "/Users/kahrens/MyProjects/pystacked"
  [1
  (BASE)      "/Applications/Stata 17/ado/base/"
  [2]  (SITE)      "/Applications/Stata 17/ado/site/"
  [3]              "."
  [4]  (PERSONAL)  "/Users/kahrens/Documents/Stata/ado/personal/"
  [5]  (PLUS)      "/Users/kahrens/Library/Application Support/Stata/ado/plus/"
  [6]  (OLDPLACE)  "~/ado/"
  [7]              "/Users/kahrens/MyProjects/pystacked"
. }

  [1]  (BASE)      "/Applications/Stata 17/ado/base/"
  [2]  (SITE)      "/Applications/Stata 17/ado/site/"
  [3]              "."
  [4]  (PERSONAL)  "/Users/kahrens/Documents/Stata/ado/personal/"
  [5]  (PLUS)      "/Users/kahrens/Library/Application Support/Stata/ado/plus/"
  [6]  (OLDPLACE)  "~/ado/"
  [7]              "/Users/kahrens/MyProjects/pystacked"
. }

. else if "`c(username)'"=="ecomes" {
. else if "`c(username)'"=="ecomes" {
. else if "`c(username)'"=="ecomes" {
. else if "`c(username)'"=="ecomes" {
. else if "`c(username)'"=="ecomes" {
.         adopath + "/Users/ecomes/Documents/GitHub/pystacked/cert"
.         adopath + "/Users/ecomes/Documents/GitHub/pystacked/cert"
.         adopath + "/Users/ecomes/Documents/GitHub/pystacked/cert"
. }

.         adopath + "/Users/ecomes/Documents/GitHub/pystacked/cert"
. }

.         adopath + "/Users/ecomes/Documents/GitHub/pystacked/cert"
. }

. else {
. }

. else {
. }

. else {
. else {
. else {
.         net install pystacked, ///
>                 from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace
. }

. 
.         net install pystacked, ///
>                 from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace
. }

. 
.         net install pystacked, ///
>                 from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace
. }

. 
.         net install pystacked, ///
>                 from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace
. }

. 
.         net install pystacked, ///
>                 from(https://raw.githubusercontent.com/aahrens1/pystacked/main) replace
. }

. 
. local ver `0'

. 
. local ver `0'

. 
. local ver `0'

. 
. local ver `0'

. 
. local ver `0'

. 
. cap python set exec "/Users/kahrens/python_envs/sk`ver'/bin/python3"
. cap python set exec "/Users/kahrens/python_envs/sk`ver'/bin/python3"
. cap python set exec "/Users/kahrens/python_envs/sk`ver'/bin/python3"
. cap python set exec "/Users/kahrens/python_envs/sk`ver'/bin/python3"
. cap python set exec "/Users/kahrens/python_envs/sk`ver'/bin/python3"

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8433027
  gradboost      |      0.1566973
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1


 
. 

. 

. 

. 
. cap log close
. cap log close
. cap log close
. cap log close



. cap log close

. log using "log_cs_pystacked_`ver'.txt", text replace
. log using "log_cs_pystacked_`ver'.txt", text replace
. log using "log_cs_pystacked_`ver'.txt", text replace
. log using "log_cs_pystacked_`ver'.txt", text replace
------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_113.txt
  log type:  text
 opened on:   3 Jan 2025, 22:38:30

------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_102.txt
  log type:  text
 opened on:   3 Jan 2025, 22:38:30

. 
. 
. which pystacked 
. which pystacked 
------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_024.txt
  log type:  text
 opened on:   3 Jan 2025, 22:38:30

------------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_121.txt
  log type:  text
 opened on:   3 Jan 2025, 22:38:30

. 
---------------------------------------------------------------------------------------------------------------------------------------------------
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_130.txt
  log type:  text
 opened on:   3 Jan 2025, 22:38:30

. 
. which pystacked 
. which pystacked 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn
. python: import sklearn
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn
. python: import sklearn
. python: import sklearn

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438407
  elasticcv      |      0.7209758
  gradboost      |      0.2351835
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4022478
  rf             |      0.5977522
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0538925
  elasticcv      |      0.8748075
  gradboost      |      0.0544639
  svm            |      0.0168361
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

. python: sklearn.__version__
'1.3.0'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

. python: sklearn.__version__
'0.24.2'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'0.24.2'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

. python: sklearn.__version__
'1.0.2'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.0.2'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

. python: sklearn.__version__
'1.1.3'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.1.3'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

. python: sklearn.__version__
'1.2.1'

. 
. do "cs_pystacked_class.do"

. 
. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.2.1'

. 
. tempfile testdata

. set seed 765

. global model v58 v1-v30

. insheet using https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0749557
  ridgecv        |      0.8220152
  gradboost      |      0.0000000
  nnet           |      0.1030291
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539241
  elasticcv      |      0.8787627
  gradboost      |      0.0504939
  svm            |      0.0168193
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/0w/r8yclchd52bfwbf8l00fkkrr0000gs/T//S_44562.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/0w/r8yclchd52bfwbf8l00fkkrr0000gs/T//S_44558.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/0w/r8yclchd52bfwbf8l00fkkrr0000gs/T//S_44560.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/0w/r8yclchd52bfwbf8l00fkkrr0000gs/T//S_44559.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. sample 15
(3,911 observations deleted)

. gen u = runiform()

. gen train = u<0.5

. gen train2 = u<.75

. save `testdata'
file /var/folders/0w/r8yclchd52bfwbf8l00fkkrr0000gs/T//S_44561.000001 saved as .dta format

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1239925
  lassocv        |      0.5579448
  rf             |      0.1378442
  nnet           |      0.1802185
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  pystacked v58 v1-v57 , type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8432998
  gradboost      |      0.1567002
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0154060
  elasticcv      |      0.2442894
  gradboost      |      0.3137956
  nnet           |      0.4440543
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0267243
  lassocv        |      0.1246441
  gradboost      |      0.5752791
  nnet           |      0.2641451
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0749556
  ridgecv        |      0.8220155
  gradboost      |      0.0000000
  nnet           |      0.1030289
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1271850
  elasticcv      |      0.5715479
  gradboost      |      0.3012671
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  logit          |      0.0179452
  rf             |      0.5161028
  nnet           |      0.0000000
  elasticcv      |      0.6826674
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0
  gradboost      |      0.0486258
  svm            |      0.2623642
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994769
  gradboost      |      0.2005231
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209759
  gradboost      |      0.2351836
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803518
  lassocv        |      0.4216454
  rf             |      0.3499604
  nnet           |      0.1642193
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803995
  lassocv        |      0.4215865
  rf             |      0.3499628
  nnet           |      0.1642276
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0538924
  elasticcv      |      0.8748077
  gradboost      |      0.0544638
  svm            |      0.0168361
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1273748
  elasticcv      |      0.5735906
  gradboost      |      0.2990347
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438407
  elasticcv      |      0.7209758
  gradboost      |      0.2351835
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2969659
  ridgecv        |      0.3923000
  gradboost      |      0.1129875
  nnet           |      0.2486939
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2926994
  ridgecv        |      0.4014700
  gradboost      |      0.1116836
  nnet           |      0.2460907
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0273986
  lassocv        |      0.1244938
  gradboost      |      0.5755232
  nnet           |      0.2631049
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0538972
  elasticcv      |      0.8749072
  gradboost      |      0.0543585
  svm            |      0.0168371
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169289
  elasticcv      |      0.2455163
  gradboost      |      0.3103246
  nnet           |      0.4451758
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0154060
  elasticcv      |      0.2442894
  gradboost      |      0.3137956
  nnet           |      0.4440543
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803518
  lassocv        |      0.4216454
  rf             |      0.3499604
  nnet           |      0.1642193
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0243928
  lassocv        |      0.1273103
  gradboost      |      0.5734194
  nnet           |      0.2666748
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175438
  elasticcv      |      0.6823223
  gradboost      |      0.0497144
  svm            |      0.2618175
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0179452
  elasticcv      |      0.6826674
  gradboost      |      0.0486258
  svm            |      0.2623642
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2969659
  ridgecv        |      0.3923000
  gradboost      |      0.1129875
  nnet           |      0.2486939
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803518
  lassocv        |      0.4216454
  rf             |      0.3499604
  nnet           |      0.1642193
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2972636
  ridgecv        |      0.3942961
  gradboost      |      0.1100849
  nnet           |      0.2498404
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0154060
  elasticcv      |      0.2442894
  gradboost      |      0.3137956
  nnet           |      0.4440543
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0179452
  elasticcv      |      0.6826674
  gradboost      |      0.0486258
  svm            |      0.2623642
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1231669
  lassocv        |      0.2972730
  gradboost      |      0.2013758
  nnet           |      0.3715274
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0157366
  elasticcv      |      0.2455108
  gradboost      |      0.3115609
  nnet           |      0.4449192
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0181467
  elasticcv      |      0.6825870
  gradboost      |      0.0484520
  svm            |      0.2623002
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386970
  lassocv        |      0.5760017
  rf             |      0.0474659
  nnet           |      0.2318607
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2530983
  ridgecv        |      0.1141101
  gradboost      |      0.1978387
  nnet           |      0.5094459
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818025
  elasticcv      |      0.9744224
  gradboost      |     -0.3114438
  nnet           |      0.3378060
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1231669
  lassocv        |      0.2972730
  gradboost      |      0.2013758
  nnet           |      0.3715274
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386918
  lassocv        |      0.5760072
  rf             |      0.0474635
  nnet           |      0.2318628
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1999714
  elasticcv      |      0.3592336
  gradboost      |      0.3615660
  svm            |      0.1796295
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386970
  lassocv        |      0.5760017
  rf             |      0.0474659
  nnet           |      0.2318607
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505683
  ridgecv        |      0.1149965
  gradboost      |      0.1986832
  nnet           |      0.5097984
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2530983
  ridgecv        |      0.1141101
  gradboost      |      0.1978387
  nnet           |      0.5094459
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804428
  elasticcv      |      0.9703585
  gradboost      |     -0.3040851
  nnet           |      0.3350215
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1231669
  lassocv        |      0.2972733
  gradboost      |      0.2013755
  nnet           |      0.3715271
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818025
  elasticcv      |      0.9744224
  gradboost      |     -0.3114438
  nnet           |      0.3378060
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1224573
  lassocv        |      0.2980001
  gradboost      |      0.2011393
  nnet           |      0.3714198
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386970
  lassocv        |      0.5760017
  rf             |      0.0474659
  nnet           |      0.2318607
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0154458
  ridgecv        |      0.7741711
  gradboost      |      0.2584962
  nnet           |     -0.2341008
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1999714
  elasticcv      |      0.3592336
  gradboost      |      0.3615660
  svm            |      0.1796295
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2530983
  ridgecv        |      0.1141101
  gradboost      |      0.1978387
  nnet           |      0.5094459
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386970
  lassocv        |      0.5760017
  rf             |      0.0474659
  nnet           |      0.2318607
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309106
  lassocv        |      0.0000000
  gradboost      |      0.8690894

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309106
  lassocv        |      0.0000000
  gradboost      |      0.8690894

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818025
  elasticcv      |      0.9744224
  gradboost      |     -0.3114438
  nnet           |      0.3378060
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2530766
  ridgecv        |      0.1139861
  gradboost      |      0.1981478
  nnet           |      0.5093360
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309106
  lassocv        |      0.0000000
  gradboost      |      0.8690894

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69257081 |
  2. | .67613465 |
  3. | .38524577 |
  4. | .55129158 |
  5. | 1.6045682 |
     |-----------|
  6. | .82604527 |
  7. | 1.7851374 |
  8. | 2.0047503 |
  9. | 1.2320343 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309104
  lassocv        |      0.0000000
  gradboost      |      0.8690896

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1999714
  elasticcv      |      0.3592336
  gradboost      |      0.3615660
  svm            |      0.1796295
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0764172
  elasticcv      |      0.9660495
  gradboost      |     -0.2926689
  nnet           |      0.3232238
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1309104
  lassocv        |      0.0000000
  gradboost      |      0.8690896

. 
.  
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1998677
  elasticcv      |      0.3524782
  gradboost      |      0.3723593
  svm            |      0.1730374
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg
(58 vars, 4,601 obs)

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.4306309
  lassocv        |      0.2500000
  rf             |      0.3943921
  gradboost      |      0.2500000

  nnet           |      0.2500000
. pystacked, table
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69257081 |
  2. | .67613465 |
  3. | .38524577 |
  4. | .55129158 |
  5. | 1.6045682 |
     |-----------|
  6. | .82604527 |
  7. | 1.7851374 |
  8. | 2.0047503 |
  9. | 1.2320343 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380347
  lassocv        |      0.0000000
  rf             |      0.5619653

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69257081 |
  2. | .67613465 |
  3. | .38524577 |
  4. | .55129158 |
  5. | 1.6045682 |
     |-----------|
  6. | .82604527 |
  7. | 1.7851374 |
  8. | 2.0047503 |
  9. | 1.2320343 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885183
  lassocv        |      0.0000000
  gradboost      |      0.1114817

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359

. // in-sample predictions
. pystacked, graph
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380347
  lassocv        |      0.0000000
  rf             |      0.5619653

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout
Number of holdout observations:   48

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739491
  lassocv        |      0.3260509

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739491
  lassocv        |      0.3260509

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596641
  rf             |      0.3403359


. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
. // in-sample predictions
. pystacked, graph
Number of holdout observations:   48
Number of holdout observations:   48

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_142.txt
  log type:  text
 closed on:   3 Jan 2025, 22:43:24
------------------------------------------------------------------------------------------------------------------------------------------------------


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380347
  lassocv        |      0.0000000
  rf             |      0.5619653

. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739491
  lassocv        |      0.3260509

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739503
  lassocv        |      0.3260497

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_132.txt
  log type:  text
 closed on:   3 Jan 2025, 22:43:31
------------------------------------------------------------------------------------------------------------------------------------------------------


. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48
Number of holdout observations:   48

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_152.txt
  log type:  text
 closed on:   3 Jan 2025, 22:43:51
------------------------------------------------------------------------------------------------------------------------------------------------------

Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.6739503
  lassocv        |      0.3260497

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0927524
  v2              |     -0.2087767
  v3              |      0.0713842
  v4              |      0.8026795
  v5              |      0.3675217
  v6              |      0.1745002
  v7              |      0.9521076
  v8              |      0.2259153
  v9              |      0.1538970
  v10             |      0.0725012
  v11             |     -0.0451868
  v12             |     -0.1304141
  v13             |     -0.0299883
  v14             |      0.0342585
  v15             |      0.2485486
  v16             |      0.8363669
  v17             |      0.4116139
  v18             |      0.0737981
  v19             |      0.1497646
  v20             |      0.4734683
  v21             |      0.2808785
  v22             |      0.2961755
  v23             |      0.8962999
  v24             |      0.2381985
  v25             |     -2.6561784
  v26             |     -1.0617995
  v27             |     -3.6930977
  v28             |      0.2339028
  v29             |     -0.9617245
  v30             |     -0.1514536
  v31             |     -0.4798314
  v32             |     -0.1653773
  v33             |     -0.5046302
  v34             |     -0.2216528
  v35             |     -0.9496137
  v36             |      0.3643417
  v37             |      0.0028702
  v38             |     -0.1266692
  v39             |     -0.3308447
  v40             |     -0.1887239
  v41             |     -1.2131609
  v42             |     -1.4872199
  v43             |     -0.2313890
  v44             |     -0.9268591
  v45             |     -0.8128207
  v46             |     -1.2851569
  v47             |     -0.1666576
  v48             |     -0.9109827
  v49             |     -0.3281005
  v50             |     -0.0577573
  v51             |     -0.1367399
  v52             |      0.3456474
  v53             |      1.3566301
  v54             |      0.8239033
  v55             |     -0.1222615
  v56             |      1.3008071
  v57             |      0.4150875
  _cons           |     -2.5026034
Note: Coefficients correspond to decision boundary function.

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48
Number of holdout observations:   48

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_160.txt
  log type:  text
 closed on:   3 Jan 2025, 22:44:08
------------------------------------------------------------------------------------------------------------------------------------------------------


. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380347
  lassocv        |      0.0000000
  rf             |      0.5619653

. pystacked, graph holdout
Number of holdout observations:   48

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_140.txt
  log type:  text
 closed on:   3 Jan 2025, 22:44:15
------------------------------------------------------------------------------------------------------------------------------------------------------


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936788
  elasticcv      |      0.2063212

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936788
  elasticcv      |      0.2063212

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2570969
  ridgecv        |      0.7429031

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145574
  v2              |     -0.1924021
  v3              |      0.0588297
  v4              |      3.0809864
  v5              |      0.3789061
  v6              |      0.2334238
  v7              |      0.8969640
  v8              |      0.2276951
  v9              |      0.1892638
  v10             |      0.0820554
  v11             |     -0.0512705
  v12             |     -0.1220133
  v13             |     -0.0239618
  v14             |      0.0460284
  v15             |      0.3179720
  v16             |      0.8626056
  v17             |      0.4202605
  v18             |      0.0638222
  v19             |      0.1429571
  v20             |      0.5323191
  v21             |      0.2923602
  v22             |      0.2163215
  v23             |      0.7860330
  v24             |      0.1926795
  v25             |     -3.2122353
  v26             |     -0.9382806
  v27             |    -31.3492542
  v28             |      0.2407729
  v29             |     -1.4883768
  v30             |     -0.1492364
  v31             |     -0.0772480
  v32             |      0.7407312
  v33             |     -0.4217332
  v34             |      0.1948387
  v35             |     -1.0935599
  v36             |      0.3757142
  v37             |      0.0171436
  v38             |     -0.1315938
  v39             |     -0.3739399
  v40             |     -0.1067304
  v41             |    -10.9923732
  v42             |     -2.0270553
  v43             |     -0.2715977
  v44             |     -0.9808854
  v45             |     -0.7978564
  v46             |     -1.3281747
  v47             |     -0.1770928
  v48             |     -1.1143904
  v49             |     -0.3171294
  v50             |     -0.0490969
  v51             |     -0.0723732
  v52             |      0.2871549
  v53             |      1.3100062
  v54             |      1.0505078
  v55             |      0.0394505
  v56             |      1.7200868
  v57             |      0.5017291
  _cons           |     -9.8065268
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2570969
  ridgecv        |      0.7429031

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145574
  v2              |     -0.1924021
  v3              |      0.0588297
  v4              |      3.0809864
  v5              |      0.3789061
  v6              |      0.2334238
  v7              |      0.8969640
  v8              |      0.2276951
  v9              |      0.1892638
  v10             |      0.0820554
  v11             |     -0.0512705
  v12             |     -0.1220133
  v13             |     -0.0239618
  v14             |      0.0460284
  v15             |      0.3179720
  v16             |      0.8626056
  v17             |      0.4202605
  v18             |      0.0638222
  v19             |      0.1429571
  v20             |      0.5323191
  v21             |      0.2923602
  v22             |      0.2163215
  v23             |      0.7860330
  v24             |      0.1926795
  v25             |     -3.2122353
  v26             |     -0.9382806
  v27             |    -31.3492542
  v28             |      0.2407729
  v29             |     -1.4883768
  v30             |     -0.1492364
  v31             |     -0.0772480
  v32             |      0.7407312
  v33             |     -0.4217332
  v34             |      0.1948387
  v35             |     -1.0935599
  v36             |      0.3757142
  v37             |      0.0171436
  v38             |     -0.1315938
  v39             |     -0.3739399
  v40             |     -0.1067304
  v41             |    -10.9923732
  v42             |     -2.0270553
  v43             |     -0.2715977
  v44             |     -0.9808854
  v45             |     -0.7978564
  v46             |     -1.3281747
  v47             |     -0.1770928
  v48             |     -1.1143904
  v49             |     -0.3171294
  v50             |     -0.0490969
  v51             |     -0.0723732
  v52             |      0.2871549
  v53             |      1.3100062
  v54             |      1.0505078
  v55             |      0.0394505
  v56             |      1.7200868
  v57             |      0.5017291
  _cons           |     -9.8065268
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936788
  elasticcv      |      0.2063212

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936782
  elasticcv      |      0.2063218

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2570969
  ridgecv        |      0.7429031

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145574
  v2              |     -0.1924021
  v3              |      0.0588297
  v4              |      3.0809864
  v5              |      0.3789061
  v6              |      0.2334238
  v7              |      0.8969640
  v8              |      0.2276951
  v9              |      0.1892638
  v10             |      0.0820554
  v11             |     -0.0512705
  v12             |     -0.1220133
  v13             |     -0.0239618
  v14             |      0.0460284
  v15             |      0.3179720
  v16             |      0.8626056
  v17             |      0.4202605
  v18             |      0.0638222
  v19             |      0.1429571
  v20             |      0.5323191
  v21             |      0.2923602
  v22             |      0.2163215
  v23             |      0.7860330
  v24             |      0.1926795
  v25             |     -3.2122353
  v26             |     -0.9382806
  v27             |    -31.3492542
  v28             |      0.2407729
  v29             |     -1.4883768
  v30             |     -0.1492364
  v31             |     -0.0772480
  v32             |      0.7407312
  v33             |     -0.4217332
  v34             |      0.1948387
  v35             |     -1.0935599
  v36             |      0.3757142
  v37             |      0.0171436
  v38             |     -0.1315938
  v39             |     -0.3739399
  v40             |     -0.1067304
  v41             |    -10.9923732
  v42             |     -2.0270553
  v43             |     -0.2715977
  v44             |     -0.9808854
  v45             |     -0.7978564
  v46             |     -1.3281747
  v47             |     -0.1770928
  v48             |     -1.1143904
  v49             |     -0.3171294
  v50             |     -0.0490969
  v51             |     -0.0723732
  v52             |      0.2871549
  v53             |      1.3100062
  v54             |      1.0505078
  v55             |      0.0394505
  v56             |      1.7200868
  v57             |      0.5017291
  _cons           |     -9.8065268
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2570104
  ridgecv        |      0.7429896

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145574
  v2              |     -0.1924022
  v3              |      0.0588296
  v4              |      3.0810159
  v5              |      0.3789061
  v6              |      0.2334238
  v7              |      0.8969641
  v8              |      0.2276950
  v9              |      0.1892637
  v10             |      0.0820554
  v11             |     -0.0512704
  v12             |     -0.1220134
  v13             |     -0.0239618
  v14             |      0.0460284
  v15             |      0.3179723
  v16             |      0.8626056
  v17             |      0.4202606
  v18             |      0.0638222
  v19             |      0.1429571
  v20             |      0.5323186
  v21             |      0.2923602
  v22             |      0.2163208
  v23             |      0.7860330
  v24             |      0.1926794
  v25             |     -3.2122374
  v26             |     -0.9382799
  v27             |    -31.3491909
  v28             |      0.2407730
  v29             |     -1.4883588
  v30             |     -0.1492365
  v31             |     -0.0772478
  v32             |      0.7407408
  v33             |     -0.4217337
  v34             |      0.1948398
  v35             |     -1.0935624
  v36             |      0.3757142
  v37             |      0.0171436
  v38             |     -0.1315938
  v39             |     -0.3739398
  v40             |     -0.1067307
  v41             |    -10.9923048
  v42             |     -2.0270583
  v43             |     -0.2715974
  v44             |     -0.9808852
  v45             |     -0.7978563
  v46             |     -1.3281751
  v47             |     -0.1770927
  v48             |     -1.1143916
  v49             |     -0.3171293
  v50             |     -0.0490969
  v51             |     -0.0723733
  v52             |      0.2871549
  v53             |      1.3100064
  v54             |      1.0505126
  v55             |      0.0394479
  v56             |      1.7200868
  v57             |      0.5017290
  _cons           |     -9.8064998
Note: Coefficients correspond to decision boundary function.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2952080
  rf             |      0.7047920

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0025293
  v2              |      0.0030240
  v3              |      0.0030042
  v4              |      0.0016396
  v5              |      0.0173837
  v6              |      0.0045838
  v7              |      0.1317767
  v8              |      0.0087424
  v9              |      0.0026882
  v10             |      0.0062391
  v11             |      0.0050575
  v12             |      0.0080176
  v13             |      0.0026640
  v14             |      0.0022059
  v15             |      0.0006526
  v16             |      0.0402371
  v17             |      0.0094800
  v18             |      0.0063048
  v19             |      0.0171155
  v20             |      0.0020291
  v21             |      0.0139287
  v22             |      0.0028976
  v23             |      0.0050899
  v24             |      0.0175639
  v25             |      0.0484362
  v26             |      0.0045080
  v27             |      0.0164160
  v28             |      0.0055945
  v29             |      0.0011961
  v30             |      0.0022316
  v31             |      0.0005629
  v32             |      0.0003580
  v33             |      0.0022487
  v34             |      0.0001047
  v35             |      0.0010154
  v36             |      0.0031669
  v37             |      0.0049397
  v38             |      0.0001896
  v39             |      0.0023093
  v40             |      0.0005439
  v41             |      0.0003075
  v42             |      0.0072801
  v43             |      0.0015980
  v44             |      0.0016355
  v45             |      0.0080167
  v46             |      0.0202660
  v47             |      0.0001055
  v48             |      0.0019994
  v49             |      0.0048309
  v50             |      0.0100224
  v51             |      0.0010982
  v52             |      0.2130719
  v53             |      0.2054246
  v54             |      0.0015506
  v55             |      0.0515210
  v56             |      0.0338967
  v57             |      0.0286981

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1556822
  rf             |      0.8443178

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0042850
  v2              |      0.0049748
  v3              |      0.0132401
  v4              |      0.0010365
  v5              |      0.0318705
  v6              |      0.0073465
  v7              |      0.0699817
  v8              |      0.0098464
  v9              |      0.0050108
  v10             |      0.0088243
  v11             |      0.0075686
  v12             |      0.0114366
  v13             |      0.0035312
  v14             |      0.0023556
  v15             |      0.0019670
  v16             |      0.0654783
  v17             |      0.0097685
  v18             |      0.0079339
  v19             |      0.0319235
  v20             |      0.0042825
  v21             |      0.0628539
  v22             |      0.0022870
  v23             |      0.0246193
  v24             |      0.0389478
  v25             |      0.0447852
  v26             |      0.0179915
  v27             |      0.0268937
  v28             |      0.0051752
  v29             |      0.0018855
  v30             |      0.0041700
  v31             |      0.0007973
  v32             |      0.0009296
  v33             |      0.0029931
  v34             |      0.0006210
  v35             |      0.0032802
  v36             |      0.0035920
  v37             |      0.0125288
  v38             |      0.0004073
  v39             |      0.0030075
  v40             |      0.0012653
  v41             |      0.0009862
  v42             |      0.0057598
  v43             |      0.0015223
  v44             |      0.0018971
  v45             |      0.0105727
  v46             |      0.0194746
  v47             |      0.0002046
  v48             |      0.0014581
  v49             |      0.0053829
  v50             |      0.0118665
  v51             |      0.0026158
  v52             |      0.1169703
  v53             |      0.0948882
  v54             |      0.0030633
  v55             |      0.0665282
  v56             |      0.0559255
  v57             |      0.0391905

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2952080
  rf             |      0.7047920

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0025293
  v2              |      0.0030240
  v3              |      0.0030042
  v4              |      0.0016396
  v5              |      0.0173837
  v6              |      0.0045838
  v7              |      0.1317767
  v8              |      0.0087424
  v9              |      0.0026882
  v10             |      0.0062391
  v11             |      0.0050575
  v12             |      0.0080176
  v13             |      0.0026640
  v14             |      0.0022059
  v15             |      0.0006526
  v16             |      0.0402371
  v17             |      0.0094800
  v18             |      0.0063048
  v19             |      0.0171155
  v20             |      0.0020291
  v21             |      0.0139287
  v22             |      0.0028976
  v23             |      0.0050899
  v24             |      0.0175639
  v25             |      0.0484362
  v26             |      0.0045080
  v27             |      0.0164160
  v28             |      0.0055945
  v29             |      0.0011961
  v30             |      0.0022316
  v31             |      0.0005629
  v32             |      0.0003580
  v33             |      0.0022487
  v34             |      0.0001047
  v35             |      0.0010154
  v36             |      0.0031669
  v37             |      0.0049397
  v38             |      0.0001896
  v39             |      0.0023093
  v40             |      0.0005439
  v41             |      0.0003075
  v42             |      0.0072801
  v43             |      0.0015980
  v44             |      0.0016355
  v45             |      0.0080167
  v46             |      0.0202660
  v47             |      0.0001055
  v48             |      0.0019994
  v49             |      0.0048309
  v50             |      0.0100224
  v51             |      0.0010982
  v52             |      0.2130719
  v53             |      0.2054246
  v54             |      0.0015506
  v55             |      0.0515210
  v56             |      0.0338967
  v57             |      0.0286981

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342869
  gradboost      |      0.8657131

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033116
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0345088
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0188783
  v25             |      0.0847092
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0324790
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342868
  gradboost      |      0.8657132

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033116
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0345088
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0188783
  v25             |      0.0847092
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0324790
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342869
  gradboost      |      0.8657131

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033116
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0345088
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0188783
  v25             |      0.0847092
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0324790
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(log_loss)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(auto)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(deviance)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(None) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(log_loss)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2952080
  rf             |      0.7047920

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0025293
  v2              |      0.0030240
  v3              |      0.0030042
  v4              |      0.0016396
  v5              |      0.0173837
  v6              |      0.0045838
  v7              |      0.1317767
  v8              |      0.0087424
  v9              |      0.0026882
  v10             |      0.0062391
  v11             |      0.0050575
  v12             |      0.0080176
  v13             |      0.0026640
  v14             |      0.0022059
  v15             |      0.0006526
  v16             |      0.0402371
  v17             |      0.0094800
  v18             |      0.0063048
  v19             |      0.0171155
  v20             |      0.0020291
  v21             |      0.0139287
  v22             |      0.0028976
  v23             |      0.0050899
  v24             |      0.0175639
  v25             |      0.0484362
  v26             |      0.0045080
  v27             |      0.0164160
  v28             |      0.0055945
  v29             |      0.0011961
  v30             |      0.0022316
  v31             |      0.0005629
  v32             |      0.0003580
  v33             |      0.0022487
  v34             |      0.0001047
  v35             |      0.0010154
  v36             |      0.0031669
  v37             |      0.0049397
  v38             |      0.0001896
  v39             |      0.0023093
  v40             |      0.0005439
  v41             |      0.0003075
  v42             |      0.0072801
  v43             |      0.0015980
  v44             |      0.0016355
  v45             |      0.0080167
  v46             |      0.0202660
  v47             |      0.0001055
  v48             |      0.0019994
  v49             |      0.0048309
  v50             |      0.0100224
  v51             |      0.0010982
  v52             |      0.2130719
  v53             |      0.2054246
  v54             |      0.0015506
  v55             |      0.0515210
  v56             |      0.0338967
  v57             |      0.0286981

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342869
  gradboost      |      0.8657131

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697290
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980871
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505585
  v12             |     -0.1205224
  v13             |     -0.0258984
  v14             |      0.0469276
  v15             |      0.3218610
  v16             |      0.8646399
  v17             |      0.4195429
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225855
  v21             |      0.2921301
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780928
  v26             |     -0.9711112
  v27             |    -35.5968947
  v28             |      0.2426825
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739797
  v32             |      0.8836896
  v33             |     -0.4070675
  v34             |      0.1678275
  v35             |     -1.1232158
  v36             |      0.3752012
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787391
  v40             |     -0.1097714
  v41             |    -14.2160208
  v42             |     -2.0756739
  v43             |     -0.2731491
  v44             |     -0.9848928
  v45             |     -0.7940638
  v46             |     -1.2974388
  v47             |     -0.1757828
  v48             |     -1.1157714
  v49             |     -0.3168070
  v50             |     -0.0501776
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153391
  v54             |      1.0699341
  v55             |      0.2032141
  v56             |      1.7614050
  v57             |      0.5055233
  _cons           |    -10.9866952
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033116
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0345088
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0188783
  v25             |      0.0847092
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0324790
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.7936782
  elasticcv      |      0.2063218

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.0939449
  v2              |     -0.2087687
  v3              |      0.0711424
  v4              |      0.8322985
  v5              |      0.3674088
  v6              |      0.1750651
  v7              |      0.9487451
  v8              |      0.2255434
  v9              |      0.1549128
  v10             |      0.0722846
  v11             |     -0.0456211
  v12             |     -0.1298287
  v13             |     -0.0301083
  v14             |      0.0348886
  v15             |      0.2481631
  v16             |      0.8418445
  v17             |      0.4100889
  v18             |      0.0735661
  v19             |      0.1492821
  v20             |      0.4687352
  v21             |      0.2815580
  v22             |      0.2919727
  v23             |      0.8978450
  v24             |      0.2373671
  v25             |     -2.6219283
  v26             |     -1.1052138
  v27             |     -3.7565029
  v28             |      0.2355017
  v29             |     -1.0034278
  v30             |     -0.1501052
  v31             |     -0.5491865
  v32             |     -0.1919725
  v33             |     -0.4998247
  v34             |     -0.2065952
  v35             |     -0.9856720
  v36             |      0.3640279
  v37             |      0.0026734
  v38             |     -0.1276458
  v39             |     -0.3325274
  v40             |     -0.1807252
  v41             |     -1.2653273
  v42             |     -1.4913100
  v43             |     -0.2290622
  v44             |     -0.9304369
  v45             |     -0.8125845
  v46             |     -1.2804159
  v47             |     -0.1668555
  v48             |     -0.9061920
  v49             |     -0.3282946
  v50             |     -0.0572943
  v51             |     -0.1362474
  v52             |      0.3436530
  v53             |      1.3519903
  v54             |      0.8501562
  v55             |     -0.0810669
  v56             |      1.2900648
  v57             |      0.4166264
  _cons           |     -2.5433030
Note: Coefficients correspond to decision boundary function.
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(log_loss)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4815436
  gradboost      |      0.3446398
  nnet           |      0.1131333
  nnet           |      0.0606833

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3594720
  rf             |      0.6405280

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1

Iteration 0:   log likelihood = -230.91815  
Iteration 1:   log likelihood = -221.30531  
Iteration 2:   log likelihood = -221.28762  
Iteration 3:   log likelihood = -221.28762  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2570104
  ridgecv        |      0.7429896

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1145574
  v2              |     -0.1924022
  v3              |      0.0588296
  v4              |      3.0810159
  v5              |      0.3789061
  v6              |      0.2334238
  v7              |      0.8969641
  v8              |      0.2276950
  v9              |      0.1892637
  v10             |      0.0820554
  v11             |     -0.0512704
  v12             |     -0.1220134
  v13             |     -0.0239618
  v14             |      0.0460284
  v15             |      0.3179723
  v16             |      0.8626056
  v17             |      0.4202606
  v18             |      0.0638222
  v19             |      0.1429571
  v20             |      0.5323186
  v21             |      0.2923602
  v22             |      0.2163208
  v23             |      0.7860330
  v24             |      0.1926794
  v25             |     -3.2122374
  v26             |     -0.9382799
  v27             |    -31.3491909
  v28             |      0.2407730
  v29             |     -1.4883588
  v30             |     -0.1492365
  v31             |     -0.0772478
  v32             |      0.7407408
  v33             |     -0.4217337
  v34             |      0.1948398
  v35             |     -1.0935624
  v36             |      0.3757142
  v37             |      0.0171436
  v38             |     -0.1315938
  v39             |     -0.3739398
  v40             |     -0.1067307
  v41             |    -10.9923048
  v42             |     -2.0270583
  v43             |     -0.2715974
  v44             |     -0.9808852
  v45             |     -0.7978563
  v46             |     -1.3281751
  v47             |     -0.1770927
  v48             |     -1.1143916
  v49             |     -0.3171293
  v50             |     -0.0490969
  v51             |     -0.0723733
  v52             |      0.2871549
  v53             |      1.3100064
  v54             |      1.0505126
  v55             |      0.0394479
  v56             |      1.7200868
  v57             |      0.5017290
  _cons           |     -9.8064998
Note: Coefficients correspond to decision boundary function.

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:   log likelihood = -230.47022  
Iteration 1:   log likelihood = -217.54677  
Iteration 2:   log likelihood = -217.49653  
Iteration 3:   log likelihood = -217.49645  
Iteration 4:   log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4815155
  gradboost      |      0.3446655
  nnet           |      0.1131199
  nnet           |      0.0606992

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4815155
  gradboost      |      0.3446655
  nnet           |      0.1131199
  nnet           |      0.0606992

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3591151
  rf             |      0.6408849

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3591150
  rf             |      0.6408850

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1

Iteration 0:   log likelihood = -230.91815  
Iteration 1:   log likelihood = -221.30531  
Iteration 2:   log likelihood = -221.28762  
Iteration 3:   log likelihood = -221.28762  
Iteration 0:   log likelihood = -230.91815  
Iteration 1:   log likelihood = -221.30531  
Iteration 2:   log likelihood = -221.28762  
Iteration 3:   log likelihood = -221.28762  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417


Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:   log likelihood = -230.47022  
Iteration 1:   log likelihood = -217.54677  
Iteration 2:   log likelihood = -217.49653  
Iteration 3:   log likelihood = -217.49645  
Iteration 4:   log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:   log likelihood = -230.47022  
Iteration 1:   log likelihood = -217.54677  
Iteration 2:   log likelihood = -217.49653  
Iteration 3:   log likelihood = -217.49645  
Iteration 4:   log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1556822
  rf             |      0.8443178

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0042850
  v2              |      0.0049748
  v3              |      0.0132401
  v4              |      0.0010365
  v5              |      0.0318705
  v6              |      0.0073465
  v7              |      0.0699817
  v8              |      0.0098464
  v9              |      0.0050108
  v10             |      0.0088243
  v11             |      0.0075686
  v12             |      0.0114366
  v13             |      0.0035312
  v14             |      0.0023556
  v15             |      0.0019670
  v16             |      0.0654783
  v17             |      0.0097685
  v18             |      0.0079339
  v19             |      0.0319235
  v20             |      0.0042825
  v21             |      0.0628539
  v22             |      0.0022870
  v23             |      0.0246193
  v24             |      0.0389478
  v25             |      0.0447852
  v26             |      0.0179915
  v27             |      0.0268937
  v28             |      0.0051752
  v29             |      0.0018855
  v30             |      0.0041700
  v31             |      0.0007973
  v32             |      0.0009296
  v33             |      0.0029931
  v34             |      0.0006210
  v35             |      0.0032802
  v36             |      0.0035920
  v37             |      0.0125288
  v38             |      0.0004073
  v39             |      0.0030075
  v40             |      0.0012653
  v41             |      0.0009862
  v42             |      0.0057598
  v43             |      0.0015223
  v44             |      0.0018971
  v45             |      0.0105727
  v46             |      0.0194746
  v47             |      0.0002046
  v48             |      0.0014581
  v49             |      0.0053829
  v50             |      0.0118665
  v51             |      0.0026158
  v52             |      0.1169703
  v53             |      0.0948882
  v54             |      0.0030633
  v55             |      0.0665282
  v56             |      0.0559255
  v57             |      0.0391905

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1532334
  lassocv        |      0.0000000
  gradboost      |      0.8467666

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .02161697 |
  2. | .01309867 |
  3. | .00747753 |
  4. | .96338004 |
  5. | .98903048 |
     |-----------|
  6. | .01358793 |
  7. | .10357895 |
  8. | .98448455 |
  9. | .07360937 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:   log likelihood = -461.39142  
Iteration 1:   log likelihood = -438.05678  
Iteration 2:   log likelihood = -437.95577  
Iteration 3:   log likelihood = -437.95561  
Iteration 4:   log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1532334
  lassocv        |      0.0000000
  gradboost      |      0.8467666

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .02161697 |
  2. | .01309867 |
  3. | .00747753 |
  4. | .96338004 |
  5. | .98903048 |
     |-----------|
  6. | .01358793 |
  7. | .10357896 |
  8. | .98448455 |
  9. | .07360937 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:   log likelihood = -461.39142  
Iteration 1:   log likelihood = -438.05678  
Iteration 2:   log likelihood = -437.95577  
Iteration 3:   log likelihood = -437.95561  
Iteration 4:   log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1532334
  lassocv        |      0.0000000
  gradboost      |      0.8467666

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .02161697 |
  2. | .01309867 |
  3. | .00747753 |
  4. | .96338004 |
  5. | .98903048 |
     |-----------|
  6. | .01358793 |
  7. | .10357896 |
  8. | .98448455 |
  9. | .07360937 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:   log likelihood = -461.39142  
Iteration 1:   log likelihood = -438.05678  
Iteration 2:   log likelihood = -437.95577  
Iteration 3:   log likelihood = -437.95561  
Iteration 4:   log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4815155
  gradboost      |      0.3446655
  nnet           |      0.1131199
  nnet           |      0.0606992

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1342868
  gradboost      |      0.8657132

Coefficients logit:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |     -0.1179572
  v2              |     -0.1900100
  v3              |      0.0581991
  v4              |      5.3697303
  v5              |      0.3795651
  v6              |      0.2380471
  v7              |      0.8980867
  v8              |      0.2277181
  v9              |      0.1992870
  v10             |      0.0830519
  v11             |     -0.0505586
  v12             |     -0.1205225
  v13             |     -0.0258984
  v14             |      0.0469277
  v15             |      0.3218611
  v16             |      0.8646400
  v17             |      0.4195431
  v18             |      0.0639256
  v19             |      0.1425001
  v20             |      0.5225853
  v21             |      0.2921300
  v22             |      0.2075707
  v23             |      0.7907714
  v24             |      0.1876250
  v25             |     -3.1780935
  v26             |     -0.9711111
  v27             |    -35.5968940
  v28             |      0.2426830
  v29             |     -1.2685532
  v30             |     -0.1515460
  v31             |     -0.0739790
  v32             |      0.8836900
  v33             |     -0.4070674
  v34             |      0.1678277
  v35             |     -1.1232159
  v36             |      0.3752013
  v37             |      0.0178832
  v38             |     -0.1347785
  v39             |     -0.3787390
  v40             |     -0.1097716
  v41             |    -14.2160206
  v42             |     -2.0756741
  v43             |     -0.2731490
  v44             |     -0.9848927
  v45             |     -0.7940639
  v46             |     -1.2974386
  v47             |     -0.1757827
  v48             |     -1.1157715
  v49             |     -0.3168070
  v50             |     -0.0501777
  v51             |     -0.0725830
  v52             |      0.2863580
  v53             |      1.3153389
  v54             |      1.0699341
  v55             |      0.2032137
  v56             |      1.7614052
  v57             |      0.5055235
  _cons           |    -10.9866950
Note: Coefficients correspond to decision boundary function.

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  v1              |      0.0000000
  v2              |      0.0000000
  v3              |      0.0000014
  v4              |      0.0003211
  v5              |      0.0141055
  v6              |      0.0012757
  v7              |      0.1363464
  v8              |      0.0033116
  v9              |      0.0001984
  v10             |      0.0004976
  v11             |      0.0025075
  v12             |      0.0017850
  v13             |      0.0000000
  v14             |      0.0014866
  v15             |      0.0000000
  v16             |      0.0573990
  v17             |      0.0036032
  v18             |      0.0023826
  v19             |      0.0035776
  v20             |      0.0009582
  v21             |      0.0345088
  v22             |      0.0011098
  v23             |      0.0045976
  v24             |      0.0188783
  v25             |      0.0847092
  v26             |      0.0008163
  v27             |      0.0308788
  v28             |      0.0058720
  v29             |      0.0000000
  v30             |      0.0002018
  v31             |      0.0000000
  v32             |      0.0000000
  v33             |      0.0000653
  v34             |      0.0000000
  v35             |      0.0000000
  v36             |      0.0004029
  v37             |      0.0065607
  v38             |      0.0000000
  v39             |      0.0001810
  v40             |      0.0001046
  v41             |      0.0000000
  v42             |      0.0073078
  v43             |      0.0001850
  v44             |      0.0003317
  v45             |      0.0029492
  v46             |      0.0283713
  v47             |      0.0000000
  v48             |      0.0004310
  v49             |      0.0017513
  v50             |      0.0015111
  v51             |      0.0000000
  v52             |      0.2294428
  v53             |      0.2104799
  v54             |      0.0000000
  v55             |      0.0568419
  v56             |      0.0324790
  v57             |      0.0092734

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3591150
  rf             |      0.6408850

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1

Iteration 0:   log likelihood = -230.91815  
Iteration 1:   log likelihood = -221.30531  
Iteration 2:   log likelihood = -221.28762  
Iteration 3:   log likelihood = -221.28762  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:   log likelihood = -230.47022  
Iteration 1:   log likelihood = -217.54677  
Iteration 2:   log likelihood = -217.49653  
Iteration 3:   log likelihood = -217.49645  
Iteration 4:   log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)
(58 vars, 4,601 obs)

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked v58 v1-v57, type(class) m(logit `meth') printopt
  3. }

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: lassocv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l1)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: elasticcv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(elasticnet)  solver(saga)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng)  l1_ratios(0 .5 1) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: ridgecv

 Stata syntax:
 cs(integer) noconstant  penalty(string) solver(string)  tol(real)  max_iter(integer) n_jobs(integer) 
 norefit intercept_scaling(real)  random_state(integer)

 Specified options are translated to: 
 Cs(10)  fit_intercept(True)  penalty(l2)  solver(newton-cg)  tol(.0001)  max_iter(100) 
 n_jobs(None)  refit(True)  intercept_scaling(1)  random_state(rng) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2) 
 min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1) 
 min_impurity_decrease(real 0)  bootstrap(string)  oob_score  n_jobs(integer 0) 
 random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(integer -1)

 Specified options are translated to: 
 n_estimators(100)  criterion(gini)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(auto)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: logit

 Stata syntax:
 noconstant penalty(string)

 Specified options are translated to: 
 fit_intercept(True)  penalty(none) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1)  criterion(string) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  max_leaf_nodes(integer -1) 
 warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1)  tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(deviance)  learning_rate(.1)  n_estimators(100)  criterion(friedman_mse)  subsample(1)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None)  tol(.0001) 
 ccp_alpha(0) 

. 
.  
.  
. *******************************************************************************
. *** from SJ paper                                                                                                                       ***
. *******************************************************************************
. 
.  insheet using ///
>  https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data, ///
>  clear comma
(58 vars, 4,601 obs)

. set seed 42

. gen train=runiform()

. replace train=train<.75
(4,601 real changes made)

.  
.  pystacked v58 v1-v57                           || ///
>     m(logit) pipe(poly2)                        || ///
>     m(gradboost) opt(n_estimators(600))         || ///
>     m(gradboost) opt(n_estimators(1000))        || ///
>     m(nnet) opt(hidden_layer_sizes(5 5))        || ///
>     m(nnet) opt(hidden_layer_sizes(5))          || ///
>     if train, type(class) njobs(8) backend(threading) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1532334
  lassocv        |      0.0000000
  gradboost      |      0.8467666

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .02161697 |
  2. | .01309867 |
  3. | .00747753 |
  4. | .96338004 |
  5. | .98903048 |
     |-----------|
  6. | .01358793 |
  7. | .10357896 |
  8. | .98448455 |
  9. | .07360937 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:   log likelihood = -461.39142  
Iteration 1:   log likelihood = -438.05678  
Iteration 2:   log likelihood = -437.95577  
Iteration 3:   log likelihood = -437.95561  
Iteration 4:   log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0807566
  lassocv        |      0.0000000
  gradboost      |      0.6112319
  nnet           |      0.3080115
logit lassocv rf nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0807563
  lassocv        |      0.0000000
  gradboost      |      0.6112265
  nnet           |      0.3080172
logit lassocv rf nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0807563
  lassocv        |      0.0000000
  gradboost      |      0.6112265
  nnet           |      0.3080172
logit lassocv rf nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0011912
  lassocv        |      0.0000000
  rf             |      0.4785094
  nnet           |      0.5202994
logit ridgecv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0602403
  lassocv        |      0.0000000
  rf             |      0.4216790
  nnet           |      0.5180808
logit ridgecv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0602404
  lassocv        |      0.0000000
  rf             |      0.4216810
  nnet           |      0.5180786
logit ridgecv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0807563
  lassocv        |      0.0000000
  gradboost      |      0.6112265
  nnet           |      0.3080172
logit lassocv rf nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0602403
  lassocv        |      0.0000000
  rf             |      0.4216790
  nnet           |      0.5180808
logit ridgecv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0340911
  gradboost      |      0.4721158
  nnet           |      0.4937931
logit elasticcv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0340931
  gradboost      |      0.4721140
  nnet           |      0.4937929
logit elasticcv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0340931
  gradboost      |      0.4721140
  nnet           |      0.4937929
logit elasticcv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803025
  nnet           |      0.4196975
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803025
  nnet           |      0.4196975
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803007
  nnet           |      0.4196993
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0340931
  gradboost      |      0.4721140
  nnet           |      0.4937929
logit elasticcv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  gradboost      |      0.4815436
  gradboost      |      0.3446398
  nnet           |      0.1131333
  nnet           |      0.0606833

.         
. mat W = e(weights)

. local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)+el(W,5,1)

. assert reldif(`sum',1)<0.001

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. gen fid = 1 + (_n>345)

.         
. pystacked v58 v1 v2 v3, method(logit rf) foldvar(fid) type(class)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3594720
  rf             |      0.6405280

. predict yb , basexb cvalid

. 
. logit v58 v1 v2 v3 if fid==1

Iteration 0:   log likelihood = -230.91815  
Iteration 1:   log likelihood = -221.30531  
Iteration 2:   log likelihood = -221.28762  
Iteration 3:   log likelihood = -221.28762  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  19.26
                                                        Prob > chi2   = 0.0002
Log likelihood = -221.28762                             Pseudo R2     = 0.0417

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   2.022795   .6649857     3.04   0.002     .7194464    3.326143
          v2 |  -.0229183   .1105851    -0.21   0.836    -.2396612    .1938246
          v3 |     .49605     .20824     2.38   0.017     .0879071    .9041928
       _cons |  -.7599564   .1407462    -5.40   0.000    -1.035814    -.484099
------------------------------------------------------------------------------

. predict log1 if fid ==2
(option pr assumed; Pr(v58))
(345 missing values generated)

. logit v58 v1 v2 v3 if fid==2

Iteration 0:   log likelihood = -230.47022  
Iteration 1:   log likelihood = -217.54677  
Iteration 2:   log likelihood = -217.49653  
Iteration 3:   log likelihood = -217.49645  
Iteration 4:   log likelihood = -217.49645  

Logistic regression                                     Number of obs =    345
                                                        LR chi2(3)    =  25.95
                                                        Prob > chi2   = 0.0000
Log likelihood = -217.49645                             Pseudo R2     = 0.0563

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
          v1 |   1.083844   .4417874     2.45   0.014     .2179562    1.949731
          v2 |    .464437   .2444287     1.90   0.057    -.0146345    .9435085
          v3 |   .8830596   .2636705     3.35   0.001     .3662749    1.399844
       _cons |  -.8840127   .1454878    -6.08   0.000    -1.169164   -.5988618
------------------------------------------------------------------------------

. predict log2 if fid==1
(option pr assumed; Pr(v58))
(345 missing values generated)

. gen double log_crossfit = log1 if fid==2
(345 missing values generated)

. replace log_crossfit = log2 if fid==1
(345 real changes made)

. 
. assert reldif(log_crossfit , yb1)<10e-4

. 
. 
. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
.                                                  
. use `testdata', clear

. 
. pystacked $model, type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1574003
  elasticcv      |      0.0000000
  gradboost      |      0.4810855
  svm            |      0.3615142


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1574005
  elasticcv      |      0.0000000
  gradboost      |      0.4810864
  svm            |      0.3615132


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1574005
  elasticcv      |      0.0000000
  gradboost      |      0.4810864
  svm            |      0.3615132


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2031120
  lassocv        |      0.0000000
  gradboost      |      0.7968880

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2031101
  lassocv        |      0.0000000
  gradboost      |      0.7968899

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803025
  nnet           |      0.4196975
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1532334
  lassocv        |      0.0000000
  gradboost      |      0.8467666

. 
. predict double yhat, pr

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .02161697 |
  2. | .01309867 |
  3. | .00747753 |
  4. | .96338004 |
  5. | .98903048 |
     |-----------|
  6. | .01358793 |
  7. | .10357895 |
  8. | .98448455 |
  9. | .07360937 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. *******************************************************************************
. *** only one predictor                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. pystacked v58 v57, type(class) m(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict double xhat1

. 
. logit v58 v57

Iteration 0:   log likelihood = -461.39142  
Iteration 1:   log likelihood = -438.05678  
Iteration 2:   log likelihood = -437.95577  
Iteration 3:   log likelihood = -437.95561  
Iteration 4:   log likelihood = -437.95561  

Logistic regression                                     Number of obs =    690
                                                        LR chi2(1)    =  46.87
                                                        Prob > chi2   = 0.0000
Log likelihood = -437.95561                             Pseudo R2     = 0.0508

------------------------------------------------------------------------------
         v58 | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]
-------------+----------------------------------------------------------------
         v57 |   .0013024   .0002272     5.73   0.000     .0008571    .0017477
       _cons |  -.7951655   .0972236    -8.18   0.000    -.9857203   -.6046107
------------------------------------------------------------------------------

. predict double xhat2 
(option pr assumed; Pr(v58))

. 
. assert reldif(xhat1,xhat2)<0.0001

. 
. *******************************************************************************
. *** predicted values/classes                                                                                            ***
. *******************************************************************************
. 
. cap drop yhat*

. 
. pystacked $model, type(class) methods(logit)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      1.0000000

. predict yhat , class

. predict yhat2  

. predict yhat3 , pr

. assert yhat>0 if yhat3>0.5

. assert yhat<1 if yhat3<0.5

. assert yhat2>0 if yhat3>0.5

. assert yhat2<1 if yhat3<0.5

. 
. *******************************************************************************
. *** try voting                                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

.                         
. pystacked $model, type(class) pyseed(123) ///
>                                                         methods(lassocv rf logit) /// 
>                                                         njobs(4) pipe1(poly2) ///
>                                                         voting voteweights(0.1 .4) ///
>                                                         votetype(soft)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2031101
  lassocv        |      0.0000000
  gradboost      |      0.7968899

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0637121
  rf             |      0.4239032
  gradboost      |      0.5123848

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417       9         .       .         .       .
  STACKING     1 |    .         4     260         .       .         .       .
  logit        0 | 0.064      399      39       389      49         .       .
  logit        1 | 0.064       22     230        32     220         .       .
  rf           0 | 0.424      421       1       395      34         .       .
  rf           1 | 0.424        0     268        26     235         .       .
  gradboost    0 | 0.512      416      13       398      34         .       .
  gradboost    1 | 0.512        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1419184
  rf             |      0.3417368
  gradboost      |      0.5163447

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417      13         .       .         .       .
  STACKING     1 |    .         4     256         .       .         .       .
  logit        0 | 0.142      399      39       389      49         .       .
  logit        1 | 0.142       22     230        32     220         .       .
  rf           0 | 0.342      421       1       393      36         .       .
  rf           1 | 0.342        0     268        28     233         .       .
  gradboost    0 | 0.516      416      13       398      34         .       .
  gradboost    1 | 0.516        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4067909
  rf             |      0.3144122
  gradboost      |      0.2787970

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .       203      19
  STACKING     1 |    .         1     118         .       .        15     128
  logit        0 | 0.407      198       8       183      16       200      20
  logit        1 | 0.407        5     114        20     106        18     127
  rf           0 | 0.314      203       1       193      22       206      22
  rf           1 | 0.314        0     121        10     100        12     125
  gradboost    0 | 0.279      203       4       196      24       203      24
  gradboost    1 | 0.279        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .        96      11
  STACKING     1 |    .         1     118         .       .         5      63
  logit        0 | 0.407      198       8       183      16        93      11
  logit        1 | 0.407        5     114        20     106         8      63
  rf           0 | 0.314      203       1       193      22        97      13
  rf           1 | 0.314        0     121        10     100         4      61
  gradboost    0 | 0.279      203       4       196      24        96      13
  gradboost    1 | 0.279        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4227930
  rf             |      0.1819019
  gradboost      |      0.3953051

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       204      19
  STACKING     1 |    .         2     118         .       .        14     128
  logit        0 | 0.423      198       8       183      16       200      20
  logit        1 | 0.423        5     114        20     106        18     127
  rf           0 | 0.182      203       1       190      21       206      22
  rf           1 | 0.182        0     121        13     101        12     125
  gradboost    0 | 0.395      203       4       196      24       203      24
  gradboost    1 | 0.395        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .        96      11
  STACKING     1 |    .         2     118         .       .         5      63
  logit        0 | 0.423      198       8       183      16        93      11
  logit        1 | 0.423        5     114        20     106         8      63
  rf           0 | 0.182      203       1       190      21        96      13
  rf           1 | 0.182        0     121        13     101         5      61
  gradboost    0 | 0.395      203       4       196      24        96      13
  gradboost    1 | 0.395        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       200      17
  STACKING     1 |    .         2     118         .       .        18     130
  logit        0 | 0.536      198       8       184      15       200      20
  logit        1 | 0.536        5     114        19     107        18     127
  rf           0 | 0.168      203       1       192      23       206      22
  rf           1 | 0.168        0     121        11      99        12     125
  gradboost    0 | 0.297      203       4       192      24       203      24
  gradboost    1 | 0.297        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1419178
  rf             |      0.3417348
  gradboost      |      0.5163474

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417      13         .       .         .       .
  STACKING     1 |    .         4     256         .       .         .       .
  logit        0 | 0.142      399      39       389      49         .       .
  logit        1 | 0.142       22     230        32     220         .       .
  rf           0 | 0.342      421       1       393      36         .       .
  rf           1 | 0.342        0     268        28     233         .       .
  gradboost    0 | 0.516      416      13       398      34         .       .
  gradboost    1 | 0.516        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4658911
  rf             |      0.5341089
  gradboost      |      0.0000000

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .       202      16
  STACKING     1 |    .         1     118         .       .        16     131
  logit        0 | 0.466      196       7       188      22       200      21
  logit        1 | 0.466        7     115        15     100        18     126
  rf           0 | 0.534      203       1       189      21       206      22
  rf           1 | 0.534        0     121        14     101        12     125
  gradboost    0 | 0.000      203       4       193      24       203      24
  gradboost    1 | 0.000        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1574005
  elasticcv      |      0.0000000
  gradboost      |      0.4810864
  svm            |      0.3615132

Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       6         .       .       200      17
  STACKING     1 |    .         2     116         .       .        18     130
  logit        0 | 0.555      198       8       184      15       200      20
  logit        1 | 0.555        5     114        19     107        18     127
  rf           0 | 0.000      203       1       189      23       206      22
  rf           1 | 0.000        0     121        14      99        12     125
  gradboost    0 | 0.445      203       4       192      24       203      24
  gradboost    1 | 0.445        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4227930
  rf             |      0.1819019
  gradboost      |      0.3953051

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       204      19
  STACKING     1 |    .         2     118         .       .        14     128
  logit        0 | 0.423      198       8       183      16       200      20
  logit        1 | 0.423        5     114        20     106        18     127
  rf           0 | 0.182      203       1       190      21       206      22
  rf           1 | 0.182        0     121        13     101        12     125
  gradboost    0 | 0.395      203       4       196      24       203      24
  gradboost    1 | 0.395        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .        96      11
  STACKING     1 |    .         2     118         .       .         5      63
  logit        0 | 0.423      198       8       183      16        93      11
  logit        1 | 0.423        5     114        20     106         8      63
  rf           0 | 0.182      203       1       190      21        96      13
  rf           1 | 0.182        0     121        13     101         5      61
  gradboost    0 | 0.395      203       4       196      24        96      13
  gradboost    1 | 0.395        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1782491
  rf             |      0.3574248
  gradboost      |      0.4643262

. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.5390052
  rf             |      0.1529283
  gradboost      |      0.3080665

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       5         .       .       200      18
  STACKING     1 |    .         2     117         .       .        18     129
  logit        0 | 0.539      196       7       188      22       200      21
  logit        1 | 0.539        7     115        15     100        18     126
  rf           0 | 0.153      203       1       187      21       206      22
  rf           1 | 0.153        0     121        16     101        12     125
  gradboost    0 | 0.308      203       4       193      24       203      24
  gradboost    1 | 0.308        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2031101
  lassocv        |      0.0000000
  gradboost      |      0.7968899

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       6         .       .       200      17
  STACKING     1 |    .         2     116         .       .        18     130
  logit        0 | 0.555      198       8       184      15       200      20
  logit        1 | 0.555        5     114        19     107        18     127
  rf           0 | 0.000      203       1       189      23       206      22
  rf           1 | 0.000        0     121        14      99        12     125
  gradboost    0 | 0.445      203       4       192      24       203      24
  gradboost    1 | 0.445        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1419184
  rf             |      0.3417368
  gradboost      |      0.5163447

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417      13         .       .         .       .
  STACKING     1 |    .         4     256         .       .         .       .
  logit        0 | 0.142      399      39       389      49         .       .
  logit        1 | 0.142       22     230        32     220         .       .
  rf           0 | 0.342      421       1       393      36         .       .
  rf           1 | 0.342        0     268        28     233         .       .
  gradboost    0 | 0.516      416      13       398      34         .       .
  gradboost    1 | 0.516        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2222832
  rf             |      0.1008356
  gradboost      |      0.6768812

. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.5390052
  rf             |      0.1529283
  gradboost      |      0.3080665

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       5         .       .       200      18
  STACKING     1 |    .         2     117         .       .        18     129
  logit        0 | 0.539      196       7       188      22       200      21
  logit        1 | 0.539        7     115        15     100        18     126
  rf           0 | 0.153      203       1       187      21       206      22
  rf           1 | 0.153        0     121        16     101        12     125
  gradboost    0 | 0.308      203       4       193      24       203      24
  gradboost    1 | 0.308        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4227930
  rf             |      0.1819019
  gradboost      |      0.3953051

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       204      19
  STACKING     1 |    .         2     118         .       .        14     128
  logit        0 | 0.423      198       8       183      16       200      20
  logit        1 | 0.423        5     114        20     106        18     127
  rf           0 | 0.182      203       1       190      21       206      22
  rf           1 | 0.182        0     121        13     101        12     125
  gradboost    0 | 0.395      203       4       196      24       203      24
  gradboost    1 | 0.395        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .        96      11
  STACKING     1 |    .         2     118         .       .         5      63
  logit        0 | 0.423      198       8       183      16        93      11
  logit        1 | 0.423        5     114        20     106         8      63
  rf           0 | 0.182      203       1       190      21        96      13
  rf           1 | 0.182        0     121        13     101         5      61
  gradboost    0 | 0.395      203       4       196      24        96      13
  gradboost    1 | 0.395        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4110651
  rf             |      0.5168489
  gradboost      |      0.0720860

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       6         .       .       200      17
  STACKING     1 |    .         2     116         .       .        18     130
  logit        0 | 0.555      198       8       184      15       200      20
  logit        1 | 0.555        5     114        19     107        18     127
  rf           0 | 0.000      203       1       189      23       206      22
  rf           1 | 0.000        0     121        14      99        12     125
  gradboost    0 | 0.445      203       4       192      24       203      24
  gradboost    1 | 0.445        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2222832
  rf             |      0.1008356
  gradboost      |      0.6768812

. pystacked, graph

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.5390052
  rf             |      0.1529283
  gradboost      |      0.3080665

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       5         .       .       200      18
  STACKING     1 |    .         2     117         .       .        18     129
  logit        0 | 0.539      196       7       188      22       200      21
  logit        1 | 0.539        7     115        15     100        18     126
  rf           0 | 0.153      203       1       187      21       206      22
  rf           1 | 0.153        0     121        16     101        12     125
  gradboost    0 | 0.308      203       4       193      24       203      24
  gradboost    1 | 0.308        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      0.1000000
  rf             |      0.4000000
  logit          |      0.5000000

. mat W = e(weights)

. assert reldif(0.1,el(W,1,1))<0.0001

. assert reldif(0.4,el(W,2,1))<0.0001

. assert reldif(0.5,el(W,3,1))<0.0001

.  
. 
. *******************************************************************************
. *** try other estimators                                                                                                        ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. local m1 logit lassocv gradboost nnet

. local m2 logit lassocv rf nnet

. local m3 logit ridgecv gradboost nnet

. local m4 logit elasticcv gradboost nnet

. local m5 logit elasticcv gradboost svm

. 
. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" "`m6'" {
  2.         di "`m'"
  3.         pystacked $model, type(class) pyseed(123) ///
>                                                         methods(`m') /// 
>                                                         njobs(4)
  4. }
logit lassocv gradboost nnet

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4649098
  rf             |      0.3468785
  gradboost      |      0.1882117

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2222832
  rf             |      0.1008356
  gradboost      |      0.6768812

. pystacked, graph

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4649098
  rf             |      0.3468797
  gradboost      |      0.1882105

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0807566
  lassocv        |      0.0000000
  gradboost      |      0.6112319
  nnet           |      0.3080115
logit lassocv rf nnet

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4649098
  rf             |      0.3468785
  gradboost      |      0.1882117

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0011912
  lassocv        |      0.0000000
  rf             |      0.4785094
  nnet           |      0.5202994
logit ridgecv gradboost nnet

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0340911
  gradboost      |      0.4721158
  nnet           |      0.4937931
logit elasticcv gradboost nnet

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3833952
  rf             |      0.3670662
  gradboost      |      0.2495385

. pystacked, graph holdout
Number of holdout observations:  365

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365
Number of holdout observations:  365

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.0.2'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.5803025
  nnet           |      0.4196975
logit elasticcv gradboost svm

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4265024
  rf             |      0.0974954
  gradboost      |      0.4760021

. pystacked, graph holdout
Number of holdout observations:  365
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = 
> sqrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm
> _start = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1;
>  min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; 
> max_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; ran
> dom_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_stat
> e = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random
> _state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; de
> cision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)
(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state =
>  RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  linsvm         |      1.0000000

. 
. pystacked $model, method(svm) ///
>                         cmdopt1( ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.1) ///
>                         tol(1e-2) ///
>                         c(1.1) ///
>                         epsilon(0.14) ///
>                         shrinking(False) ///
>                         max_iter(10) ///
>                         ) showopt       

Base learner: svm
kernel = poly; degree = 2; gamma = auto; coef0 = 0.1; tol = 0.01; C = 1.1; epsilon = 0.14; max_iter = 10; shrinking = False; cache_size = 200; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

. 
. pystacked $model, method(nnet) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.5; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(lassocv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.1) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt bfolds(3) 

Base learner: lassocv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 1; eps = 0.1; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = True
> ; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.01) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt  bfolds(3)    

Base learner: ridgecv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 0; eps = 0.01; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = Tru
> e; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

.                         
. pystacked $model, method(gradboost) ///
>                         cmdopt1( ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         alpha(.8) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = squared_error; learning_rate = 0.2; n_estimators = 400; subsample = 0.8; criterion = friedman_mse; min_samples_split = 5; min_samples_leaf = 0.
> 1; min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt
> ; alpha = 0.8; max_leaf_nodes = 4; warm_start = True; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(rf) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = squared_error; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_featu
> res = sqrt; max_leaf_nodes = 4; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = None; random_state = RandomState(MT19937)
> ; warm_start = True; ccp_alpha = 0; max_samples = None; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

.  
. 
end of do-file

. do "cs_pystacked_reg.do"

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.0.2'

. 
. global xvars crim-lstat

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. pystacked medh longi-medi 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4265024
  rf             |      0.0974954
  gradboost      |      0.4760021

. pystacked, graph holdout
Number of holdout observations:  365

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.2.1'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = 
> sqrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm
> _start = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1;
>  min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; 
> max_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; ran
> dom_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_stat
> e = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random
> _state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; de
> cision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4265024
  rf             |      0.0974954
  gradboost      |      0.4760021

. pystacked, graph holdout
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1574005
  elasticcv      |      0.0000000
  gradboost      |      0.4810850
  svm            |      0.3615145

(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state =
>  RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  linsvm         |      1.0000000

. 
. pystacked $model, method(svm) ///
>                         cmdopt1( ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.1) ///
>                         tol(1e-2) ///
>                         c(1.1) ///
>                         epsilon(0.14) ///
>                         shrinking(False) ///
>                         max_iter(10) ///
>                         ) showopt       

Base learner: svm
kernel = poly; degree = 2; gamma = auto; coef0 = 0.1; tol = 0.01; C = 1.1; epsilon = 0.14; max_iter = 10; shrinking = False; cache_size = 200; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

. 
. pystacked $model, method(nnet) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.5; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(lassocv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.1) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt bfolds(3) 

Base learner: lassocv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 1; eps = 0.1; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = True
> ; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.01) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt  bfolds(3)    

Base learner: ridgecv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 0; eps = 0.01; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = Tru
> e; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

.                         
. pystacked $model, method(gradboost) ///
>                         cmdopt1( ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         alpha(.8) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = squared_error; learning_rate = 0.2; n_estimators = 400; subsample = 0.8; criterion = friedman_mse; min_samples_split = 5; min_samples_leaf = 0.
> 1; min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt
> ; alpha = 0.8; max_leaf_nodes = 4; warm_start = True; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(rf) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = squared_error; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_featu
> res = sqrt; max_leaf_nodes = 4; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = None; random_state = RandomState(MT19937)
> ; warm_start = True; ccp_alpha = 0; max_samples = None; 

Single base learner: no stacking or cross-validation done.

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

.  
. 
end of do-file

. do "cs_pystacked_reg.do"

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.2.1'

. 
. global xvars crim-lstat

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. pystacked medh longi-medi 
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = 
> sqrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm
> _start = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1;
>  min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; 
> max_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2031120
  lassocv        |      0.0000000
  gradboost      |      0.7968880

. 
. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; ran
> dom_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_stat
> e = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random
> _state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; de
> cision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.1.3'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state =
>  RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  linsvm         |      1.0000000

. 
. pystacked $model, method(svm) ///
>                         cmdopt1( ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.1) ///
>                         tol(1e-2) ///
>                         c(1.1) ///
>                         epsilon(0.14) ///
>                         shrinking(False) ///
>                         max_iter(10) ///
>                         ) showopt       

Base learner: svm
kernel = poly; degree = 2; gamma = auto; coef0 = 0.1; tol = 0.01; C = 1.1; epsilon = 0.14; max_iter = 10; shrinking = False; cache_size = 200; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

. 
. pystacked $model, method(nnet) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.5; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(lassocv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.1) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt bfolds(3) 

Base learner: lassocv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 1; eps = 0.1; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = True
> ; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.01) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt  bfolds(3)    

Base learner: ridgecv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 0; eps = 0.01; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = Tru
> e; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

.                         
. pystacked $model, method(gradboost) ///
>                         cmdopt1( ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         alpha(.8) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = squared_error; learning_rate = 0.2; n_estimators = 400; subsample = 0.8; criterion = friedman_mse; min_samples_split = 5; min_samples_leaf = 0.
> 1; min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt
> ; alpha = 0.8; max_leaf_nodes = 4; warm_start = True; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(rf) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = squared_error; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_featu
> res = sqrt; max_leaf_nodes = 4; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = None; random_state = RandomState(MT19937)
> ; warm_start = True; ccp_alpha = 0; max_samples = None; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0637121
  rf             |      0.4239032
  gradboost      |      0.5123848

. pystacked, table

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       417       9         .       .         .       .
  STACKING     1 |    .         4     260         .       .         .       .
  logit        0 | 0.064      399      39       389      49         .       .
  logit        1 | 0.064       22     230        32     220         .       .
  rf           0 | 0.424      421       1       395      34         .       .
  rf           1 | 0.424        0     268        26     235         .       .
  gradboost    0 | 0.512      416      13       398      34         .       .
  gradboost    1 | 0.512        5     256        23     235         .       .

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

.  
. 
end of do-file

. do "cs_pystacked_reg.do"

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.3.0'

. 
. global xvars crim-lstat

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = 
> sqrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm
> _start = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

. set seed 42

. pystacked medh longi-medi 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1;
>  min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; 
> max_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; ran
> dom_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_stat
> e = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random
> _state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; de
> cision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4067909
  rf             |      0.3144122
  gradboost      |      0.2787970

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .       203      19
  STACKING     1 |    .         1     118         .       .        15     128
  logit        0 | 0.407      198       8       183      16       200      20
  logit        1 | 0.407        5     114        20     106        18     127
  rf           0 | 0.314      203       1       193      22       206      22
  rf           1 | 0.314        0     121        10     100        12     125
  gradboost    0 | 0.279      203       4       196      24       203      24
  gradboost    1 | 0.279        0     118         7      98        15     123

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:  175

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .        96      11
  STACKING     1 |    .         1     118         .       .         5      63
  logit        0 | 0.407      198       8       183      16        93      11
  logit        1 | 0.407        5     114        20     106         8      63
  rf           0 | 0.314      203       1       193      22        97      13
  rf           1 | 0.314        0     121        10     100         4      61
  gradboost    0 | 0.279      203       4       196      24        96      13
  gradboost    1 | 0.279        0     118         7      98         5      61

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) table holdout
(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state =
>  RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  linsvm         |      1.0000000

. 
. pystacked $model, method(svm) ///
>                         cmdopt1( ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.1) ///
>                         tol(1e-2) ///
>                         c(1.1) ///
>                         epsilon(0.14) ///
>                         shrinking(False) ///
>                         max_iter(10) ///
>                         ) showopt       

Base learner: svm
kernel = poly; degree = 2; gamma = auto; coef0 = 0.1; tol = 0.01; C = 1.1; epsilon = 0.14; max_iter = 10; shrinking = False; cache_size = 200; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

. 
. pystacked $model, method(nnet) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.5; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(lassocv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.1) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt bfolds(3) 

Base learner: lassocv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 1; eps = 0.1; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = True
> ; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.01) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt  bfolds(3)    

Base learner: ridgecv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 0; eps = 0.01; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = Tru
> e; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

.                         
. pystacked $model, method(gradboost) ///
>                         cmdopt1( ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         alpha(.8) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = squared_error; learning_rate = 0.2; n_estimators = 400; subsample = 0.8; criterion = friedman_mse; min_samples_split = 5; min_samples_leaf = 0.
> 1; min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt
> ; alpha = 0.8; max_leaf_nodes = 4; warm_start = True; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(rf) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = squared_error; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_featu
> res = sqrt; max_leaf_nodes = 4; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = None; random_state = RandomState(MT19937)
> ; warm_start = True; ccp_alpha = 0; max_samples = None; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

.  
. 
end of do-file

. do "cs_pystacked_reg.do"

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'1.1.3'

. 
. global xvars crim-lstat

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
. clear all
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       201       4         .       .       200      17
  STACKING     1 |    .         2     118         .       .        18     130
  logit        0 | 0.536      198       8       184      15       200      20
  logit        1 | 0.536        5     114        19     107        18     127
  rf           0 | 0.168      203       1       192      23       206      22
  rf           1 | 0.168        0     121        11      99        12     125
  gradboost    0 | 0.297      203       4       192      24       203      24
  gradboost    1 | 0.297        0     118        11      98        15     123

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. pystacked medh longi-medi 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4658911
  rf             |      0.5341089
  gradboost      |      0.0000000

. pystacked, table holdout
Number of holdout observations:  365

Confusion matrix: In-Sample, CV, Holdout
-----------------------------------------------------------------------------
  Method         | Weight      In-Sample             CV             Holdout
                 |             0       1         0       1         0       1
-----------------+-----------------------------------------------------------
  STACKING     0 |    .       202       4         .       .       202      16
  STACKING     1 |    .         1     118         .       .        16     131
  logit        0 | 0.466      196       7       188      22       200      21
  logit        1 | 0.466        7     115        15     100        18     126
  rf           0 | 0.534      203       1       189      21       206      22
  rf           1 | 0.534        0     121        14     101        12     125
  gradboost    0 | 0.000      203       4       193      24       203      24
  gradboost    1 | 0.000        0     118        10      98        15     123

. 
. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. use `testdata', clear

. 
. // holdout sample 1
. cap drop h1

. gen h1 = !train2

. 
. // full sample
. pystacked $model, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1782491
  rf             |      0.3574248
  gradboost      |      0.4643262

. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  gradboost      |      1.0000000

. 
. 
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5951811
  lassocv        |      0.4048189

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -84952.1889666
  latitude        | -90121.0864612
  houseage        |  14601.0212277
  rooms           | -16401.1751454
  bedrooms        |  46262.4329834
  population      | -43119.4559888
  households      |  18099.3238179
  medinc          |  76171.0753808
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9980780
  elasticcv      |      0.0019220

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |    -67.1368934
  latitude        |   -205.5623744
  houseage        |    153.3336201
  rooms           |    188.3205133
  bedrooms        |     69.3490681
  population      |    -37.8745232
  households      |     91.2265446
  medinc          |    986.1153261
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9995233
  ridgecv        |      0.0004767

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -85796.6543473
  latitude        | -90939.6512800
  houseage        |  14552.3755170
  rooms           | -17848.7734991
  bedrooms        |  47772.8556714
  population      | -43638.4785453
  households      |  18469.2458130
  medinc          |  76463.1478941
  _cons           | 206855.8169089

. 
. // with holdout sample
. pystacked $model if train, type(class) pyseed(123) methods(logit rf gradboost)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.4110651
  rf             |      0.5168489
  gradboost      |      0.0720860

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  gradboost      |      1.0000000

. 
. 
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5951811
  lassocv        |      0.4048189

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -84952.1889666
  latitude        | -90121.0864612
  houseage        |  14601.0212277
  rooms           | -16401.1751454
  bedrooms        |  46262.4329834
  population      | -43119.4559888
  households      |  18099.3238179
  medinc          |  76171.0753808
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9980780
  elasticcv      |      0.0019220

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |    -67.1368934
  latitude        |   -205.5623744
  houseage        |    153.3336201
  rooms           |    188.3205133
  bedrooms        |     69.3490681
  population      |    -37.8745232
  households      |     91.2265446
  medinc          |    986.1153261
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9995233
  ridgecv        |      0.0004767

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -85796.6543473
  latitude        | -90939.6512800
  houseage        |  14552.3755170
  rooms           | -17848.7734991
  bedrooms        |  47772.8556714
  population      | -43638.4785453
  households      |  18469.2458130
  medinc          |  76463.1478941
  _cons           | 206855.8169089

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:  175

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  gradboost      |      1.0000000

. 
. 
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  gradboost      |      1.0000000

. 
. 
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') showc
  3. }

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') showc
  3. }

. // histogram option
. pystacked, graph hist holdout
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5951811
  lassocv        |      0.4048189

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -84952.1889666
  latitude        | -90121.0864612
  houseage        |  14601.0212277
  rooms           | -16401.1751454
  bedrooms        |  46262.4329834
  population      | -43119.4559888
  households      |  18099.3238179
  medinc          |  76171.0753808
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5951811
  lassocv        |      0.4048189

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -84952.1889666
  latitude        | -90121.0864612
  houseage        |  14601.0212277
  rooms           | -16401.1751454
  bedrooms        |  46262.4329834
  population      | -43119.4559888
  households      |  18099.3238179
  medinc          |  76171.0753808
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9980780
  elasticcv      |      0.0019220

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |    -67.1368934
  latitude        |   -205.5623744
  houseage        |    153.3336201
  rooms           |    188.3205133
  bedrooms        |     69.3490681
  population      |    -37.8745232
  households      |     91.2265446
  medinc          |    986.1153261
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9980780
  elasticcv      |      0.0019220

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |    -67.1368934
  latitude        |   -205.5623744
  houseage        |    153.3336201
  rooms           |    188.3205133
  bedrooms        |     69.3490681
  population      |    -37.8745232
  households      |     91.2265446
  medinc          |    986.1153261
  _cons           | 206855.8169089

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:  365

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9995233
  ridgecv        |      0.0004767

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -85796.6543473
  latitude        | -90939.6512800
  houseage        |  14552.3755170
  rooms           | -17848.7734991
  bedrooms        |  47772.8556714
  population      | -43638.4785453
  households      |  18469.2458130
  medinc          |  76463.1478941
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9995233
  ridgecv        |      0.0004767

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -85796.6543473
  latitude        | -90939.6512800
  houseage        |  14552.3755170
  rooms           | -17848.7734991
  bedrooms        |  47772.8556714
  population      | -43638.4785453
  households      |  18469.2458130
  medinc          |  76463.1478941
  _cons           | 206855.8169089

. // graphing options - learner graphs
. pystacked, lgraph(percent) hist holdout
Number of holdout observations:  365

. 
. // as pystacked option
. pystacked $model if train, type(class) pyseed(123) ///
>         methods(logit rf gradboost) graph holdout
Number of holdout observations:  365

. 
. // syntax 2
. pystacked $model || method(logit) || method(rf) || method(gradboost) || if train, ///
>         type(class) pyseed(123)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.3833952
  rf             |      0.3670662
  gradboost      |      0.2495385

. pystacked, graph holdout
Number of holdout observations:  365

. 
. 
end of do-file

. do "cs_pystacked_options.do"

.  
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'0.24.2'

. 
. global model lpsa lcavol lweight age lbph svi lcp gleason pgg45

. 
. *******************************************************************************
. *** check options classification                                                                                        ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. sum lpsa, meanonly

. replace lpsa = lpsa > `r(mean)'
(97 real changes made)

. 
. pystacked $model, method(rf) type(class) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         criterion(entropy) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         bootstrap(True) ///
>                         n_jobs(3) ///
>                         max_samples(10) ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = entropy; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = 
> sqrt; max_leaf_nodes = None; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = 3; random_state = RandomState(MT19937); warm
> _start = False; ccp_alpha = 0; max_samples = 10; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

. 
. pystacked $model, method(gradboost) type(class) ///
>                         cmdopt1( ///
>                         loss(exponential) ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = exponential; learning_rate = 0.2; n_estimators = 400; criterion = friedman_mse; subsample = 0.8; min_samples_split = 5; min_samples_leaf = 0.1;
>  min_weight_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; 
> max_leaf_nodes = 4; warm_start = False; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(elasticcv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         l1_ratios(0 0.1 1) ///
>                         ) showopt  bfolds(4)  

Base learner: elasticcv
Cs = 9; fit_intercept = False; penalty = elasticnet; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; ran
> dom_state = RandomState(MT19937); l1_ratios = (0, 0.1, 1); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  elasticcv      |      1.0000000

. 
. pystacked $model, method(lassocv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4) 

Base learner: lassocv
Cs = 9; fit_intercept = False; penalty = l1; solver = saga; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random_stat
> e = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) type(class) ///
>                         cmdopt1( ///
>                         c(9) ///
>                         nocons ///
>                         tol(0.001) ///
>                         max_iter(90) ///
>                         n_jobs(2) ///
>                         intercept_scaling(1.1) ///
>                         ) showopt bfolds(4)     

Base learner: ridgecv
Cs = 9; fit_intercept = False; penalty = l2; solver = newton-cg; tol = 0.001; max_iter = 90; n_jobs = 2; refit = True; intercept_scaling = 1.1; random
> _state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

. 
. pystacked $model, method(nnet) type(class) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         power_t(0.4) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.4; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(svm) type(class) ///
>                         cmdopt1( ///
>                         c(0.1) ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.01) ///
>                         shrinking(False) ///
>                         tol(1e-2) ///
>                         cache_size(150) ///
>                         max_iter(10) ///
>                         ) showopt 

Base learner: svm
C = 0.1; kernel = poly; degree = 2; gamma = auto; coef0 = 0.01; shrinking = False; probability = True; tol = 0.01; cache_size = 150; max_iter = 10; de
> cision_function_shape = ovr; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

.                         
.                         
.                         
. *******************************************************************************
. *** check options regression                                                                                            ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. pystacked $model, method(linsvm) ///
>                         cmdopt1( ///
>                         epsilon(0.01) ///
>                         tol(1e-3) ///
>                         c(1.1) ///
>                         loss(squared_epsilon_insensitive) ///
>                         nocons ///
>                         intercept_scaling(1.1) ///
>                         dual(False) ///
>                         max_iter(900) ///
>                         ) showopt       

Base learner: linsvm
epsilon = 0.01; tol = 0.001; C = 1.1; loss = squared_epsilon_insensitive; fit_intercept = False; intercept_scaling = 1.1; dual = False; random_state =
>  RandomState(MT19937); max_iter = 900; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  linsvm         |      1.0000000

. 
. pystacked $model, method(svm) ///
>                         cmdopt1( ///
>                         kernel(poly) ///
>                         degree(2) ///
>                         gamma(auto) ///
>                         coef0(0.1) ///
>                         tol(1e-2) ///
>                         c(1.1) ///
>                         epsilon(0.14) ///
>                         shrinking(False) ///
>                         max_iter(10) ///
>                         ) showopt       

Base learner: svm
kernel = poly; degree = 2; gamma = auto; coef0 = 0.1; tol = 0.01; C = 1.1; epsilon = 0.14; max_iter = 10; shrinking = False; cache_size = 200; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  svm            |      1.0000000

. 
. pystacked $model, method(nnet) ///
>                         cmdopt1( ///
>                         hidden_layer_sizes(5 5) ///
>                         activation(logistic) ///
>                         alpha(0.0002) ///
>                         learning_rate(adaptive) ///
>                         learning_rate_init(0.01) ///
>                         max_iter(100) ///
>                         shuffle(False) ///
>                         momentum(0.8) ///
>                         validation_fraction(0.15) ///
>                         beta_1(0.91) ///
>                         beta_2(0.991) ///
>                         epsilon(1e-7) ///
>                         n_iter_no_change(9) ///
>                         max_fun(14000) ///
>                         ) showopt                       

Base learner: nnet
hidden_layer_sizes = (5, 5); activation = logistic; solver = adam; alpha = 0.0002; batch_size = auto; learning_rate = adaptive; learning_rate_init = 0
> .01; power_t = 0.5; max_iter = 100; shuffle = False; random_state = RandomState(MT19937); tol = 0.0001; verbose = False; warm_start = False; momentu
> m = 0.8; nesterovs_momentum = True; early_stopping = False; validation_fraction = 0.15; beta_1 = 0.91; beta_2 = 0.991; epsilon = 1e-07; n_iter_no_ch
> ange = 9; max_fun = 14000; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  nnet           |      1.0000000

. 
. pystacked $model, method(lassocv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.1) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt bfolds(3) 

Base learner: lassocv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 1; eps = 0.1; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = True
> ; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  lassocv        |      1.0000000

. 
. pystacked $model, method(ridgecv) ///
>                         cmdopt1( ///
>                         alphas(0.01 0.1 1 2) ///
>                         eps(0.01) ///
>                         n_alphas(20) ///
>                         nocons ///
>                         max_iter(500) ///
>                         tol(0.001) ///
>                         positive ///
>                         ) showopt  bfolds(3)    

Base learner: ridgecv
alphas = [0.01, 0.1, 1, 2]; l1_ratio = 0; eps = 0.01; n_alphas = 20; fit_intercept = False; max_iter = 500; tol = 0.001; n_jobs = None; positive = Tru
> e; selection = cyclic; random_state = RandomState(MT19937); 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ridgecv        |      1.0000000

.                         
. pystacked $model, method(gradboost) ///
>                         cmdopt1( ///
>                         learning_rate(0.2) ///
>                         n_estimators(400) ///
>                         subsample(0.8) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         alpha(.8) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         validation_fraction(.15) ///
>                         ) showopt  

Base learner: gradboost
loss = ls; learning_rate = 0.2; n_estimators = 400; subsample = 0.8; criterion = friedman_mse; min_samples_split = 5; min_samples_leaf = 0.1; min_weig
> ht_fraction_leaf = 0.1; max_depth = 3; min_impurity_decrease = 0.1; init = None; random_state = RandomState(MT19937); max_features = sqrt; alpha = 0
> .8; max_leaf_nodes = 4; warm_start = True; validation_fraction = 0.15; n_iter_no_change = None; tol = 0.0001; ccp_alpha = 0; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      1.0000000

. 
. pystacked $model, method(rf) ///
>                         cmdopt1( ///
>                         n_estimators(400) ///
>                         min_samples_split(5) ///
>                         min_samples_leaf(0.1) ///
>                         min_weight_fraction_leaf(.1) ///
>                         max_depth(3) ///        
>                         min_impurity_decrease(.1) ///
>                         max_features(sqrt) ///
>                         max_leaf_nodes(4) /// 
>                         warm_start ///
>                         ) showopt 

Base learner: rf
n_estimators = 400; criterion = mse; max_depth = 3; min_samples_split = 5; min_samples_leaf = 0.1; min_weight_fraction_leaf = 0.1; max_features = sqrt
> ; max_leaf_nodes = 4; min_impurity_decrease = 0.1; bootstrap = True; oob_score = False; n_jobs = None; random_state = RandomState(MT19937); warm_sta
> rt = True; ccp_alpha = 0; max_samples = None; 

Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  rf             |      1.0000000

.  
. 
end of do-file

. do "cs_pystacked_reg.do"

. 
. which pystacked 
/Users/kahrens/MyProjects/pystacked/pystacked.ado
*! pystacked v0.7.5
*! last edited: 7aug2023
*! authors: aa/ms

. python: import sklearn

. python: sklearn.__version__
'0.24.2'

. 
. global xvars crim-lstat

. 
. *******************************************************************************
. *** check that it works without default methods                                                         ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. pystacked medh longi-medi 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0074044
  rf             |      0.9925956

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1614559
  latitude        |      0.1530579
  houseage        |      0.0645715
  rooms           |      0.0251580
  bedrooms        |      0.0264423
  population      |      0.0360831
  households      |      0.0197125
  medinc          |      0.5135187

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  gradboost      |      1.0000000

. 
. 
. *******************************************************************************
. *** check that printing the coefficients works                                                          ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') showc
  3. }

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5951811
  lassocv        |      0.4048189

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients lassocv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -84952.1889666
  latitude        | -90121.0864612
  houseage        |  14601.0212277
  rooms           | -16401.1751454
  bedrooms        |  46262.4329834
  population      | -43119.4559888
  households      |  18099.3238179
  medinc          |  76171.0753808
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9980780
  elasticcv      |      0.0019220

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients elasticcv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |    -67.1368934
  latitude        |   -205.5623744
  houseage        |    153.3336201
  rooms           |    188.3205133
  bedrooms        |     69.3490681
  population      |    -37.8745232
  households      |     91.2265446
  medinc          |    986.1153261
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.9995233
  ridgecv        |      0.0004767

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Coefficients ridgecv:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -85796.6543473
  latitude        | -90939.6512800
  houseage        |  14552.3755170
  rooms           | -17848.7734991
  bedrooms        |  47772.8556714
  population      | -43638.4785453
  households      |  18469.2458130
  medinc          |  76463.1478941
  _cons           | 206855.8169089

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0074044
  rf             |      0.9925956

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1614559
  latitude        |      0.1530579
  houseage        |      0.0645715
  rooms           |      0.0251580
  bedrooms        |      0.0264423
  population      |      0.0360831
  households      |      0.0197125
  medinc          |      0.5135187

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  gradboost      |      1.0000000

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1350887
  latitude        |      0.1477318
  houseage        |      0.0512887
  rooms           |      0.0024538
  bedrooms        |      0.0215079
  population      |      0.0235235
  households      |      0.0092327
  medinc          |      0.6091728

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') printopt
  3. }

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: lassocv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(1)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: elasticcv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(.5)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: ridgecv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(0 1 10)  l1_ratio(0)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2)  min_samples_leaf(real 1) 
 min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1)  min_impurity_decrease(real 0)  bootstrap(string) 
 oob_score  n_jobs(integer 0)  random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(real -1) 

 Specified options are translated to: 
 n_estimators(100)  criterion(squared_error)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(auto)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0074044
  rf             |      0.9925956

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1614559
  latitude        |      0.1530579
  houseage        |      0.0645715
  rooms           |      0.0251580
  bedrooms        |      0.0264423
  population      |      0.0360831
  households      |      0.0197125
  medinc          |      0.5135187

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  criterion(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  alpha(real 0.9) 
 max_leaf_nodes(integer -1)  warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1) 
 tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(squared_error)  learning_rate(.1)  n_estimators(100)  subsample(1)  criterion(friedman_mse)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  alpha(.9)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None) 
 tol(.0001)  ccp_alpha(0) 

. 
. 
. *******************************************************************************
. *** check against SJ paper                                                                                                      ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. gen train=runiform()

. replace train=train<.75
(20,640 real changes made)

. replace medh = medh/10e3 
variable medhousevalue was long now double
(20,640 real changes made)

. label var medh 

. set seed 42

. pystacked medh longi-medi if train,                        ///
>     type(regress)                                          ///
>     methods(ols lassocv lassocv rf gradboost)              ///
>     pipe3(poly2) cmdopt5(learning_rate(0.01)               ///
>     n_estimators(1000))

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  gradboost      |      1.0000000

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1350887
  latitude        |      0.1477318
  houseage        |      0.0512887
  rooms           |      0.0024538
  bedrooms        |      0.0215079
  population      |      0.0235235
  households      |      0.0092327
  medinc          |      0.6091728

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') printopt
  3. }

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: lassocv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(1)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: elasticcv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(.5)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: ridgecv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(0 1 10)  l1_ratio(0)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2)  min_samples_leaf(real 1) 
 min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1)  min_impurity_decrease(real 0)  bootstrap(string) 
 oob_score  n_jobs(integer 0)  random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(real -1) 

 Specified options are translated to: 
 n_estimators(100)  criterion(squared_error)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  criterion(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  alpha(real 0.9) 
 max_leaf_nodes(integer -1)  warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1) 
 tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(squared_error)  learning_rate(.1)  n_estimators(100)  subsample(1)  criterion(friedman_mse)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  alpha(.9)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None) 
 tol(.0001)  ccp_alpha(0) 

. 
. 
. *******************************************************************************
. *** check against SJ paper                                                                                                      ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. gen train=runiform()

. replace train=train<.75
(20,640 real changes made)

. replace medh = medh/10e3 
variable medhousevalue was long now double
(20,640 real changes made)

. label var medh 

. set seed 42

. pystacked medh longi-medi if train,                        ///
>     type(regress)                                          ///
>     methods(ols lassocv lassocv rf gradboost)              ///
>     pipe3(poly2) cmdopt5(learning_rate(0.01)               ///
>     n_estimators(1000))

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  gradboost      |      1.0000000

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1350887
  latitude        |      0.1477318
  houseage        |      0.0512887
  rooms           |      0.0024538
  bedrooms        |      0.0215079
  population      |      0.0235235
  households      |      0.0092327
  medinc          |      0.6091728

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') printopt
  3. }

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: lassocv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(1)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: elasticcv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(.5)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: ridgecv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(0 1 10)  l1_ratio(0)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2)  min_samples_leaf(real 1) 
 min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1)  min_impurity_decrease(real 0)  bootstrap(string) 
 oob_score  n_jobs(integer 0)  random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(real -1) 

 Specified options are translated to: 
 n_estimators(100)  criterion(squared_error)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  criterion(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  alpha(real 0.9) 
 max_leaf_nodes(integer -1)  warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1) 
 tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(squared_error)  learning_rate(.1)  n_estimators(100)  subsample(1)  criterion(friedman_mse)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  alpha(.9)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None) 
 tol(.0001)  ccp_alpha(0) 

. 
. 
. *******************************************************************************
. *** check against SJ paper                                                                                                      ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. gen train=runiform()

. replace train=train<.75
(20,640 real changes made)

. replace medh = medh/10e3 
variable medhousevalue was long now double
(20,640 real changes made)

. label var medh 

. set seed 42

. pystacked medh longi-medi if train,                        ///
>     type(regress)                                          ///
>     methods(ols lassocv lassocv rf gradboost)              ///
>     pipe3(poly2) cmdopt5(learning_rate(0.01)               ///
>     n_estimators(1000))

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0074044
  rf             |      0.9925956

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1614559
  latitude        |      0.1530579
  houseage        |      0.0645715
  rooms           |      0.0251580
  bedrooms        |      0.0264423
  population      |      0.0360831
  households      |      0.0197125
  medinc          |      0.5135187

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  gradboost      |      1.0000000

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1350887
  latitude        |      0.1477318
  houseage        |      0.0512887
  rooms           |      0.0024538
  bedrooms        |      0.0215079
  population      |      0.0235235
  households      |      0.0092327
  medinc          |      0.6091728

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') printopt
  3. }

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: lassocv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(1)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: elasticcv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(.5)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: ridgecv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(0 1 10)  l1_ratio(0)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2)  min_samples_leaf(real 1) 
 min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1)  min_impurity_decrease(real 0)  bootstrap(string) 
 oob_score  n_jobs(integer 0)  random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(real -1) 

 Specified options are translated to: 
 n_estimators(100)  criterion(squared_error)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(1.0)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  criterion(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  alpha(real 0.9) 
 max_leaf_nodes(integer -1)  warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1) 
 tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(squared_error)  learning_rate(.1)  n_estimators(100)  subsample(1)  criterion(friedman_mse)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  alpha(.9)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None) 
 tol(.0001)  ccp_alpha(0) 

. 
. 
. *******************************************************************************
. *** check against SJ paper                                                                                                      ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. gen train=runiform()

. replace train=train<.75
(20,640 real changes made)

. replace medh = medh/10e3 
variable medhousevalue was long now double
(20,640 real changes made)

. label var medh 

. set seed 42

. pystacked medh longi-medi if train,                        ///
>     type(regress)                                          ///
>     methods(ols lassocv lassocv rf gradboost)              ///
>     pipe3(poly2) cmdopt5(learning_rate(0.01)               ///
>     n_estimators(1000))

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0074044
  rf             |      0.9925956

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance rf:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1614559
  latitude        |      0.1530579
  houseage        |      0.0645715
  rooms           |      0.0251580
  bedrooms        |      0.0264423
  population      |      0.0360831
  households      |      0.0197125
  medinc          |      0.5135187

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  gradboost      |      1.0000000

Coefficients ols:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       | -42823.7451854
  latitude        | -42576.7237048
  houseage        |   1156.3037973
  rooms           |     -8.1816470
  bedrooms        |    113.4106837
  population      |    -38.5350952
  households      |     48.3082813
  medinc          |  40248.5135985
  _cons           | -3.5940230e+06

Variable importance gradboost:
----------------------------------------
  Predictor       |      Value
------------------+---------------------
  longitude       |      0.1350887
  latitude        |      0.1477318
  houseage        |      0.0512887
  rooms           |      0.0024538
  bedrooms        |      0.0215079
  population      |      0.0235235
  households      |      0.0092327
  medinc          |      0.6091728

. 
. 
. *******************************************************************************
. *** check that printing the options works                                                                       ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. 
. foreach meth in lassocv elasticcv ridgecv rf gradboost {
  2.         pystacked medh longi-medi , m(ols `meth') printopt
  3. }

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: lassocv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(1)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: elasticcv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(None)  l1_ratio(.5)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: ridgecv

 Stata syntax:
 alphas(numlist >0)  l1_ratio(real .5)  eps(real)  n_alphas(integer)  noconstant  max_iter(integer)  tol(real) 
 n_jobs(integer)  positive  selection(string)  random_state(integer) 

 Specified options are translated to: 
 alphas(0 1 10)  l1_ratio(0)  eps(.001)  n_alphas(100)  fit_intercept(True)  max_iter(1000) 
 tol(.0001)  n_jobs(None)  positive(False)  selection(cyclic)  random_state(rng) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: rf

 Stata syntax:
 n_estimators(integer 100)  criterion(string)  max_depth(integer -1)  min_samples_split(real 2)  min_samples_leaf(real 1) 
 min_weight_fraction_leaf(real 0)  max_features(string)  max_leaf_nodes(integer -1)  min_impurity_decrease(real 0)  bootstrap(string) 
 oob_score  n_jobs(integer 0)  random_state(integer -1)  warm_start  ccp_alpha(real 0)  max_samples(real -1) 

 Specified options are translated to: 
 n_estimators(100)  criterion(mse)  max_depth(None)  min_samples_split(2)  min_samples_leaf(1)  min_weight_fraction_leaf(0) 
 max_features(auto)  max_leaf_nodes(None)  min_impurity_decrease(0)  bootstrap(True)  oob_score(False)  n_jobs(None) 
 random_state(rng)  warm_start(False)  ccp_alpha(0)  max_samples(None) 

Machine learner: ols

 Stata syntax:
 noconstant positive

 Specified options are translated to: 
 fit_intercept(True)  positive(False) 

Machine learner: gradboost

 Stata syntax:
 loss(string)  criterion(string)  learning_rate(real 0.1)  n_estimators(integer 100)  subsample(real 1) 
 min_samples_split(real 2)  min_samples_leaf(real 1)  min_weight_fraction_leaf(real 0)  max_depth(integer 3) 
 min_impurity_decrease(real 0)  init(string)  random_state(integer -1)  max_features(string)  alpha(real 0.9) 
 max_leaf_nodes(integer -1)  warm_start  validation_fraction(real 0.1)  n_iter_no_change(integer -1) 
 tol(real 1e-4)  ccp_alpha(real 0) 

 Specified options are translated to: 
 loss(ls)  learning_rate(.1)  n_estimators(100)  subsample(1)  criterion(friedman_mse)  min_samples_split(2) 
 min_samples_leaf(1)  min_weight_fraction_leaf(0)  max_depth(3)  min_impurity_decrease(0)  init(None)  random_state(rng) 
 max_features(None)  alpha(.9)  max_leaf_nodes(None)  warm_start(False)  validation_fraction(.1)  n_iter_no_change(None) 
 tol(.0001)  ccp_alpha(0) 

. 
. 
. *******************************************************************************
. *** check against SJ paper                                                                                                      ***
. *******************************************************************************
. 
. clear all

. use https://statalasso.github.io/dta/cal_housing.dta, clear

. set seed 42

. gen train=runiform()

. replace train=train<.75
(20,640 real changes made)

. replace medh = medh/10e3 
variable medhousevalue was long now double
(20,640 real changes made)

. label var medh 

. set seed 42

. pystacked medh longi-medi if train,                        ///
>     type(regress)                                          ///
>     methods(ols lassocv lassocv rf gradboost)              ///
>     pipe3(poly2) cmdopt5(learning_rate(0.01)               ///
>     n_estimators(1000))

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  lassocv        |      0.0000000
  rf             |      0.8382714
  gradboost      |      0.1617286

. 
. mat W = e(weights)

. assert reldif(0,el(W,1,1))<0.005

. assert reldif(0,el(W,2,1))<0.005

. assert reldif(0,el(W,3,1))<0.005

. assert reldif(0.8382714,el(W,4,1))<0.005

. assert reldif(0.1617286,el(W,5,1))<0.005

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. gen fid = 1 + (_n>250)

.         
. pystacked medv $xvars, method(ols rf) foldvar(fid)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  rf             |      1.0000000

. predict yb , basexb cvalid

. 
. reg medv $xvars if _n<=250

      Source |       SS           df       MS      Number of obs   =       250
-------------+----------------------------------   F(13, 236)      =    107.69
       Model |  14969.2255        13  1151.47888   Prob > F        =    0.0000
    Residual |  2523.45072       236  10.6925878   R-squared       =    0.8557
-------------+----------------------------------   Adj R-squared   =    0.8478
       Total |  17492.6762       249  70.2517116   Root MSE        =      3.27

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |   1.208708   .5165797     2.34   0.020     .1910112    2.226404
          zn |   .0246793   .0139997     1.76   0.079     -.002901    .0522596
       indus |   .0101453    .051355     0.20   0.844    -.0910274    .1113181
        chas |   .4227349   .8157238     0.52   0.605    -1.184296    2.029765
         nox |  -8.480356   4.493227    -1.89   0.060    -17.33231    .3716021
          rm |   8.865556   .4935031    17.96   0.000     7.893322     9.83779
         age |   -.048789   .0117209    -4.16   0.000      -.07188    -.025698
         dis |  -1.123173   .2121414    -5.29   0.000    -1.541106   -.7052403
         rad |   .2089508   .1511725     1.38   0.168    -.0888692    .5067707
         tax |  -.0160083   .0039309    -4.07   0.000    -.0237525   -.0082641
     ptratio |  -.6580104   .1244101    -5.29   0.000    -.9031065   -.4129142
           b |   .0178036   .0052194     3.41   0.001      .007521    .0280863
       lstat |  -.1155318    .057027    -2.03   0.044    -.2278789   -.0031847
       _cons |   -9.94033   5.878804    -1.69   0.092    -21.52197    1.641306
------------------------------------------------------------------------------

. predict ols1 if _n>250
(option xb assumed; fitted values)
(250 missing values generated)

. reg medv $xvars if _n>250

      Source |       SS           df       MS      Number of obs   =       256
-------------+----------------------------------   F(13, 242)      =     52.56
       Model |  17507.9606        13   1346.7662   Prob > F        =    0.0000
    Residual |  6200.28792       242  25.6210244   R-squared       =    0.7385
-------------+----------------------------------   Adj R-squared   =    0.7244
       Total |  23708.2486       255  92.9735238   Root MSE        =    5.0617

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |  -.1115731   .0360786    -3.09   0.002    -.1826413    -.040505
          zn |   .0467109   .0226302     2.06   0.040     .0021336    .0912883
       indus |  -.1248697   .1586528    -0.79   0.432    -.4373864     .187647
        chas |   6.203216   1.432213     4.33   0.000      3.38202    9.024411
         nox |  -25.37038    6.91223    -3.67   0.000    -38.98619   -11.75456
          rm |   .8986557   .5743273     1.56   0.119     -.232663    2.029974
         age |    .029812   .0256754     1.16   0.247    -.0207637    .0803878
         dis |  -1.569539     .33795    -4.64   0.000    -2.235238   -.9038402
         rad |   .3418979   .0982913     3.48   0.001     .1482821    .5355137
         tax |  -.0045875   .0070141    -0.65   0.514     -.018404    .0092289
     ptratio |  -1.387848   .2406709    -5.77   0.000    -1.861925   -.9137712
           b |   .0058822   .0031425     1.87   0.062    -.0003079    .0120723
       lstat |  -.7205029   .0727863    -9.90   0.000    -.8638784   -.5771273
       _cons |   66.56659   7.426122     8.96   0.000      51.9385    81.19467
------------------------------------------------------------------------------

. predict ols2 if _n<=250
(option xb assumed; fitted values)
(256 missing values generated)

. gen ols_crossfit = ols1 if _n>250
(250 missing values generated)

. replace ols_crossfit = ols2 if _n<=250
(250 real changes made)

. 
. assert reldif(ols_crossfit , yb1)<10e-6

. 
. 
. *******************************************************************************
. *** pystacked with one predictor                                                                                        ***
. *******************************************************************************
. 
. sysuse auto , clear
(1978 automobile data)

. pystacked price mpg, type(reg) m(ols)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      1.0000000

. predict double xhat1

. 
. reg price mpg

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(1, 72)        =     20.26
       Model |   139449474         1   139449474   Prob > F        =    0.0000
    Residual |   495615923        72  6883554.48   R-squared       =    0.2196
-------------+----------------------------------   Adj R-squared   =    0.2087
       Total |   635065396        73  8699525.97   Root MSE        =    2623.7

------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879
       _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03
------------------------------------------------------------------------------

. predict double xhat2 
(option xb assumed; fitted values)

. 
. assert reldif(xhat1,xhat2)<10e-6

. 
. 
. *******************************************************************************
. *** check stdscaler default with regularized linear learners                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. pystacked medv $xvars, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9835543
  lassocv        |      0.0164457

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars, method(gradboost lassocv) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9639648
  lassocv        |      0.0360352

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

.         
. pystacked medv $xvars || m(gradboost) || m(lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9271398
  lassocv        |      0.0728602

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars || m(gradboost) || m(lassocv) pipe(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9722477
  lassocv        |      0.0277523

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

. 
. 
. *******************************************************************************
. *** xvar option                                                                                                                         ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. global xuse c.(crim lstat)##c.(crim lstat)

. global xall c.(crim-lstat)##c.(crim-lstat)

. 
. set seed 789

. pystacked medv $xuse, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb

.  
. set seed 789

. pystacked medv $xall, method(gradboost lassocv) xvars1($xuse) xvars2($xuse) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb2

. 
. set seed 789

. pystacked medv $xall || method(gradboost) xvars($xuse) || m(lassocv) xvars($xuse),  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb3

. 
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1($xuse) xvars2($xuse)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb4

. 
. ** this should be different
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb5

. 
. assert reldif(xb,xb2)<1e-5

. assert reldif(xb,xb3)<1e-5

. assert reldif(xb,xb4)<1e-5

. assert xb!=xb5

. 
. 
. *******************************************************************************
. *** xvar vs pipeline                                                                                                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars2(c.(crim-lstat)##c.(crim-lstat))  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222623
  lassocv        |      0.1777377

. predict double xb1

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) pipe2(poly2) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222622
  lassocv        |      0.1777378

. predict double xb2

. 
. assert reldif(xb1,xb2)<1e-4

. 
. 
. *******************************************************************************
. *** voting                                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, tab clear
(11 vars, 97 obs)

. 
. global xvars lcavol-pgg

. 
. set seed 124345

. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5000000
  lassoic        |      0.1000000
  rf             |      0.4000000

. 
. // should cause error
. cap pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .9)      

. assert _rc == 198                                                

.                                         
. *******************************************************************************
. *** check pipeline                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf ) ///
>                                                  pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0691339
  lassoic        |      0.7905221
  rf             |      0.1403440

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic rf ) pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe3) : "poly2"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe2) : "poly2"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars1) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe1) : "poly2"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.0.2"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  45 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict a, basexb

. 
.  
. pystacked lpsa c.($xvars)##c.($xvars), ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic  rf ) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  lassocv        |      0.0000000
  rf             |      0.8382714
  gradboost      |      0.1617286

. 
. mat W = e(weights)

. assert reldif(0,el(W,1,1))<0.005

. assert reldif(0,el(W,2,1))<0.005

. assert reldif(0,el(W,3,1))<0.005

. assert reldif(0.8382714,el(W,4,1))<0.005

. assert reldif(0.1617286,el(W,5,1))<0.005

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. gen fid = 1 + (_n>250)

.         
. pystacked medv $xvars, method(ols rf) foldvar(fid)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  rf             |      1.0000000

. predict yb , basexb cvalid

. 
. reg medv $xvars if _n<=250

      Source |       SS           df       MS      Number of obs   =       250
-------------+----------------------------------   F(13, 236)      =    107.69
       Model |  14969.2255        13  1151.47888   Prob > F        =    0.0000
    Residual |  2523.45072       236  10.6925878   R-squared       =    0.8557
-------------+----------------------------------   Adj R-squared   =    0.8478
       Total |  17492.6762       249  70.2517116   Root MSE        =      3.27

note: svi omitted because of collinearity.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassoic        |      0.5411446
  rf             |      0.4588554

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic  rf ) pipe2(nostdscaler)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe3) : "passthrough"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe2) : "passthrough"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars1) : "lcavol lweight age lbph lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __0000.."
              e(pipe1) : "passthrough"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.0.2"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  44 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict b, basexb

. list lpsa a* b* if _n <= 10

     +-----------------------------------------------------------------------------------+
     |      lpsa   age         a1         a2         a3         b1         b2         b3 |
     |-----------------------------------------------------------------------------------|
  1. | -.4307829    50   .2531799   1.079373   .3325487   .2531799   1.079373   .3325487 |
  2. | -.1625189    58   .5338573   .9922831   .0659286   .5338573   .9922831   .0659286 |
  3. | -.1625189    74   .2774684   .7602873   .3647012   .2774684   .7602873   .3647012 |
  4. | -.1625189    58   .4093789   .8741754   .0859689   .4093789   .8741754   .0859689 |
  5. |  .3715636    62   1.388298   1.827831   .6633989   1.388298   1.827831   .6633989 |
     |-----------------------------------------------------------------------------------|
  6. |  .7654678    50   .6148925   1.051359   .4205081   .6148925   1.051359   .4205081 |
  7. |  .7654678    64   1.843947    1.93333   1.027373   1.843947    1.93333   1.027373 |
  8. |  .8544153    58   1.820537   1.894149   1.162384   1.820537   1.894149   1.162384 |
  9. |  1.047319    47   1.383648   1.367152   .7764618   1.383648   1.367152   .7764618 |
 10. |  1.047319    63   .8287085   1.473098   1.236331   .8287085   1.473098   1.236331 |
     +-----------------------------------------------------------------------------------+

. 
. assert reldif(a1,b1)<1e-5

. assert reldif(a2,b2)<1e-5

. 
. *******************************************************************************
. *** check that xvar() subsetting works                                                                          ***
. *******************************************************************************
. 
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |   1.208708   .5165797     2.34   0.020     .1910112    2.226404
          zn |   .0246793   .0139997     1.76   0.079     -.002901    .0522596
       indus |   .0101453    .051355     0.20   0.844    -.0910274    .1113181
        chas |   .4227349   .8157238     0.52   0.605    -1.184296    2.029765
         nox |  -8.480356   4.493227    -1.89   0.060    -17.33231    .3716021
          rm |   8.865556   .4935031    17.96   0.000     7.893322     9.83779
         age |   -.048789   .0117209    -4.16   0.000      -.07188    -.025698
         dis |  -1.123173   .2121414    -5.29   0.000    -1.541106   -.7052403
         rad |   .2089508   .1511725     1.38   0.168    -.0888692    .5067707
         tax |  -.0160083   .0039309    -4.07   0.000    -.0237525   -.0082641
     ptratio |  -.6580104   .1244101    -5.29   0.000    -.9031065   -.4129142
           b |   .0178036   .0052194     3.41   0.001      .007521    .0280863
       lstat |  -.1155318    .057027    -2.03   0.044    -.2278789   -.0031847
       _cons |   -9.94033   5.878804    -1.69   0.092    -21.52197    1.641306
------------------------------------------------------------------------------

. predict ols1 if _n>250
(option xb assumed; fitted values)
(250 missing values generated)

. reg medv $xvars if _n>250

      Source |       SS           df       MS      Number of obs   =       256
-------------+----------------------------------   F(13, 242)      =     52.56
       Model |  17507.9606        13   1346.7662   Prob > F        =    0.0000
    Residual |  6200.28792       242  25.6210244   R-squared       =    0.7385
-------------+----------------------------------   Adj R-squared   =    0.7244
       Total |  23708.2486       255  92.9735238   Root MSE        =    5.0617

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |  -.1115731   .0360786    -3.09   0.002    -.1826413    -.040505
          zn |   .0467109   .0226302     2.06   0.040     .0021336    .0912883
       indus |  -.1248697   .1586528    -0.79   0.432    -.4373864     .187647
        chas |   6.203216   1.432213     4.33   0.000      3.38202    9.024411
         nox |  -25.37038    6.91223    -3.67   0.000    -38.98619   -11.75456
          rm |   .8986557   .5743273     1.56   0.119     -.232663    2.029974
         age |    .029812   .0256754     1.16   0.247    -.0207637    .0803878
         dis |  -1.569539     .33795    -4.64   0.000    -2.235238   -.9038402
         rad |   .3418979   .0982913     3.48   0.001     .1482821    .5355137
         tax |  -.0045875   .0070141    -0.65   0.514     -.018404    .0092289
     ptratio |  -1.387848   .2406709    -5.77   0.000    -1.861925   -.9137712
           b |   .0058822   .0031425     1.87   0.062    -.0003079    .0120723
       lstat |  -.7205029   .0727863    -9.90   0.000    -.8638784   -.5771273
       _cons |   66.56659   7.426122     8.96   0.000      51.9385    81.19467
------------------------------------------------------------------------------

. predict ols2 if _n<=250
(option xb assumed; fitted values)
(256 missing values generated)

. gen ols_crossfit = ols1 if _n>250
(250 missing values generated)

. replace ols_crossfit = ols2 if _n<=250
(250 real changes made)

. 
. assert reldif(ols_crossfit , yb1)<10e-6

. 
. 
. *******************************************************************************
. *** pystacked with one predictor                                                                                        ***
. *******************************************************************************
. 
. sysuse auto , clear
(1978 automobile data)

. pystacked price mpg, type(reg) m(ols)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      1.0000000

. predict double xhat1

. 
. reg price mpg

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(1, 72)        =     20.26
       Model |   139449474         1   139449474   Prob > F        =    0.0000
    Residual |   495615923        72  6883554.48   R-squared       =    0.2196
-------------+----------------------------------   Adj R-squared   =    0.2087
       Total |   635065396        73  8699525.97   Root MSE        =    2623.7

------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879
       _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03
------------------------------------------------------------------------------

. predict double xhat2 
(option xb assumed; fitted values)

. 
. assert reldif(xhat1,xhat2)<10e-6

. 
. 
. *******************************************************************************
. *** check stdscaler default with regularized linear learners                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. pystacked medv $xvars, method(gradboost lassocv)   
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv crim lstat, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9835543
  lassocv        |      0.0164457

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars, method(gradboost lassocv) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb2

. 
. set seed 789

. pystacked medv crim-lstat || method(gradboost) xvars(crim lstat) || m(lassocv) xvars(crim lstat), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9639648
  lassocv        |      0.0360352

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

.         
. pystacked medv $xvars || m(gradboost) || m(lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 26.407074   26.407074   26.407074 |
  2. | 22.074846   22.074846   22.074846 |
  3. |  34.43726    34.43726    34.43726 |
  4. | 35.627232   35.627232   35.627232 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *** with factor variables
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv i.rad##c.crim, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9271398
  lassocv        |      0.0728602

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars || m(gradboost) || m(lassocv) pipe(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat), method(gradboost lassocv) xvars1(i.rad##c.crim) xvars2(i.rad##c.crim) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  lassocv        |      0.0000000
  rf             |      0.8382714
  gradboost      |      0.1617286

. 
. mat W = e(weights)

. assert reldif(0,el(W,1,1))<0.005

. assert reldif(0,el(W,2,1))<0.005

. assert reldif(0,el(W,3,1))<0.005

. assert reldif(0.8382714,el(W,4,1))<0.005

. assert reldif(0.1617286,el(W,5,1))<0.005

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. gen fid = 1 + (_n>250)

.         
. pystacked medv $xvars, method(ols rf) foldvar(fid)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9722477
  lassocv        |      0.0277523

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

. 
. 
. *******************************************************************************
. *** xvar option                                                                                                                         ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. global xuse c.(crim lstat)##c.(crim lstat)

. global xall c.(crim-lstat)##c.(crim-lstat)

. 
. set seed 789

. pystacked medv $xuse, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb2

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat) || method(gradboost) xvars(i.rad##c.crim) || m(lassocv) xvars(i.rad##c.crim), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  rf             |      1.0000000

. predict yb , basexb cvalid

. 
. reg medv $xvars if _n<=250

      Source |       SS           df       MS      Number of obs   =       250
-------------+----------------------------------   F(13, 236)      =    107.69
       Model |  14969.2255        13  1151.47888   Prob > F        =    0.0000
    Residual |  2523.45072       236  10.6925878   R-squared       =    0.8557
-------------+----------------------------------   Adj R-squared   =    0.8478
       Total |  17492.6762       249  70.2517116   Root MSE        =      3.27

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |   1.208708   .5165797     2.34   0.020     .1910112    2.226404
          zn |   .0246793   .0139997     1.76   0.079     -.002901    .0522596
       indus |   .0101453    .051355     0.20   0.844    -.0910274    .1113181
        chas |   .4227349   .8157238     0.52   0.605    -1.184296    2.029765
         nox |  -8.480356   4.493227    -1.89   0.060    -17.33231    .3716021
          rm |   8.865556   .4935031    17.96   0.000     7.893322     9.83779
         age |   -.048789   .0117209    -4.16   0.000      -.07188    -.025698
         dis |  -1.123173   .2121414    -5.29   0.000    -1.541106   -.7052403
         rad |   .2089508   .1511725     1.38   0.168    -.0888692    .5067707
         tax |  -.0160083   .0039309    -4.07   0.000    -.0237525   -.0082641
     ptratio |  -.6580104   .1244101    -5.29   0.000    -.9031065   -.4129142
           b |   .0178036   .0052194     3.41   0.001      .007521    .0280863
       lstat |  -.1155318    .057027    -2.03   0.044    -.2278789   -.0031847
       _cons |   -9.94033   5.878804    -1.69   0.092    -21.52197    1.641306
------------------------------------------------------------------------------

. predict ols1 if _n>250
(option xb assumed; fitted values)
(250 missing values generated)

. reg medv $xvars if _n>250

      Source |       SS           df       MS      Number of obs   =       256
-------------+----------------------------------   F(13, 242)      =     52.56
       Model |  17507.9606        13   1346.7662   Prob > F        =    0.0000
    Residual |  6200.28792       242  25.6210244   R-squared       =    0.7385
-------------+----------------------------------   Adj R-squared   =    0.7244
       Total |  23708.2486       255  92.9735238   Root MSE        =    5.0617

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |  -.1115731   .0360786    -3.09   0.002    -.1826413    -.040505
          zn |   .0467109   .0226302     2.06   0.040     .0021336    .0912883
       indus |  -.1248697   .1586528    -0.79   0.432    -.4373864     .187647
        chas |   6.203216   1.432213     4.33   0.000      3.38202    9.024411
         nox |  -25.37038    6.91223    -3.67   0.000    -38.98619   -11.75456
          rm |   .8986557   .5743273     1.56   0.119     -.232663    2.029974
         age |    .029812   .0256754     1.16   0.247    -.0207637    .0803878
         dis |  -1.569539     .33795    -4.64   0.000    -2.235238   -.9038402
         rad |   .3418979   .0982913     3.48   0.001     .1482821    .5355137
         tax |  -.0045875   .0070141    -0.65   0.514     -.018404    .0092289
     ptratio |  -1.387848   .2406709    -5.77   0.000    -1.861925   -.9137712
           b |   .0058822   .0031425     1.87   0.062    -.0003079    .0120723
       lstat |  -.7205029   .0727863    -9.90   0.000    -.8638784   -.5771273
       _cons |   66.56659   7.426122     8.96   0.000      51.9385    81.19467
------------------------------------------------------------------------------

. predict ols2 if _n<=250
(option xb assumed; fitted values)
(256 missing values generated)

. gen ols_crossfit = ols1 if _n>250
(250 missing values generated)

. replace ols_crossfit = ols2 if _n<=250
(250 real changes made)

. 
. assert reldif(ols_crossfit , yb1)<10e-6

. 
. 
. *******************************************************************************
. *** pystacked with one predictor                                                                                        ***
. *******************************************************************************
. 
. sysuse auto , clear
(1978 automobile data)

. pystacked price mpg, type(reg) m(ols)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      1.0000000

. predict double xhat1

. 
. reg price mpg

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(1, 72)        =     20.26
       Model |   139449474         1   139449474   Prob > F        =    0.0000
    Residual |   495615923        72  6883554.48   R-squared       =    0.2196
-------------+----------------------------------   Adj R-squared   =    0.2087
       Total |   635065396        73  8699525.97   Root MSE        =    2623.7

------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879
       _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03
------------------------------------------------------------------------------

. predict double xhat2 
(option xb assumed; fitted values)

. 
. assert reldif(xhat1,xhat2)<10e-6

. 
. 
. *******************************************************************************
. *** check stdscaler default with regularized linear learners                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. pystacked medv $xvars, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb

.  
. set seed 789

. pystacked medv $xall, method(gradboost lassocv) xvars1($xuse) xvars2($xuse) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 25.944031   25.944031   25.944031 |
  2. | 27.627625   27.627625   27.627625 |
  3. | 29.591053   29.591053   29.591053 |
  4. | 30.817581   30.817581   30.817581 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *******************************************************************************
. *** try various combinations of estimators                                                                      ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9835543
  lassocv        |      0.0164457

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars, method(gradboost lassocv) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb2

. 
. set seed 789

. pystacked medv $xall || method(gradboost) xvars($xuse) || m(lassocv) xvars($xuse),  
(11 vars, 97 obs)

. 
. local m1 ols lassocv gradboost nnet

. local m2 ols lassocv rf nnet

. local m3 ols ridgecv gradboost nnet

. local m4 ols elasticcv gradboost nnet

. local m5 ols elasticcv gradboost svm

. 
. gen lpsa1 = lpsa>2.47

. 
. local flearners nnls1 nnls0 singlebest ols avg

. 
. foreach f of local flearners  {
  2. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" {
  3. 
.                 
.         di "---------------------------------"
  4.         di "Regress"
  5.         di "Candidate learners: `m'"
  6.         di "Final estimator: `f'"
  7.         
.         pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(`m') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
  8.                                                  
.         if ("`f'"=="nnls1") {
  9.                 mat W = e(weights)
 10.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 11.                 assert reldif(`sum',1)<0.001
 12.         }
 13.         if ("`f'"=="singlebest") {
 14.                 mat W = e(weights)
 15.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 16.                 assert reldif(`sum',1)<0.001
 17.         }
 18.         if ("`f'"=="avg") {
 19.                 mat W = e(weights)
 20.                 assert reldif(el(W,1,1),0.25)<0.001
 21.                 assert reldif(el(W,2,1),0.25)<0.001
 22.                 assert reldif(el(W,3,1),0.25)<0.001
 23.                 assert reldif(el(W,4,1),0.25)<0.001
 24.         }
 25.         
.         local mc = subinstr("`m'","ols","logit",.)
 26.         local mc = subinstr("`mc'"," lassoic","",.)
 27.         local mc = subinstr("`mc'"," linsvm","",.)
 28.                                                  
.         di "---------------------------------"
 29.         di "Classify"
 30.         di "Candidate learners: `m'"
 31.         di "Final estimator: `f'"
 32.                                                  
.         pystacked lpsa1 lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(class) pyseed(243) ///
>                                                  methods(`mc') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
 33.                                                  
.         if ("`f'"=="nnls1") {
 34.                 mat W = e(weights)
 35.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 36.                 assert reldif(`sum',1)<0.001
 37.         }
 38.         if ("`f'"=="singlebest") {
 39.                 mat W = e(weights)
 40.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 41.                 assert reldif(`sum',1)<0.001
 42.         }
 43.         if ("`f'"=="avg") {
 44.                 mat W = e(weights)
 45.                 assert reldif(el(W,1,1),0.25)<0.001
 46.                 assert reldif(el(W,2,1),0.25)<0.001
 47.                 assert reldif(el(W,3,1),0.25)<0.001
 48.                 assert reldif(el(W,4,1),0.25)<0.001
 49.         }
 50.                                                  
. }
 51. }
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb3

. 
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1($xuse) xvars2($xuse)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9639648
  lassocv        |      0.0360352

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

.         
. pystacked medv $xvars || m(gradboost) || m(lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb4

. 
. ** this should be different
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.5805509
  gradboost      |      0.3399453
  nnet           |      0.0795038
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9271398
  lassocv        |      0.0728602

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars || m(gradboost) || m(lassocv) pipe(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb5

. 
. assert reldif(xb,xb2)<1e-5

. assert reldif(xb,xb3)<1e-5

. assert reldif(xb,xb4)<1e-5

. assert xb!=xb5

. 
. 
. *******************************************************************************
. *** xvar vs pipeline                                                                                                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars2(c.(crim-lstat)##c.(crim-lstat))  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9722477
  lassocv        |      0.0277523

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

. 
. 
. *******************************************************************************
. *** xvar option                                                                                                                         ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. global xuse c.(crim lstat)##c.(crim lstat)

. global xall c.(crim-lstat)##c.(crim-lstat)

. 
. set seed 789

. pystacked medv $xuse, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0207007
  lassocv        |      0.6999951
  gradboost      |      0.1672106
  nnet           |      0.1120936
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb

.  
. set seed 789

. pystacked medv $xall, method(gradboost lassocv) xvars1($xuse) xvars2($xuse) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb2

. 
. set seed 789

. pystacked medv $xall || method(gradboost) xvars($xuse) || m(lassocv) xvars($xuse),  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4022450
  rf             |      0.5977550
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb3

. 
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1($xuse) xvars2($xuse)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222623
  lassocv        |      0.1777377

. predict double xb1

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) pipe2(poly2) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb4

. 
. ** this should be different
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1302032
  lassocv        |      0.5995944
  rf             |      0.0230851
  nnet           |      0.2471174
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb5

. 
. assert reldif(xb,xb2)<1e-5

. assert reldif(xb,xb3)<1e-5

. assert reldif(xb,xb4)<1e-5

. assert xb!=xb5

. 
. 
. *******************************************************************************
. *** xvar vs pipeline                                                                                                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars2(c.(crim-lstat)##c.(crim-lstat))  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8433028
  gradboost      |      0.1566972
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222622
  lassocv        |      0.1777378

. predict double xb2

. 
. assert reldif(xb1,xb2)<1e-4

. 
. 
. *******************************************************************************
. *** voting                                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, tab clear
(11 vars, 97 obs)

. 
. global xvars lcavol-pgg

. 
. set seed 124345

. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5000000
  lassoic        |      0.1000000
  rf             |      0.4000000

. 
. // should cause error
. cap pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .9)      

. assert _rc == 198                                                

.                                         
. *******************************************************************************
. *** check pipeline                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818471
  ridgecv        |      0.8136714
  gradboost      |      0.0000000
  nnet           |      0.1044816
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf ) ///
>                                                  pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222623
  lassocv        |      0.1777377

. predict double xb1

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) pipe2(poly2) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0691339
  lassoic        |      0.7905221
  rf             |      0.1403440

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic rf ) pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe3) : "poly2"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe2) : "poly2"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars1) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe1) : "poly2"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.1.3"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  45 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict a, basexb

. 
.  
. pystacked lpsa c.($xvars)##c.($xvars), ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic  rf ) pipe2(nostdscaler)
note: svi omitted because of collinearity.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassoic        |      0.5411446
  rf             |      0.4588554

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic  rf ) pipe2(nostdscaler)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe3) : "passthrough"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe2) : "passthrough"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars1) : "lcavol lweight age lbph lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __0000.."
              e(pipe1) : "passthrough"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.1.3"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  44 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict b, basexb

. list lpsa a* b* if _n <= 10

     +-----------------------------------------------------------------------------------+
     |      lpsa   age         a1         a2         a3         b1         b2         b3 |
     |-----------------------------------------------------------------------------------|
  1. | -.4307829    50   .2531799   1.079373   .3325487   .2531799   1.079373   .3325487 |
  2. | -.1625189    58   .5338573   .9922831   .0659286   .5338573   .9922831   .0659286 |
  3. | -.1625189    74   .2774684   .7602873   .3647012   .2774684   .7602873   .3647012 |
  4. | -.1625189    58   .4093789   .8741754   .0859689   .4093789   .8741754   .0859689 |
  5. |  .3715636    62   1.388298   1.827831   .6633989   1.388298   1.827831   .6633989 |
     |-----------------------------------------------------------------------------------|
  6. |  .7654678    50   .6148925   1.051359   .4205081   .6148925   1.051359   .4205081 |
  7. |  .7654678    64   1.843947    1.93333   1.027373   1.843947    1.93333   1.027373 |
  8. |  .8544153    58   1.820537   1.894149   1.162384   1.820537   1.894149   1.162384 |
  9. |  1.047319    47   1.383648   1.367152   .7764618   1.383648   1.367152   .7764618 |
 10. |  1.047319    63   .8287085   1.473098   1.236331   .8287085   1.473098   1.236331 |
     +-----------------------------------------------------------------------------------+

. 
. assert reldif(a1,b1)<1e-5

. assert reldif(a2,b2)<1e-5

. 
. *******************************************************************************
. *** check that xvar() subsetting works                                                                          ***
. *******************************************************************************
. 
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv crim lstat, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1268245
  elasticcv      |      0.5723710
  gradboost      |      0.3008045
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb2

. 
. set seed 789

. pystacked medv crim-lstat || method(gradboost) xvars(crim lstat) || m(lassocv) xvars(crim lstat), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209751
  gradboost      |      0.2351843
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 26.407074   26.407074   26.407074 |
  2. | 22.074846   22.074846   22.074846 |
  3. |  34.43726    34.43726    34.43726 |
  4. | 35.627232   35.627232   35.627232 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *** with factor variables
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv i.rad##c.crim, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222622
  lassocv        |      0.1777378

. predict double xb2

. 
. assert reldif(xb1,xb2)<1e-4

. 
. 
. *******************************************************************************
. *** voting                                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat), method(gradboost lassocv) xvars1(i.rad##c.crim) xvars2(i.rad##c.crim) pyseed(-1)
(11 vars, 97 obs)

. 
. global xvars lcavol-pgg

. 
. set seed 124345

. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5000000
  lassoic        |      0.1000000
  rf             |      0.4000000

. 
. // should cause error
. cap pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .9)      

. assert _rc == 198                                                

.                                         
. *******************************************************************************
. *** check pipeline                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539241
  elasticcv      |      0.8787627
  gradboost      |      0.0504939
  svm            |      0.0168193
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb2

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat) || method(gradboost) xvars(i.rad##c.crim) || m(lassocv) xvars(i.rad##c.crim), pyseed(-1)
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf ) ///
>                                                  pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 25.944031   25.944031   25.944031 |
  2. | 27.627625   27.627625   27.627625 |
  3. | 29.591053   29.591053   29.591053 |
  4. | 30.817581   30.817581   30.817581 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *******************************************************************************
. *** try various combinations of estimators                                                                      ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0
(11 vars, 97 obs)

. 
. local m1 ols lassocv gradboost nnet

. local m2 ols lassocv rf nnet

. local m3 ols ridgecv gradboost nnet

. local m4 ols elasticcv gradboost nnet

. local m5 ols elasticcv gradboost svm

. 
. gen lpsa1 = lpsa>2.47

. 
. local flearners nnls1 nnls0 singlebest ols avg

. 
. foreach f of local flearners  {
  2. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" {
  3. 
.                 
.         di "---------------------------------"
  4.         di "Regress"
  5.         di "Candidate learners: `m'"
  6.         di "Final estimator: `f'"
  7.         
.         pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(`m') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
  8.                                                  
.         if ("`f'"=="nnls1") {
  9.                 mat W = e(weights)
 10.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 11.                 assert reldif(`sum',1)<0.001
 12.         }
 13.         if ("`f'"=="singlebest") {
 14.                 mat W = e(weights)
 15.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 16.                 assert reldif(`sum',1)<0.001
 17.         }
 18.         if ("`f'"=="avg") {
 19.                 mat W = e(weights)
 20.                 assert reldif(el(W,1,1),0.25)<0.001
 21.                 assert reldif(el(W,2,1),0.25)<0.001
 22.                 assert reldif(el(W,3,1),0.25)<0.001
 23.                 assert reldif(el(W,4,1),0.25)<0.001
 24.         }
 25.         
.         local mc = subinstr("`m'","ols","logit",.)
 26.         local mc = subinstr("`mc'"," lassoic","",.)
 27.         local mc = subinstr("`mc'"," linsvm","",.)
 28.                                                  
.         di "---------------------------------"
 29.         di "Classify"
 30.         di "Candidate learners: `m'"
 31.         di "Final estimator: `f'"
 32.                                                  
.         pystacked lpsa1 lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(class) pyseed(243) ///
>                                                  methods(`mc') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
 33.                                                  
.         if ("`f'"=="nnls1") {
 34.                 mat W = e(weights)
 35.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 36.                 assert reldif(`sum',1)<0.001
 37.         }
 38.         if ("`f'"=="singlebest") {
 39.                 mat W = e(weights)
 40.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 41.                 assert reldif(`sum',1)<0.001
 42.         }
 43.         if ("`f'"=="avg") {
 44.                 mat W = e(weights)
 45.                 assert reldif(el(W,1,1),0.25)<0.001
 46.                 assert reldif(el(W,2,1),0.25)<0.001
 47.                 assert reldif(el(W,3,1),0.25)<0.001
 48.                 assert reldif(el(W,4,1),0.25)<0.001
 49.         }
 50.                                                  
. }
 51. }
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0611815
  lassoic        |      0.4296642
  rf             |      0.5091543

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic rf ) pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe3) : "poly2"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe2) : "poly2"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars1) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe1) : "poly2"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.2.1"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  45 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict a, basexb

. 
.  
. pystacked lpsa c.($xvars)##c.($xvars), ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic  rf ) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.5805509
  gradboost      |      0.3399453
  nnet           |      0.0795038
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0
note: svi omitted because of collinearity.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassoic        |      0.3825348
  rf             |      0.6174652

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic  rf ) pipe2(nostdscaler)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe3) : "passthrough"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe2) : "passthrough"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars1) : "lcavol lweight age lbph lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __0000.."
              e(pipe1) : "passthrough"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "1.2.1"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  44 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict b, basexb

. list lpsa a* b* if _n <= 10

     +-----------------------------------------------------------------------------------+
     |      lpsa   age         a1         a2         a3         b1         b2         b3 |
     |-----------------------------------------------------------------------------------|
  1. | -.4307829    50   .2531799   1.108728   .3325487   .2531799   1.108728   .3325487 |
  2. | -.1625189    58   .5338573    .828543   .0659286   .5338573    .828543   .0659286 |
  3. | -.1625189    74   .2774684   .2845545   .3647012   .2774684   .2845545   .3647012 |
  4. | -.1625189    58   .4093789   .7067597   .0859689   .4093789   .7067597   .0859689 |
  5. |  .3715636    62   1.388298   1.639569   .6633989   1.388298   1.639569   .6633989 |
     |-----------------------------------------------------------------------------------|
  6. |  .7654678    50   .6148925   1.082111   .4205081   .6148925   1.082111   .4205081 |
  7. |  .7654678    64   1.843947   1.833611   1.027373   1.843947   1.833611   1.027373 |
  8. |  .8544153    58   1.820537   2.099344   1.162384   1.820537   2.099344   1.162384 |
  9. |  1.047319    47   1.383648   1.410242   .7764618   1.383648   1.410242   .7764618 |
 10. |  1.047319    63   .8287085    1.22876   1.236331   .8287085    1.22876   1.236331 |
     +-----------------------------------------------------------------------------------+

. 
. assert reldif(a1,b1)<1e-5

. assert reldif(a2,b2)<1e-5

. 
. *******************************************************************************
. *** check that xvar() subsetting works                                                                          ***
. *******************************************************************************
. 
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv crim lstat, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0207010
  lassocv        |      0.6999931
  gradboost      |      0.1672110
  nnet           |      0.1120949
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb2

. 
. set seed 789

. pystacked medv crim-lstat || method(gradboost) xvars(crim lstat) || m(lassocv) xvars(crim lstat), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4022450
  rf             |      0.5977550
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 26.407074   26.407074   26.407074 |
  2. | 22.074846   22.074846   22.074846 |
  3. |  34.43726    34.43726    34.43726 |
  4. | 35.627232   35.627232   35.627232 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *** with factor variables
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv i.rad##c.crim, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1035802
  lassocv        |      0.3790606
  rf             |      0.5437758
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat), method(gradboost lassocv) xvars1(i.rad##c.crim) xvars2(i.rad##c.crim) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1247955
  lassocv        |      0.5569869
  rf             |      0.1380195
  nnet           |      0.1801982
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb2

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat) || method(gradboost) xvars(i.rad##c.crim) || m(lassocv) xvars(i.rad##c.crim), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8433028
  gradboost      |      0.1566972
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 25.944031   25.944031   25.944031 |
  2. | 27.627625   27.627625   27.627625 |
  3. | 29.591053   29.591053   29.591053 |
  4. | 30.817581   30.817581   30.817581 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *******************************************************************************
. *** try various combinations of estimators                                                                      ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. local m1 ols lassocv gradboost nnet

. local m2 ols lassocv rf nnet

. local m3 ols ridgecv gradboost nnet

. local m4 ols elasticcv gradboost nnet

. local m5 ols elasticcv gradboost svm

. 
. gen lpsa1 = lpsa>2.47

. 
. local flearners nnls1 nnls0 singlebest ols avg

. 
. foreach f of local flearners  {
  2. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" {
  3. 
.                 
.         di "---------------------------------"
  4.         di "Regress"
  5.         di "Candidate learners: `m'"
  6.         di "Final estimator: `f'"
  7.         
.         pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(`m') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
  8.                                                  
.         if ("`f'"=="nnls1") {
  9.                 mat W = e(weights)
 10.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 11.                 assert reldif(`sum',1)<0.001
 12.         }
 13.         if ("`f'"=="singlebest") {
 14.                 mat W = e(weights)
 15.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 16.                 assert reldif(`sum',1)<0.001
 17.         }
 18.         if ("`f'"=="avg") {
 19.                 mat W = e(weights)
 20.                 assert reldif(el(W,1,1),0.25)<0.001
 21.                 assert reldif(el(W,2,1),0.25)<0.001
 22.                 assert reldif(el(W,3,1),0.25)<0.001
 23.                 assert reldif(el(W,4,1),0.25)<0.001
 24.         }
 25.         
.         local mc = subinstr("`m'","ols","logit",.)
 26.         local mc = subinstr("`mc'"," lassoic","",.)
 27.         local mc = subinstr("`mc'"," linsvm","",.)
 28.                                                  
.         di "---------------------------------"
 29.         di "Classify"
 30.         di "Candidate learners: `m'"
 31.         di "Final estimator: `f'"
 32.                                                  
.         pystacked lpsa1 lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(class) pyseed(243) ///
>                                                  methods(`mc') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
 33.                                                  
.         if ("`f'"=="nnls1") {
 34.                 mat W = e(weights)
 35.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 36.                 assert reldif(`sum',1)<0.001
 37.         }
 38.         if ("`f'"=="singlebest") {
 39.                 mat W = e(weights)
 40.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 41.                 assert reldif(`sum',1)<0.001
 42.         }
 43.         if ("`f'"=="avg") {
 44.                 mat W = e(weights)
 45.                 assert reldif(el(W,1,1),0.25)<0.001
 46.                 assert reldif(el(W,2,1),0.25)<0.001
 47.                 assert reldif(el(W,3,1),0.25)<0.001
 48.                 assert reldif(el(W,4,1),0.25)<0.001
 49.         }
 50.                                                  
. }
 51. }
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2927002
  ridgecv        |      0.4014688
  gradboost      |      0.1116836
  nnet           |      0.2460908
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.5805509
  gradboost      |      0.3399453
  nnet           |      0.0795038
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818471
  ridgecv        |      0.8136712
  gradboost      |      0.0000000
  nnet           |      0.1044816
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0207010
  lassocv        |      0.6999931
  gradboost      |      0.1672110
  nnet           |      0.1120949
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169289
  elasticcv      |      0.2455163
  gradboost      |      0.3103246
  nnet           |      0.4451758
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4022450
  rf             |      0.5977550
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1268245
  elasticcv      |      0.5723711
  gradboost      |      0.3008044
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  lassocv        |      0.0000000
  rf             |      0.8384152
  gradboost      |      0.1615848

. 
. mat W = e(weights)

. assert reldif(0,el(W,1,1))<0.005

. assert reldif(0,el(W,2,1))<0.005

. assert reldif(0,el(W,3,1))<0.005

. assert reldif(0.8382714,el(W,4,1))<0.005

. assert reldif(0.1617286,el(W,5,1))<0.005

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. gen fid = 1 + (_n>250)

.         
. pystacked medv $xvars, method(ols rf) foldvar(fid)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209751
  gradboost      |      0.2351843
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  rf             |      1.0000000

. predict yb , basexb cvalid

. 
. reg medv $xvars if _n<=250

      Source |       SS           df       MS      Number of obs   =       250
-------------+----------------------------------   F(13, 236)      =    107.69
       Model |  14969.2255        13  1151.47888   Prob > F        =    0.0000
    Residual |  2523.45072       236  10.6925878   R-squared       =    0.8557
-------------+----------------------------------   Adj R-squared   =    0.8478
       Total |  17492.6762       249  70.2517116   Root MSE        =      3.27

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |   1.208708   .5165797     2.34   0.020     .1910112    2.226404
          zn |   .0246793   .0139997     1.76   0.079     -.002901    .0522596
       indus |   .0101453    .051355     0.20   0.844    -.0910274    .1113181
        chas |   .4227349   .8157238     0.52   0.605    -1.184296    2.029765
         nox |  -8.480356   4.493227    -1.89   0.060    -17.33231    .3716021
          rm |   8.865556   .4935031    17.96   0.000     7.893322     9.83779
         age |   -.048789   .0117209    -4.16   0.000      -.07188    -.025698
         dis |  -1.123173   .2121414    -5.29   0.000    -1.541106   -.7052403
         rad |   .2089508   .1511725     1.38   0.168    -.0888692    .5067707
         tax |  -.0160083   .0039309    -4.07   0.000    -.0237525   -.0082641
     ptratio |  -.6580104   .1244101    -5.29   0.000    -.9031065   -.4129142
           b |   .0178036   .0052194     3.41   0.001      .007521    .0280863
       lstat |  -.1155318    .057027    -2.03   0.044    -.2278789   -.0031847
       _cons |   -9.94033   5.878804    -1.69   0.092    -21.52197    1.641306
------------------------------------------------------------------------------

. predict ols1 if _n>250
(option xb assumed; fitted values)
(250 missing values generated)

. reg medv $xvars if _n>250

      Source |       SS           df       MS      Number of obs   =       256
-------------+----------------------------------   F(13, 242)      =     52.56
       Model |  17507.9606        13   1346.7662   Prob > F        =    0.0000
    Residual |  6200.28792       242  25.6210244   R-squared       =    0.7385
-------------+----------------------------------   Adj R-squared   =    0.7244
       Total |  23708.2486       255  92.9735238   Root MSE        =    5.0617

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |  -.1115731   .0360786    -3.09   0.002    -.1826413    -.040505
          zn |   .0467109   .0226302     2.06   0.040     .0021336    .0912883
       indus |  -.1248697   .1586528    -0.79   0.432    -.4373864     .187647
        chas |   6.203216   1.432213     4.33   0.000      3.38202    9.024411
         nox |  -25.37038    6.91223    -3.67   0.000    -38.98619   -11.75456
          rm |   .8986557   .5743273     1.56   0.119     -.232663    2.029974
         age |    .029812   .0256754     1.16   0.247    -.0207637    .0803878
         dis |  -1.569539     .33795    -4.64   0.000    -2.235238   -.9038402
         rad |   .3418979   .0982913     3.48   0.001     .1482821    .5355137
         tax |  -.0045875   .0070141    -0.65   0.514     -.018404    .0092289
     ptratio |  -1.387848   .2406709    -5.77   0.000    -1.861925   -.9137712
           b |   .0058822   .0031425     1.87   0.062    -.0003079    .0120723
       lstat |  -.7205029   .0727863    -9.90   0.000    -.8638784   -.5771273
       _cons |   66.56659   7.426122     8.96   0.000      51.9385    81.19467
------------------------------------------------------------------------------

. predict ols2 if _n<=250
(option xb assumed; fitted values)
(256 missing values generated)

. gen ols_crossfit = ols1 if _n>250
(250 missing values generated)

. replace ols_crossfit = ols2 if _n<=250
(250 real changes made)

. 
. assert reldif(ols_crossfit , yb1)<10e-6

. 
. 
. *******************************************************************************
. *** pystacked with one predictor                                                                                        ***
. *******************************************************************************
. 
. sysuse auto , clear
(1978 automobile data)

. pystacked price mpg, type(reg) m(ols)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      1.0000000

. predict double xhat1

. 
. reg price mpg

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(1, 72)        =     20.26
       Model |   139449474         1   139449474   Prob > F        =    0.0000
    Residual |   495615923        72  6883554.48   R-squared       =    0.2196
-------------+----------------------------------   Adj R-squared   =    0.2087
       Total |   635065396        73  8699525.97   Root MSE        =    2623.7

------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879
       _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03
------------------------------------------------------------------------------

. predict double xhat2 
(option xb assumed; fitted values)

. 
. assert reldif(xhat1,xhat2)<10e-6

. 
. 
. *******************************************************************************
. *** check stdscaler default with regularized linear learners                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. pystacked medv $xvars, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1247955
  lassocv        |      0.5569869
  rf             |      0.1380195
  nnet           |      0.1801982
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175438
  elasticcv      |      0.6823223
  gradboost      |      0.0497144
  svm            |      0.2618175
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8433028
  gradboost      |      0.1566972
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9835543
  lassocv        |      0.0164457

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars, method(gradboost lassocv) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539241
  elasticcv      |      0.8787627
  gradboost      |      0.0504939
  svm            |      0.0168193
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9639648
  lassocv        |      0.0360352

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

.         
. pystacked medv $xvars || m(gradboost) || m(lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9271398
  lassocv        |      0.0728602

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars || m(gradboost) || m(lassocv) pipe(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9722477
  lassocv        |      0.0277523

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

. 
. 
. *******************************************************************************
. *** xvar option                                                                                                                         ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. global xuse c.(crim lstat)##c.(crim lstat)

. global xall c.(crim-lstat)##c.(crim-lstat)

. 
. set seed 789

. pystacked medv $xuse, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb

.  
. set seed 789

. pystacked medv $xall, method(gradboost lassocv) xvars1($xuse) xvars2($xuse) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818471
  ridgecv        |      0.8136712
  gradboost      |      0.0000000
  nnet           |      0.1044816
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb2

. 
. set seed 789

. pystacked medv $xall || method(gradboost) xvars($xuse) || m(lassocv) xvars($xuse),  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb3

. 
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1($xuse) xvars2($xuse)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7474007
  lassocv        |      0.2525993

. predict double xb4

. 
. ** this should be different
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb5

. 
. assert reldif(xb,xb2)<1e-5

. assert reldif(xb,xb3)<1e-5

. assert reldif(xb,xb4)<1e-5

. assert xb!=xb5

. 
. 
. *******************************************************************************
. *** xvar vs pipeline                                                                                                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars2(c.(crim-lstat)##c.(crim-lstat))  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803995
  lassocv        |      0.4215865
  rf             |      0.3499628
  nnet           |      0.1642276
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1268245
  elasticcv      |      0.5723711
  gradboost      |      0.3008044
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209751
  gradboost      |      0.2351843
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222623
  lassocv        |      0.1777377

. predict double xb1

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) pipe2(poly2) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539241
  elasticcv      |      0.8787627
  gradboost      |      0.0504939
  svm            |      0.0168193
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2926994
  ridgecv        |      0.4014700
  gradboost      |      0.1116836
  nnet           |      0.2460907
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169289
  elasticcv      |      0.2455163
  gradboost      |      0.3103246
  nnet           |      0.4451758
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222622
  lassocv        |      0.1777378

. predict double xb2

. 
. assert reldif(xb1,xb2)<1e-4

. 
. 
. *******************************************************************************
. *** voting                                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, tab clear
(11 vars, 97 obs)

. 
. global xvars lcavol-pgg

. 
. set seed 124345

. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5000000
  lassoic        |      0.1000000
  rf             |      0.4000000

. 
. // should cause error
. cap pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .9)      

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

. assert _rc == 198                                                

.                                         
. *******************************************************************************
. *** check pipeline                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf ) ///
>                                                  pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175438
  elasticcv      |      0.6823223
  gradboost      |      0.0497144
  svm            |      0.2618175
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0815535
  lassoic        |      0.8189498
  rf             |      0.0994968

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic rf ) pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe3) : "poly2"
             e(pyopt3) : "{'n_estimators':100,'criterion':'mse','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_fraction_l.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe2) : "poly2"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars1) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe1) : "poly2"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "0.24.2"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  45 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict a, basexb

. 
.  
. pystacked lpsa c.($xvars)##c.($xvars), ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic  rf ) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803995
  lassocv        |      0.4215865
  rf             |      0.3499628
  nnet           |      0.1642276
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols
note: svi omitted because of collinearity.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassoic        |      0.7533920
  rf             |      0.2466080

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic  rf ) pipe2(nostdscaler)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe3) : "passthrough"
             e(pyopt3) : "{'n_estimators':100,'criterion':'mse','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_fraction_l.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe2) : "passthrough"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars1) : "lcavol lweight age lbph lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __0000.."
              e(pipe1) : "passthrough"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.10.1"
          e(numpy_ver) : "1.24.2"
        e(sklearn_ver) : "0.24.2"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  44 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict b, basexb

. list lpsa a* b* if _n <= 10

     +-----------------------------------------------------------------------------------+
     |      lpsa   age         a1         a2         a3         b1         b2         b3 |
     |-----------------------------------------------------------------------------------|
  1. | -.4307829    50   .2531799    1.19435   .3325487   .2531799    1.19435   .3325487 |
  2. | -.1625189    58   .5338573   1.205396   .0659286   .5338573   1.205396   .0659286 |
  3. | -.1625189    74   .2774684   1.251625   .3647012   .2774684   1.251625   .3647012 |
  4. | -.1625189    58   .4093789   1.101081   .0859689   .4093789   1.101081   .0859689 |
  5. |  .3715636    62   1.388298   2.006096   .6633989   1.388298   2.006096   .6633989 |
     |-----------------------------------------------------------------------------------|
  6. |  .7654678    50   .6148925   1.149503   .4205081   .6148925   1.149503   .4205081 |
  7. |  .7654678    64   1.843947   2.014224   1.027373   1.843947   2.014224   1.027373 |
  8. |  .8544153    58   1.820537   2.018032   1.162384   1.820537   2.018032   1.162384 |
  9. |  1.047319    47   1.383648   1.377017   .7764618   1.383648   1.377017   .7764618 |
 10. |  1.047319    63   .8287085   1.710201   1.236331   .8287085   1.710201   1.236331 |
     +-----------------------------------------------------------------------------------+

. 
. assert reldif(a1,b1)<1e-5

. assert reldif(a2,b2)<1e-5

. 
. *******************************************************************************
. *** check that xvar() subsetting works                                                                          ***
. *******************************************************************************
. 
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv crim lstat, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb2

. 
. set seed 789

. pystacked medv crim-lstat || method(gradboost) xvars(crim lstat) || m(lassocv) xvars(crim lstat), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821268
  lassocv        |      0.1178732

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 26.407074   26.407074   26.407074 |
  2. | 22.074846   22.074846   22.074846 |
  3. |  34.43726    34.43726    34.43726 |
  4. | 35.627232   35.627232   35.627232 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *** with factor variables
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv i.rad##c.crim, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat), method(gradboost lassocv) xvars1(i.rad##c.crim) xvars2(i.rad##c.crim) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2926994
  ridgecv        |      0.4014700
  gradboost      |      0.1116836
  nnet           |      0.2460907
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb2

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat) || method(gradboost) xvars(i.rad##c.crim) || m(lassocv) xvars(i.rad##c.crim), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 25.944031   25.944031   25.944031 |
  2. | 27.627625   27.627625   27.627625 |
  3. | 29.591053   29.591053   29.591053 |
  4. | 30.817581   30.817581   30.817581 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *******************************************************************************
. *** try various combinations of estimators                                                                      ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0
(11 vars, 97 obs)

. 
. local m1 ols lassocv gradboost nnet

. local m2 ols lassocv rf nnet

. local m3 ols ridgecv gradboost nnet

. local m4 ols elasticcv gradboost nnet

. local m5 ols elasticcv gradboost svm

. 
. gen lpsa1 = lpsa>2.47

. 
. local flearners nnls1 nnls0 singlebest ols avg

. 
. foreach f of local flearners  {
  2. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" {
  3. 
.                 
.         di "---------------------------------"
  4.         di "Regress"
  5.         di "Candidate learners: `m'"
  6.         di "Final estimator: `f'"
  7.         
.         pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(`m') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
  8.                                                  
.         if ("`f'"=="nnls1") {
  9.                 mat W = e(weights)
 10.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 11.                 assert reldif(`sum',1)<0.001
 12.         }
 13.         if ("`f'"=="singlebest") {
 14.                 mat W = e(weights)
 15.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 16.                 assert reldif(`sum',1)<0.001
 17.         }
 18.         if ("`f'"=="avg") {
 19.                 mat W = e(weights)
 20.                 assert reldif(el(W,1,1),0.25)<0.001
 21.                 assert reldif(el(W,2,1),0.25)<0.001
 22.                 assert reldif(el(W,3,1),0.25)<0.001
 23.                 assert reldif(el(W,4,1),0.25)<0.001
 24.         }
 25.         
.         local mc = subinstr("`m'","ols","logit",.)
 26.         local mc = subinstr("`mc'"," lassoic","",.)
 27.         local mc = subinstr("`mc'"," linsvm","",.)
 28.                                                  
.         di "---------------------------------"
 29.         di "Classify"
 30.         di "Candidate learners: `m'"
 31.         di "Final estimator: `f'"
 32.                                                  
.         pystacked lpsa1 lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(class) pyseed(243) ///
>                                                  methods(`mc') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
 33.                                                  
.         if ("`f'"=="nnls1") {
 34.                 mat W = e(weights)
 35.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 36.                 assert reldif(`sum',1)<0.001
 37.         }
 38.         if ("`f'"=="singlebest") {
 39.                 mat W = e(weights)
 40.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 41.                 assert reldif(`sum',1)<0.001
 42.         }
 43.         if ("`f'"=="avg") {
 44.                 mat W = e(weights)
 45.                 assert reldif(el(W,1,1),0.25)<0.001
 46.                 assert reldif(el(W,2,1),0.25)<0.001
 47.                 assert reldif(el(W,3,1),0.25)<0.001
 48.                 assert reldif(el(W,4,1),0.25)<0.001
 49.         }
 50.                                                  
. }
 51. }
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0324857
  lassocv        |      0.6030728
  rf             |     -0.0279832
  nnet           |      0.2789362
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.5805509
  gradboost      |      0.3399453
  nnet           |      0.0795038
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169289
  elasticcv      |      0.2455163
  gradboost      |      0.3103246
  nnet           |      0.4451758
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0199229
  lassocv        |      0.6982814
  gradboost      |      0.1709262
  nnet           |      0.1108695
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4042571
  rf             |      0.5957429
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505684
  ridgecv        |      0.1149962
  gradboost      |      0.1986833
  nnet           |      0.5097985
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175438
  elasticcv      |      0.6823223
  gradboost      |      0.0497144
  svm            |      0.2618175
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1302032
  lassocv        |      0.5995944
  rf             |      0.0230851
  nnet           |      0.2471174
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8472496
  gradboost      |      0.1527504
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804428
  elasticcv      |      0.9703585
  gradboost      |     -0.3040851
  nnet           |      0.3350215
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818471
  ridgecv        |      0.8136714
  gradboost      |      0.0000000
  nnet           |      0.1044815
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1268245
  elasticcv      |      0.5723710
  gradboost      |      0.3008045
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209751
  gradboost      |      0.2351843
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386918
  lassocv        |      0.5760072
  rf             |      0.0474635
  nnet           |      0.2318628
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539323
  elasticcv      |      0.8788342
  gradboost      |      0.0503982
  svm            |      0.0168354
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505683
  ridgecv        |      0.1149965
  gradboost      |      0.1986832
  nnet           |      0.5097984
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362264
  lassocv        |      0.4458344
  rf             |      0.5157711
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1035802
  lassocv        |      0.3790606
  rf             |      0.5437758
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804428
  elasticcv      |      0.9703585
  gradboost      |     -0.3040851
  nnet           |      0.3350215
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  lassocv        |      0.0000000
  rf             |      0.8382666
  gradboost      |      0.1617334

. 
. mat W = e(weights)

. assert reldif(0,el(W,1,1))<0.005

. assert reldif(0,el(W,2,1))<0.005

. assert reldif(0,el(W,3,1))<0.005

. assert reldif(0.8382714,el(W,4,1))<0.005

. assert reldif(0.1617286,el(W,5,1))<0.005

. 
. 
. *******************************************************************************
. *** foldvar                                                                                                                                     ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. gen fid = 1 + (_n>250)

.         
. pystacked medv $xvars, method(ols rf) foldvar(fid)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  rf             |      1.0000000

. predict yb , basexb cvalid

. 
. reg medv $xvars if _n<=250

      Source |       SS           df       MS      Number of obs   =       250
-------------+----------------------------------   F(13, 236)      =    107.69
       Model |  14969.2255        13  1151.47888   Prob > F        =    0.0000
    Residual |  2523.45072       236  10.6925878   R-squared       =    0.8557
-------------+----------------------------------   Adj R-squared   =    0.8478
       Total |  17492.6762       249  70.2517116   Root MSE        =      3.27

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |   1.208708   .5165797     2.34   0.020     .1910112    2.226404
          zn |   .0246793   .0139997     1.76   0.079     -.002901    .0522596
       indus |   .0101453    .051355     0.20   0.844    -.0910274    .1113181
        chas |   .4227349   .8157238     0.52   0.605    -1.184296    2.029765
         nox |  -8.480356   4.493227    -1.89   0.060    -17.33231    .3716021
          rm |   8.865556   .4935031    17.96   0.000     7.893322     9.83779
         age |   -.048789   .0117209    -4.16   0.000      -.07188    -.025698
         dis |  -1.123173   .2121414    -5.29   0.000    -1.541106   -.7052403
         rad |   .2089508   .1511725     1.38   0.168    -.0888692    .5067707
         tax |  -.0160083   .0039309    -4.07   0.000    -.0237525   -.0082641
     ptratio |  -.6580104   .1244101    -5.29   0.000    -.9031065   -.4129142
           b |   .0178036   .0052194     3.41   0.001      .007521    .0280863
       lstat |  -.1155318    .057027    -2.03   0.044    -.2278789   -.0031847
       _cons |   -9.94033   5.878804    -1.69   0.092    -21.52197    1.641306
------------------------------------------------------------------------------

. predict ols1 if _n>250
(option xb assumed; fitted values)
(250 missing values generated)

. reg medv $xvars if _n>250

      Source |       SS           df       MS      Number of obs   =       256
-------------+----------------------------------   F(13, 242)      =     52.56
       Model |  17507.9606        13   1346.7662   Prob > F        =    0.0000
    Residual |  6200.28792       242  25.6210244   R-squared       =    0.7385
-------------+----------------------------------   Adj R-squared   =    0.7244
       Total |  23708.2486       255  92.9735238   Root MSE        =    5.0617

------------------------------------------------------------------------------
        medv | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
        crim |  -.1115731   .0360786    -3.09   0.002    -.1826413    -.040505
          zn |   .0467109   .0226302     2.06   0.040     .0021336    .0912883
       indus |  -.1248697   .1586528    -0.79   0.432    -.4373864     .187647
        chas |   6.203216   1.432213     4.33   0.000      3.38202    9.024411
         nox |  -25.37038    6.91223    -3.67   0.000    -38.98619   -11.75456
          rm |   .8986557   .5743273     1.56   0.119     -.232663    2.029974
         age |    .029812   .0256754     1.16   0.247    -.0207637    .0803878
         dis |  -1.569539     .33795    -4.64   0.000    -2.235238   -.9038402
         rad |   .3418979   .0982913     3.48   0.001     .1482821    .5355137
         tax |  -.0045875   .0070141    -0.65   0.514     -.018404    .0092289
     ptratio |  -1.387848   .2406709    -5.77   0.000    -1.861925   -.9137712
           b |   .0058822   .0031425     1.87   0.062    -.0003079    .0120723
       lstat |  -.7205029   .0727863    -9.90   0.000    -.8638784   -.5771273
       _cons |   66.56659   7.426122     8.96   0.000      51.9385    81.19467
------------------------------------------------------------------------------

. predict ols2 if _n<=250
(option xb assumed; fitted values)
(256 missing values generated)

. gen ols_crossfit = ols1 if _n>250
(250 missing values generated)

. replace ols_crossfit = ols2 if _n<=250
(250 real changes made)

. 
. assert reldif(ols_crossfit , yb1)<10e-6

. 
. 
. *******************************************************************************
. *** pystacked with one predictor                                                                                        ***
. *******************************************************************************
. 
. sysuse auto , clear
(1978 automobile data)

. pystacked price mpg, type(reg) m(ols)
Single base learner: no stacking or cross-validation done.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      1.0000000

. predict double xhat1

. 
. reg price mpg

      Source |       SS           df       MS      Number of obs   =        74
-------------+----------------------------------   F(1, 72)        =     20.26
       Model |   139449474         1   139449474   Prob > F        =    0.0000
    Residual |   495615923        72  6883554.48   R-squared       =    0.2196
-------------+----------------------------------   Adj R-squared   =    0.2087
       Total |   635065396        73  8699525.97   Root MSE        =    2623.7

------------------------------------------------------------------------------
       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]
-------------+----------------------------------------------------------------
         mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879
       _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03
------------------------------------------------------------------------------

. predict double xhat2 
(option xb assumed; fitted values)

. 
. assert reldif(xhat1,xhat2)<10e-6

. 
. 
. *******************************************************************************
. *** check stdscaler default with regularized linear learners                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

. 
. pystacked medv $xvars, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2927839
  ridgecv        |      0.4019069
  gradboost      |      0.1108738
  nnet           |      0.2463880
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9835541
  lassocv        |      0.0164459

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars, method(gradboost lassocv) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9639648
  lassocv        |      0.0360352

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

.         
. pystacked medv $xvars || m(gradboost) || m(lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386918
  lassocv        |      0.5760072
  rf             |      0.0474635
  nnet           |      0.2318628
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9271398
  lassocv        |      0.0728602

. di "`e(pipe2)'"
stdscaler

. assert "`e(pipe2)'"=="stdscaler"

. 
. pystacked medv $xvars || m(gradboost) || m(lassocv) pipe(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169880
  elasticcv      |      0.2447005
  gradboost      |      0.3137330
  nnet           |      0.4418860
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.9722477
  lassocv        |      0.0277523

. di "`e(pipe2)'"
passthrough

. assert "`e(pipe2)'"=="passthrough"

. 
. 
. *******************************************************************************
. *** xvar option                                                                                                                         ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. global xuse c.(crim lstat)##c.(crim lstat)

. global xall c.(crim-lstat)##c.(crim-lstat)

. 
. set seed 789

. pystacked medv $xuse, method(gradboost lassocv)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7472712
  lassocv        |      0.2527288

. predict double xb

.  
. set seed 789

. pystacked medv $xall, method(gradboost lassocv) xvars1($xuse) xvars2($xuse) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175501
  elasticcv      |      0.6824137
  gradboost      |      0.0495716
  svm            |      0.2618651
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505683
  ridgecv        |      0.1149965
  gradboost      |      0.1986832
  nnet           |      0.5097984
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7472712
  lassocv        |      0.2527288

. predict double xb2

. 
. set seed 789

. pystacked medv $xall || method(gradboost) xvars($xuse) || m(lassocv) xvars($xuse),  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7472712
  lassocv        |      0.2527288

. predict double xb3

. 
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1($xuse) xvars2($xuse)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.7472712
  lassocv        |      0.2527288

. predict double xb4

. 
. ** this should be different
. set seed 789

. pystacked medv crim, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat)   

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821028
  lassocv        |      0.1178972

. predict double xb5

. 
. assert reldif(xb,xb2)<1e-5

. assert reldif(xb,xb3)<1e-5

. assert reldif(xb,xb4)<1e-5

. assert xb!=xb5

. 
. 
. *******************************************************************************
. *** xvar vs pipeline                                                                                                            ***
. *******************************************************************************
. 
. insheet using "/Users/kahrens/Dropbox (PP)/ddml/Data/housing.csv", ///
>         clear comma
(14 vars, 506 obs)

.         
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars2(c.(crim-lstat)##c.(crim-lstat))  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804428
  elasticcv      |      0.9703585
  gradboost      |     -0.3040851
  nnet           |      0.3350215
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222622
  lassocv        |      0.1777378

. predict double xb1

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) pipe2(poly2) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. // in-sample predictions
. pystacked, graph
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8222957
  lassocv        |      0.1777043

. predict double xb2

. 
. assert reldif(xb1,xb2)<1e-4

. 
. 
. *******************************************************************************
. *** voting                                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data, tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)
(11 vars, 97 obs)

. 
. global xvars lcavol-pgg

. 
. set seed 124345

. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.5000000
  lassoic        |      0.1000000
  rf             |      0.4000000

. 
. // should cause error
. cap pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf) ///
>                                                  pipe1(poly2) pipe2(poly2) /// 
>                                                  voting voteweights(.5 .9)      

. assert _rc == 198                                                

.                                         
. *******************************************************************************
. *** check pipeline                                                                                                                      ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. 
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf ) ///
>                                                  pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)  

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0611815
  lassoic        |      0.4296642
  rf             |      0.5091543

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic rf ) pipe1(poly2) pipe2(poly2 nostdscaler) pipe3(poly2)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe3) : "poly2"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe2) : "poly2"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45"
             e(xvars1) : "lcavol lweight age lbph svi lcp gleason pgg45"
              e(pipe1) : "poly2"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.11.1"
          e(numpy_ver) : "1.25.2"
        e(sklearn_ver) : "1.3.0"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  45 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict a, basexb

. 
.  
. pystacked lpsa c.($xvars)##c.($xvars), ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic  rf ) pipe2(nostdscaler)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols
note: svi omitted because of collinearity.

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassoic        |      0.3825348
  rf             |      0.6174652

. ereturn list

scalars:
                  e(N) =  97
             e(mcount) =  3

macros:
          e(globalopt) : "type(regress) pyseed(243) methods(ols lassoic  rf ) pipe2(nostdscaler)"
           e(xvars_o3) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars3) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe3) : "passthrough"
             e(pyopt3) : "{'n_estimators':100,'criterion':'squared_error','max_depth':None,'min_samples_split':2,'min_samples_leaf':1,'min_weight_.."
            e(method3) : "rf"
           e(xvars_o2) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars2) : "lcavol lweight age lbph svi lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __.."
              e(pipe2) : "passthrough"
             e(pyopt2) : "{'criterion':'aic','fit_intercept':True,'max_iter':500,'positive':False}"
            e(method2) : "lassoic"
           e(xvars_o1) : "lcavol lweight age lbph svi lcp gleason pgg45 c.lcavol#c.lcavol c.lcavol#c.lweight c.lcavol#c.age c.lcavol#c.lbph c.lcav.."
             e(xvars1) : "lcavol lweight age lbph lcp gleason pgg45 __000005 __000006 __000007 __000008 __000009 __00000A __00000B __00000C __0000.."
              e(pipe1) : "passthrough"
             e(pyopt1) : "{'fit_intercept':True,'positive':False}"
            e(method1) : "ols"
           e(finalest) : "nnls1"
               e(type) : "reg"
             e(depvar) : "lpsa"
            e(predict) : "pystacked_p"
                e(cmd) : "pystacked"
         e(python_ver) : "3.10.13 (main, Aug 24 2023, 22:46:18) [Clang 14.0.0 (clang-1400.0.29.202)]"
          e(scipy_ver) : "1.11.1"
          e(numpy_ver) : "1.25.2"
        e(sklearn_ver) : "1.3.0"
           e(base_est) : "ols lassoic rf"

matrices:
             e(coefs3) :  44 x 1
             e(coefs2) :  45 x 1
             e(coefs1) :  44 x 1
            e(weights) :  3 x 1

functions:
             e(sample)   

. predict b, basexb

. list lpsa a* b* if _n <= 10

     +-----------------------------------------------------------------------------------+
     |      lpsa   age         a1         a2         a3         b1         b2         b3 |
     |-----------------------------------------------------------------------------------|
  1. | -.4307829    50   .2531799   1.108728   .3325487   .2531799   1.108728   .3325487 |
  2. | -.1625189    58   .5338573    .828543   .0659286   .5338573    .828543   .0659286 |
  3. | -.1625189    74   .2774684   .2845545   .3647012   .2774684   .2845545   .3647012 |
  4. | -.1625189    58   .4093789   .7067597   .0859689   .4093789   .7067597   .0859689 |
  5. |  .3715636    62   1.388298   1.639569   .6633989   1.388298   1.639569   .6633989 |
     |-----------------------------------------------------------------------------------|
  6. |  .7654678    50   .6148925   1.082111   .4205081   .6148925   1.082111   .4205081 |
  7. |  .7654678    64   1.843947   1.833611   1.027373   1.843947   1.833611   1.027373 |
  8. |  .8544153    58   1.820537   2.099344   1.162384   1.820537   2.099344   1.162384 |
  9. |  1.047319    47   1.383648   1.410242   .7764618   1.383648   1.410242   .7764618 |
 10. |  1.047319    63   .8287085    1.22876   1.236331   .8287085    1.22876   1.236331 |
     +-----------------------------------------------------------------------------------+

. 
. assert reldif(a1,b1)<1e-5

. assert reldif(a2,b2)<1e-5

. 
. *******************************************************************************
. *** check that xvar() subsetting works                                                                          ***
. *******************************************************************************
. 
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv crim lstat, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821028
  lassocv        |      0.1178972

. predict double xb

. 
. set seed 789

. pystacked medv crim-lstat, method(gradboost lassocv) xvars1(crim lstat) xvars2(crim lstat) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821028
  lassocv        |      0.1178972

. predict double xb2

. 
. set seed 789

. pystacked medv crim-lstat || method(gradboost) xvars(crim lstat) || m(lassocv) xvars(crim lstat), pyseed(-1)

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.8821028
  lassocv        |      0.1178972

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 26.407166   26.407166   26.407166 |
  2. | 22.074953   22.074953   22.074953 |
  3. | 34.437157   34.437157   34.437157 |
  4. | 35.627125   35.627125   35.627125 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *** with factor variables
. 
. insheet using https://statalasso.github.io/dta/housing.csv, clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg
(14 vars, 506 obs)

. 
. set seed 789

. pystacked medv i.rad##c.crim, method(gradboost lassocv) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051820
  lassocv        |      0.6424728
  rf             |      0.2677433
  nnet           |     -0.2781696
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat), method(gradboost lassocv) xvars1(i.rad##c.crim) xvars2(i.rad##c.crim) pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0324857
  lassocv        |      0.6030728
  rf             |     -0.0279832
  nnet           |      0.2789362
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb2

. 
. set seed 789

. pystacked medv i.rad##c.(crim-lstat) || method(gradboost) xvars(i.rad##c.crim) || m(lassocv) xvars(i.rad##c.crim), pyseed(-1)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  gradboost      |      0.4791765
  lassocv        |      0.5208235

. predict double xb3

. list xb* if _n<5

     +-----------------------------------+
     |        xb         xb2         xb3 |
     |-----------------------------------|
  1. | 25.944031   25.944031   25.944031 |
  2. | 27.627625   27.627625   27.627625 |
  3. | 29.591053   29.591053   29.591053 |
  4. | 30.817581   30.817581   30.817581 |
     +-----------------------------------+

. assert reldif(xb,xb2)<10e-9

. assert reldif(xb,xb3)<10e-9

. 
. 
. *******************************************************************************
. *** try various combinations of estimators                                                                      ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 
(11 vars, 97 obs)

. 
. local m1 ols lassocv gradboost nnet

. local m2 ols lassocv rf nnet

. local m3 ols ridgecv gradboost nnet

. local m4 ols elasticcv gradboost nnet

. local m5 ols elasticcv gradboost svm

. 
. gen lpsa1 = lpsa>2.47

. 
. local flearners nnls1 nnls0 singlebest ols avg

. 
. foreach f of local flearners  {
  2. foreach m in "`m1'" "`m2'" "`m3'" "`m4'" "`m5'" {
  3. 
.                 
.         di "---------------------------------"
  4.         di "Regress"
  5.         di "Candidate learners: `m'"
  6.         di "Final estimator: `f'"
  7.         
.         pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(`m') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
  8.                                                  
.         if ("`f'"=="nnls1") {
  9.                 mat W = e(weights)
 10.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 11.                 assert reldif(`sum',1)<0.001
 12.         }
 13.         if ("`f'"=="singlebest") {
 14.                 mat W = e(weights)
 15.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 16.                 assert reldif(`sum',1)<0.001
 17.         }
 18.         if ("`f'"=="avg") {
 19.                 mat W = e(weights)
 20.                 assert reldif(el(W,1,1),0.25)<0.001
 21.                 assert reldif(el(W,2,1),0.25)<0.001
 22.                 assert reldif(el(W,3,1),0.25)<0.001
 23.                 assert reldif(el(W,4,1),0.25)<0.001
 24.         }
 25.         
.         local mc = subinstr("`m'","ols","logit",.)
 26.         local mc = subinstr("`mc'"," lassoic","",.)
 27.         local mc = subinstr("`mc'"," linsvm","",.)
 28.                                                  
.         di "---------------------------------"
 29.         di "Classify"
 30.         di "Candidate learners: `m'"
 31.         di "Final estimator: `f'"
 32.                                                  
.         pystacked lpsa1 lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(class) pyseed(243) ///
>                                                  methods(`mc') /// 
>                                                  njobs(4) ///
>                                                  pipe2(poly2) pipe1(poly2) ///
>                                                  finalest(`f') 
 33.                                                  
.         if ("`f'"=="nnls1") {
 34.                 mat W = e(weights)
 35.                 local sum = el(W,1,1)+el(W,2,1)+el(W,3,1)+el(W,4,1)
 36.                 assert reldif(`sum',1)<0.001
 37.         }
 38.         if ("`f'"=="singlebest") {
 39.                 mat W = e(weights)
 40.                 local sum = (el(W,1,1)>0)+(el(W,2,1)>0)+(el(W,3,1)>0)+(el(W,4,1)>0)
 41.                 assert reldif(`sum',1)<0.001
 42.         }
 43.         if ("`f'"=="avg") {
 44.                 mat W = e(weights)
 45.                 assert reldif(el(W,1,1),0.25)<0.001
 46.                 assert reldif(el(W,2,1),0.25)<0.001
 47.                 assert reldif(el(W,3,1),0.25)<0.001
 48.                 assert reldif(el(W,4,1),0.25)<0.001
 49.         }
 50.                                                  
. }
 51. }
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. // in-sample predictions
. pystacked, graph
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.5805509
  gradboost      |      0.3399454
  nnet           |      0.0795038
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505684
  ridgecv        |      0.1149962
  gradboost      |      0.1986833
  nnet           |      0.5097985
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0207010
  lassocv        |      0.6999931
  gradboost      |      0.1672110
  nnet           |      0.1120949
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804338
  elasticcv      |      0.9703027
  gradboost      |     -0.3040140
  nnet           |      0.3349884
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.4022478
  rf             |      0.5977522
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1247955
  lassocv        |      0.5569869
  rf             |      0.1380195
  nnet           |      0.1801982
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      0.8433028
  gradboost      |      0.1566972
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0818471
  ridgecv        |      0.8136712
  gradboost      |      0.0000000
  nnet           |      0.1044816
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7994770
  gradboost      |      0.2005230
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1268245
  elasticcv      |      0.5723711
  gradboost      |      0.3008044
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306317
  rf             |      0.3943913

. // in-sample predictions
. pystacked, graph

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0438406
  elasticcv      |      0.7209759
  gradboost      |      0.2351836
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls1

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_102.txt
  log type:  text
 closed on:   3 Jan 2025, 23:00:17
------------------------------------------------------------------------------------------------------------------------------------------------------


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0539241
  elasticcv      |      0.8787627
  gradboost      |      0.0504939
  svm            |      0.0168193
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0506805
  lassocv        |      0.7987339
  gradboost      |      0.1427979
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0258772
  lassocv        |      0.1237956
  gradboost      |      0.5807496
  nnet           |      0.2601174
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0362671
  lassocv        |      0.4455111
  rf             |      0.5161028
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0803995
  lassocv        |      0.4215865
  rf             |      0.3499628
  nnet           |      0.1642276
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0667473
  ridgecv        |      0.9393065
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1741808
  lassocv        |      0.4270701
  rf             |      0.3987491

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.469            .            .
  ols            | 0.174       0.501        1.179            .
  lassocv        | 0.427       0.689        0.836            .
  rf             | 0.399       0.288        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2926994
  ridgecv        |      0.4014700
  gradboost      |      0.1116836
  nnet           |      0.2460907
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      0.7571634
  gradboost      |      0.2544217
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: nnls0

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6592350
  rf             |      0.3407650

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.659       0.546        0.648        1.441
  rf             | 0.341       0.258        0.675        1.530

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.659       0.546        0.648        1.586
  rf             | 0.341       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_113.txt
  log type:  text
 closed on:   3 Jan 2025, 23:00:38
------------------------------------------------------------------------------------------------------------------------------------------------------


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0169289
  elasticcv      |      0.2455163
  gradboost      |      0.3103246
  nnet           |      0.4451758
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0721744
  elasticcv      |      0.4946972
  gradboost      |      0.3948518
  svm            |      0.0493350
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: nnls0

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0175438
  elasticcv      |      0.6823223
  gradboost      |      0.0497144
  svm            |      0.2618175
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.530

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4355100
  lassocv        |      0.0000000
  rf             |      0.5644900

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.666
  ols            | 0.436       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.564       0.258        0.618        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: singlebest

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1741808
  lassocv        |      0.4270701
  rf             |      0.3987491

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.0000000
  rf             |      1.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: singlebest
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  lassocv        |      1.0000000
  rf             |      0.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  ridgecv        |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: singlebest

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  ridgecv        |      0.0000000
  gradboost      |      1.0000000
  nnet           |      0.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6592350
  rf             |      0.3407650

. // in-sample predictions
. pystacked, graph

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  nnet           |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: singlebest

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_121.txt
  log type:  text
 closed on:   3 Jan 2025, 23:00:57
------------------------------------------------------------------------------------------------------------------------------------------------------


. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      0.0000000
  gradboost      |      0.0000000
  nnet           |      1.0000000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: singlebest

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0000000
  elasticcv      |      1.0000000
  gradboost      |      0.0000000
  svm            |      0.0000000
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |     -0.0009287
  lassocv        |      0.8087335
  gradboost      |      0.1946095
  nnet           |     -0.1870912
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: ols

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1240919
  lassocv        |      0.2873197
  gradboost      |      0.2186900
  nnet           |      0.3602219
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1051612
  lassocv        |      0.6419153
  rf             |      0.2683163
  nnet           |     -0.2784574
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: ols

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0386918
  lassocv        |      0.5760072
  rf             |      0.0474635
  nnet           |      0.2318628
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0155114
  ridgecv        |      0.7764277
  gradboost      |      0.2561606
  nnet           |     -0.2338248
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: ols

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2505683
  ridgecv        |      0.1149965
  gradboost      |      0.1986832
  nnet           |      0.5097984
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0333156
  elasticcv      |      0.5410825
  gradboost      |      0.3846985
  nnet           |     -0.3741885
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.0804428
  elasticcv      |      0.9703585
  gradboost      |     -0.3040851
  nnet           |      0.3350215
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0857856
  elasticcv      |      0.7634550
  gradboost      |      0.1541100
  svm            |      0.0714999
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: ols

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4355100
  lassocv        |      0.0000000
  rf             |      0.5644900

. pystacked, graph holdout
Number of holdout observations:   48

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.1745922
  elasticcv      |      0.3743233
  gradboost      |      0.3793685
  svm            |      0.1705477
---------------------------------
Regress
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv gradboost nnet
Final estimator: avg

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_024.txt
  log type:  text
 closed on:   3 Jan 2025, 23:01:24
------------------------------------------------------------------------------------------------------------------------------------------------------


Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols lassocv rf nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  lassocv        |      0.2500000
  rf             |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols ridgecv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  ridgecv        |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost nnet
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  nnet           |      0.2500000
---------------------------------
Regress
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000
---------------------------------
Classify
Candidate learners: ols elasticcv gradboost svm
Final estimator: avg

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  logit          |      0.2500000
  elasticcv      |      0.2500000
  gradboost      |      0.2500000
  svm            |      0.2500000

. 
. *******************************************************************************
. *** check that predicted value = weighted avg of transform variables            ***
. *******************************************************************************
. 
. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. predict double yhat, xb

. list yhat if _n < 10

     +-----------+
     |      yhat |
     |-----------|
  1. | .69256735 |
  2. | .67613238 |
  3. | .38524425 |
  4. | .55128974 |
  5. | 1.6045648 |
     |-----------|
  6. |  .8260448 |
  7. | 1.7851343 |
  8. | 2.0047467 |
  9. | 1.2320337 |
     +-----------+

. 
. predict double t, basexb

. 
. mat W = e(weights)

. gen myhat = t1*el(W,1,1)+t2*el(W,2,1)+t3*el(W,3,1)

. 
. assert reldif(yhat,myhat)<0.0001

. 
. 
. *******************************************************************************
. *** check for error message when data in memory changed                                         ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. 
. set seed 124345

. 
. pystacked lpsa lcavol lweight age lbph svi lcp gleason pgg45, ///
>                                                  type(regress) pyseed(243) 

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.8885154
  lassocv        |      0.0000000
  gradboost      |      0.1114846

.                                                  
. replace lcavol = 2 * lcavol
(97 real changes made)

. 
. cap predict double yhat, xb

. assert _rc != 0

. 
. *******************************************************************************
. *** check table option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. pystacked, table

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.470            .            .
  ols            | 0.175       0.501        1.179            .
  lassocv        | 0.431       0.689        0.836            .
  rf             | 0.394       0.287        0.829            .

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // default holdout - all available obs
. pystacked, table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.466
  ols            | 0.000       0.317      182.461        5.538
  lassocv        | 0.660       0.546        0.648        1.441
  rf             | 0.340       0.258        0.675        1.531

. // specified holdout sample
. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.439            .        1.617
  ols            | 0.000       0.317      182.461        6.128
  lassocv        | 0.660       0.546        0.648        1.586
  rf             | 0.340       0.258        0.675        1.689

. // holdout sample overlaps with estimation sample
. cap noi pystacked, table holdout(h2)
error - holdout and estimation samples overlap

. assert _rc != 0

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  table holdout
Number of holdout observations:   48

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.546            .        1.441
  ols            | 0.000       0.317       56.850        5.538
  lassocv        | 1.000       0.546        0.632        1.441
  rf             | 0.000       0.258        0.751        1.531

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) ||  if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, table holdout(h1)
Number of holdout observations:   37

RMSPE: In-Sample, CV, Holdout
-----------------------------------------------------------------
  Method         | Weight   In-Sample        CV         Holdout
-----------------+-----------------------------------------------
  STACKING       |    .        0.357            .        1.667
  ols            | 0.438       0.526        0.633        1.678
  lassocv        | 0.000       0.570        0.646        1.543
  rf             | 0.562       0.258        0.619        1.689

. 
. *******************************************************************************
. *** check graph option                                                                                                          ***
. *******************************************************************************
. 
. clear

. insheet using https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data,  tab clear
(11 vars, 97 obs)

. global xvars lcavol-pgg

. 
. set seed 124345

. 
. // holdout sample 1
. cap drop h1

. gen h1 = _n>60

. // holdout sample 2
. cap drop h2

. gen h2 = _n>40

. 
. // postestimation syntax
. 
. // full sample
. pystacked lpsa $xvars, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.1749770
  lassocv        |      0.4306309
  rf             |      0.3943921

. // in-sample predictions
. pystacked, graph

. 
. // with holdout sample
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.0000000
  lassocv        |      0.6596640
  rf             |      0.3403360

. // in-sample predictions
. pystacked, graph

. // default holdout - all available obs
. pystacked, graph holdout
Number of holdout observations:   48

. // specified holdout sample
. pystacked, graph holdout(h1)
Number of holdout observations:   37

. // graphing options - combined graph
. pystacked, graph(subtitle("subtitle goes here")) holdout
Number of holdout observations:   48

. // graphing options - learner graphs
. pystacked, lgraph(ytitle("ytitle goes here")) holdout
Number of holdout observations:   48

. 
. // as pystacked option
. pystacked lpsa $xvars if _n<50, ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassocv rf) ///
>                                                  pipe1(poly2) pipe2(poly2) ///
>                                                  graph holdout
Number of holdout observations:   48

. 
. // syntax 2
. pystacked lpsa $xvars || method(ols) || method(lassocv) || method(rf) || if _n<50 , ///
>                                                  type(regress) pyseed(243) ///
>                                                  methods(ols lassoic rf)

Stacking weights:
---------------------------------------
  Method         |      Weight
-----------------+---------------------
  ols            |      0.4380350
  lassocv        |      0.0000000
  rf             |      0.5619650

. pystacked, graph holdout
Number of holdout observations:   48

.  
. 
end of do-file

. 
. di "############################"
############################

. di "all completed"
all completed

. 
. log close
      name:  <unnamed>
       log:  /Users/kahrens/MyProjects/pystacked/cert/log_cs_pystacked_130.txt
  log type:  text
 closed on:   3 Jan 2025, 23:02:32
------------------------------------------------------------------------------------------------------------------------------------------------------

. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
. 
end of do-file
